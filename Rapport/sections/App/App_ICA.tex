\chapter{Independent Component Analysis}\label{app:ICA}
This appendix provides the basic theory of independent component analysis (ICA). The theory is necessary if one wants a deeper understanding towards the justification of using the result from ICA as a reference for evaluation of the main algorithm proposed in this thesis. 
The appendix concludes with an algorithm specifying ICA method applied in the thesis. Additionally, a verification test is conducted to evaluate the applied ICA method on the synthetic data, cf. section \ref{sec:dataset}.   
\input{sections/ch_compressive/ICA.tex} 

\section{Fixed-Point Algorithm - FastICA}
An advantage of gradient algorithms is the possibility of fast adoption in non-stationary environments due the use of all input, $\textbf{y}$, at once. A disadvantage of the gradient algorithm is the resulting slow convergence, depending on the choice of $\gamma$ for which a bad choice in practice can disable convergence. A fixed-point iteration algorithm to maximize the non-Gaussianity is an alternative that could be used.

Consider the gradient step derived in section \ref{sec:gra_kur}.
In the fixed-point iteration the sequence of $\gamma$ is omitted and replaced by a constant. This builds upon the fact that for a stable point of the gradient algorithm the gradient must point in the direction of $\mathbf{b}$, hence be equal to $\mathbf{b}$. In this case adding the gradient to $\mathbf{b}$ does not change the direction and convergence is achieved. 
Letting the gradient given in \eqref{eq:kurt} be equal to $\mathbf{b}$ and considering the same simplifications again suggest the new update step as \cite[p. 179]{ICA}
\begin{align*}
\mathbf{b} \gets \mathbb{E}[\mathbf{y}(\textbf{b}^T \textbf{y})^3] - 3 \mathbf{b}.
\end{align*}
After the fixed-point iteration $\textbf{b}$ is again divided by its norm to withhold the constraint $\Vert \textbf{b} \Vert = 1$. 
Instead of $\gamma$ the fixed-point algorithm compute $\mathbf{b}$ directly from previous $\mathbf{b}$.

The fixed-point algorithm is referred to as FastICA. The algorithm has shown to converge fast and reliably, when the current and previous $\mathbf{b}$ point in the same direction \cite[p. 179]{ICA}. 

\subsection{Negentropy}
An alternative measure of non-Gaussianity is the negentropy, based on the differential entropy. The differential entropy $H$ of a random vector $\mathbf{y}$ with density $p_y (\boldsymbol{\eta})$ is defined as
\begin{align*}
H(\mathbf{y}) = - \int p_y (\boldsymbol{\eta}) \log (p_y (\boldsymbol{\eta})) \ d\boldsymbol{\eta}.
\end{align*}
The entropy describes the information that a random variable gives. The more unpredictable and unstructured a random variable is higher is the entropy, e.g. Gaussian random variables have a high entropy, in fact they have the highest entropy among the random variables of the same variance \cite[p. 182]{ICA}.

Negentropy is a normalised version of the differential entropy such that the measure of non-Gaussianity is zero when the random variable is Gaussian and non-negative otherwise. The negentropy $J$ of a random vector $\mathbf{y}$ is defined as 
\begin{align*}
J(\mathbf{y}) = H(\mathbf{y}_{\text{gauss}}) - H(\mathbf{y}),
\end{align*}
with $\mathbf{y}_{\text{gauss}}$ being a Gaussian random variable of the same covariance and correlation as $\mathbf{y}$ \cite[p. 182]{ICA}.
As the kurtosis is sensitive for outliers, the negentropy is instead difficult to compute computationally as the negentropy requires an estimate of the pdf. As such an approximation of the negentropy is needed.
To approximate the negentropy, it is common to use the higher order cumulants including the kurtosis. The following approximation of the scalar case is stated without further elaboration, and the derivation can be found in \cite[p. 183]{ICA}. 
\begin{align*}
J(\mathbf{y}) \approx \frac{1}{12} \mathbb{E}[y^{3}]^2 \frac{1}{48}\text{kurt}(y)^2.
\end{align*}

\subsection{Fixed-Point Algorithm with Negentropy}
Maximization of negentropy by use of the fixed-point algorithm is now presented, for derivation of the fixed-point iteration see \cite[p. 188]{ICA}. Algorithm \ref{alg:fastICA} show FastICA using negentropy.
This is the algorithm applied for comparison with the source recovery methods tested in this thesis.    
\begin{algorithm}[H]
\caption{FastICA -- with negentropy }
\begin{algorithmic}[1]
			\Procedure{Pre-processing}{$\textbf{y}$}
			\State $\text{Center measurements} \quad \textbf{y} \gets \textbf{y} - \bar{\textbf{y}}$
			\State $\text{Whitening} \quad \textbf{y}\gets \textbf{y}_{\text{white}}$ 
			\EndProcedure  
			\State
            \Procedure{FastICA}{$\textbf{y}$}    
			\State$k=0$            
            \State$\text{Initialize random vector} \quad \textbf{b}_{(k)}$ \Comment{unit norm}
            \For{$j \gets 1,2, \hdots ,N$ }
            
            	\While{$\text{convergence critia not meet}$} 
               		\State $k = k+1$
                	\State $\textbf{b}_{(k)} \gets \mathbb{E}[ \textbf{y}(\textbf{b}_{(k-1)}^T \textbf{y})] - \mathbb{E}[g'(\textbf{b}_{(k-1)}^T \textbf{y})] \textbf{b}_{(k-1)}$ \Comment{$g$ cf. \cite[p. 190]{ICA}} 
                	\State $\textbf{b}_{(k)} \gets \textbf{b}_{(k-1)}/\Vert \textbf{b}_{(k-1)} \Vert $ 
          		\EndWhile
          		\State $x_{j} = \textbf{b}^T\textbf{y}$
          	\EndFor
          	
            \EndProcedure
        \end{algorithmic} 
        \label{alg:fastICA}
\end{algorithm}

\section{Verification of FastICA on Synthetic Data}\label{app:ica_test}
The purpose of this section is to verify the FastICA algorithm used in this thesis. By this verification the purpose is to justify the FastICA algorithm as a reference point with respect to performance of the developed main algorithm.

The FastICA algorithm is tested on synthetic data simulated as described in section \ref{sec:dataset}. 
Consider the following linear system, which makes a model of EEG measurements 
\begin{align*}
\mathbf{Y} = \mathbf{AX},
\end{align*}
where $\mathbf{Y} \in \mathbb{R}^{M \times L}$, $\mathbf{A} \in \mathbb{R}^{M\times N}$ and $\mathbf{X} \in \mathbb{R}^{N \times L}$. 
It is expected that the FastICA algorithm manages to solve the linear system for $\mathbf{X}$ and $\mathbf{A}$ given only the measurements $\mathbf{Y}$, in the case where $M = N$.  

The FastICA algorithm is applied to $\mathbf{Y}$ and returns the estimates $\hat{\mathbf{X}}_{\text{ICA}}$ and $\hat{\mathbf{A}}_{\text{ICA}}$. 
When using the FastICA algorithm the output $\hat{\mathbf{X}}_{\text{ICA}}$ do not correspond one to one with the true source signals, which become an issue when the estimation error is measured by the mean squared error (MSE) cf. section \ref{sec:mse}.  The FastICA algorithm is invariant towards the amplitude and phase of the source signal.
Furthermore, the rows are not necessarily place the exact location. 
In order to get a valid MSE measure of the estimate, a function is defined to fit the estimate to the true source signal $\mathbf{X}$. 
The function manages to pair the rows and change the phase, such that the total MSE is minimized. 
Furthermore, each row of the estimate is scaled by the relationship between the maximum value of the true row and the estimated row.
From empirical observations only the phase shift performed by multiplying with $(-1)$ has shown necessarily, hence it is easily applied to the fitting function.
When the fitting function is applied two the estimate, the full potential of the FastICA algorithm is considered reached.       

Figure \ref{fig:appica1} illustrates $\hat{\mathbf{X}}_{\text{ICA}}$, without use of the fitting function, resulting from the FastICA algorithm applied to a simulated deterministic data set $\mathbf{Y}$ specified by $M = N = k = 4$ and $L = 1000$. 
In the figure $\mathbf{Y}$, $\mathbf{X}$ and $\hat{\mathbf{X}}_{\text{ICA}}$ are plotted separately and it is clear to see the invariance towards amplitude and phase.
The MSE from the original $\hat{\mathbf{X}}_{\text{ICA}}$ becomes
\begin{align*}
\text{MSE}(\mathbf{X}, \hat{\mathbf{X}}_{\text{ICA}}) = 0.608.
\end{align*}
In figure \ref{fig:appica2} the fitting function has been applied to $\hat{\mathbf{X}}_{\text{ICA}}$. Each row of the fitted estimate is now plotted with the corresponding row of the true source signals. 
The resulting MSE becomes 
\begin{align*}
\text{MSE}(\mathbf{X}, \hat{\mathbf{X}}_{\text{ICA}}) = 0.046.
\end{align*}  
This is an essential change from the first measured MSE, and it is considered to provide a more valid measure of the estimate. 
From the visualization and the corresponding MSE, it is found that the FastICA algorithm manages to estimate the source signals of the deterministic data set with a sufficiently small error. 
\begin{figure}[H]
\begin{widepage}
    \begin{minipage}[t]{.45\textwidth}
		\centering
		\includegraphics[scale=0.45]{figures/ICAapp/ICA_app1.png}
	\caption{Plot of simulated deterministic data set $\mathbf{Y}$, specified by $M = N = k = 4$ and $L = 1000$. Corresponding plot of the true $\mathbf{X}$ and the estimated $\hat{\mathbf{X}}$ by ICA.}
	\label{fig:appica1}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
		\centering
		\includegraphics[scale=0.45]{figures/ICAapp/ICA_app2.png}
	\caption{Direct comparison of the true $\mathbf{X}$ and $\hat{\mathbf{X}}_{\text{ICA}}$ after applying the fitting function.}
	\label{fig:appica2}
    \end{minipage}
\end{widepage}
\end{figure}
\noindent
A similar test is now performed on a stochastic data set $\mathbf{Y}$, cf. section \ref{sec:stoch_data}, again specified by $M = N = k = 4$ and $L = 1000$. 
Figure \ref{fig:appica3} show the comparison of the fitted $\hat{\mathbf{X}}_{\text{ICA}}$ and the true source signals $\mathbf{X}$. 
Note that only the first 100 samples are plotted for easier visualization. The resulting MSE becomes:
\begin{align*}
\text{MSE}(\mathbf{X}, \hat{\mathbf{X}}_{\text{ICA}}) = 0.037.
\end{align*}       
Again the MSE is considered sufficiently small and by that the FastICA is considered verified with respect to solving a linear system with $M = N$. 
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figures/ICAapp/ICA_app3.png}
	\caption{ICA applied to simulated stochastic data set $\mathbf{Y}$, specified by $M = N = k = 4$ and $L = 1000$ with direct comparison of the true $\mathbf{X}$ and $\hat{\mathbf{X}}_{\text{ICA}}$ after applying the fitting function.}
	\label{fig:appica3}
\end{figure}
\noindent
Consider now the case where $k \leq N = M$, that is the sources signal matrix has $k$ non-zeros rows. 
The FastICA algorithm is now applied to a stochastic data set $\mathbf{Y} $ specified by $N = M = 6$, $k = 4$ and $L = 1000$. 
Figure \ref{fig:appica5} and \ref{fig:appica6} show the comparison of the resulting $\hat{\mathbf{X}}_{\text{ICA}}$ and the true $\mathbf{X}$ before and after the application of the fitting function, respectively. The resulting MSE becomes:
\begin{align*}
\text{MSE}(\mathbf{X}, \hat{\mathbf{X}}_{\text{ICA}}) = 1.784.
\end{align*} 
It is seen from figure \ref{fig:appica6} that the FastICA algorithm manages to detect the zero rows of $\mathbf{X}$. Without further test, this indicates the possibility of estimating $k$ from the FastICA algorithm. 
\begin{figure}[H]
\begin{widepage}
    \begin{minipage}[t]{.45\textwidth}
		\centering
		\includegraphics[scale=0.5]{figures/ICAapp/ICA_app5.png}
	\caption{Plot of simulated stochastic data set $\mathbf{Y}$, specified by $M = N = 6$, $k = 4$ and $L = 1000$. Corresponding plot of the true $\mathbf{X}$ and the estimated $\hat{\mathbf{X}}$ by ICA.}
	\label{fig:appica5}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
		\centering
		\includegraphics[scale=0.5]{figures/ICAapp/ICA_app6.png}
	\caption{Direct comparison of the true $\mathbf{X}$ and $\hat{\mathbf{X}}_{\text{ICA}}$ after applying the fitting function.}
	\label{fig:appica6}
    \end{minipage}
\end{widepage}
\end{figure}
\noindent
With these tests the quality of the FastICA algorithm has been verified. 
As such the FastICA algorithm can be used as a reference, when applied to real EEG data. 
It is further established that $k \leq M$ can be estimated by FastICA. 
Remember though that the ICA estimate is conditioned under $k \leq N = M$.
However, this condition is not necessarily withheld for real EEG measurements as the true $N$ is always unknown.  