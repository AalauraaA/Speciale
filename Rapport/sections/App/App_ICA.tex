\chapter{Extended ICA Algorithms}\label{app:ICA}
This appendix provide an extension to the basic algorithm for ICA regarding the measure of non-Gaussianity and the computation method. This extended algorithm is referred to as fast ICA and is more commonly used for source separation. This is the algorithm used to apply ICA on EEG measurements for comparison within the thesis.        

\section{Fixed-Point Algorithm - FastICA}
An advantage of gradient algorithms is the possibility of fast adoption in non-stationary environments due the use of all input, $\textbf{y}$, at once. A disadvantage of the gradient algorithm is the resulting slow convergence, depending on the choice of $\gamma$ for which a bad choice in practise can disable convergence. A fixed-point iteration algorithm to maximise the non-Gaussianity is an alternative that could be used.\\
Consider the gradient step derived in section \ref{sec:gra_kur}.
In the fixed point iteration the sequence of $\gamma$ is omitted and replaced by a constant. This builds upon the fact that for a stable point\todo{wiki: The fixed point is stable if the absolute value of the derivative of \textbf{w} at the point is strictly less than 1?} of the gradient algorithm the gradient must point in the direction of $\textbf{b}_j$, hence be equal to $\textbf{b}_j$. In this case adding the gradient to $\textbf{b}_j$ does not change the direction and convergence is achieved.\\    
Letting the gradient given in \eqref{eq:kurt} be equal to $\mathbf{w}$ and  considering the same simplifications again
suggests the new update step as \cite[p. 179]{ICA}
\begin{align*}
\mathbf{b}_j \gets \mathbb{E}[\mathbf{y}(\textbf{b}_{j}^T \textbf{y})^3] - 3 \mathbf{b}_j.
\end{align*}
After the fixed point iteration $\textbf{b}_j$ is again divided by its norm to withhold the constraint $\Vert \textbf{b}_j \Vert = 1$.   
Instead of $\gamma$ the fixed-point algorithm compute $\mathbf{b}_j$ directly from previous $\mathbf{b}_j$.\\
The fixed-point algorithm is referred to as FastICA. The algorithm has shown to converge fast and reliably, then the current and previous $\mathbf{w}$ laid in the same direction \cite[p. 179]{ICA}. 

\subsection{Negentropy}
An alternative measure of non-Gaussianity is the negentropy, which is based on the differential entropy. The differential entropy $H$ of a random vector $\mathbf{y}$ with density $p_y (\boldsymbol{\eta})$ is defined as
\begin{align*}
H(\mathbf{y}) = - \int p_y (\boldsymbol{\eta}) \log (p_y (\boldsymbol{\eta})) \ d\boldsymbol{\eta}.
\end{align*}
The entropy describes the information that a random variable gives. The more unpredictable and unstructured a random variable is higher is the entropy, e.g. Gaussian random variables have a high entropy, in fact the highest entropy among the random variables of the same variance \cite[p. 182]{ICA}.
\\ \\
Negentropy is a normalised version of the differential entropy such that the measure of non-Gaussianity is zero when the random variable is Gaussian and non-negative otherwise. The negentropy $J$ of a random vector $\mathbf{y}$ is defined as 
\begin{align*}
J(\mathbf{y}) = H(\mathbf{y}_{\text{gaus}}) - H(\mathbf{y}),
\end{align*}
with $\mathbf{y}_{\text{gaus}}$ being a Gaussian random variable of the same covariance and correlation as $\mathbf{y}$ \cite[p. 182]{ICA}.
\\ \\
As the kurtosis is sensitive for outliers the negentropy is instead difficult to compute computationally as the negentropy require a estimate of the pdf. As such an approximation of the negentropy is needed.\\

To approximate the negentropy it is common to use the higher order comulants including the kurtosis. The following approximation is stated without further elaboration, the derivation can be found in \cite[p. 182]{ICA}. 

\subsection{Fixed-Point Algorithm with Negentropy}
Maximization of negentropy by use of the fixed-point algorithm is now presented, for derivation of the fixed point iteration see \cite[p. 188]{ICA}. Algorithm \ref{alg:fastICA} show Fast ICA using negentropy, this is the algorithm which is implemented for comparison with the source separation methods which are tested in this thesis.    

\begin{algorithm}[H]
\caption{Fast ICA -- with negentropy }
\begin{algorithmic}[1]
			\Procedure{Pre-processing}{$\textbf{y}$}
			\State $\text{Center measurements} \quad \textbf{y} \gets \textbf{y} - \bar{\textbf{y}}$
			\State $\text{Whitening} \quad \textbf{y}\gets \textbf{y}_{white}$ 
			\EndProcedure  
			\State
            \Procedure{FastICA}{$\textbf{y}$}    
			\State$k=0$            
            \State$\text{Initialise random vector} \quad \textbf{b}_{j(k)}$ \Comment{unit norm}
            \For{$j \gets 1,2, \hdots ,N$ }
            
            	\While{$\text{convergance critia not meet}$} 
               		\State $k = k+1$
                	\State $\textbf{b}_{j(k)} \gets \mathbb{E}[ \textbf{y}(\textbf{b}_{j}^T \textbf{y})] - \mathbb{E}[g'(\textbf{b}_{j}^T \textbf{y})] \textbf{b}_{j}$ \Comment{$g$ defined in \cite[p. 190]{ICA}} 
                	\State $\textbf{b}_{j(k)} \gets \textbf{b}_j/\Vert \textbf{b}_j \Vert $ 
          		\EndWhile
          		\State $x_{j} = \textbf{b}_{j}^T\textbf{y}$
          	\EndFor
          	
            \EndProcedure
        \end{algorithmic} 
        \label{alg:fastICA}
\end{algorithm}


%\section{OMP}
%$\mathbf{A}^\ast$ is the adjoint of a matrix $\mathbf{A}$.
%\begin{algorithm}[H]
%\caption{Orthogonal Matching Pursuit (OMP)}\label{alg:OMP}
%\begin{algorithmic}[1]
%			\State$k = 0$			
%			\State$\text{Initialize} \quad S_{(0)} =\emptyset$ 
%			\State$\text{Initialize} \quad x_{(0)} =\mathbf{0}$
%            \Procedure{OMP}{$\mathbf{A}, \mathbf{y}$}    
%            \While{$\text{stopping criteria not meet}$} 
%                \State$k = k + 1$
%				\State$j_{(k)} = \arg \max_{j \in [N]} \lbrace \vert (\mathbf{A}^\ast (\mathbf{y} - \mathbf{Ax}_{(k-1)}))_j \vert \rbrace$				
%				\State$S_{(k)} = S_{(k-1)} \cup \lbrace j_{(k)} \rbrace$ 
%				\State$\mathbf{x}_{(k)} = \arg \min_{\mathbf{z} \in \mathbb{C}^N} \lbrace \Vert \mathbf{y} - \mathbf{Az} \Vert_2 \  \vert \ \text{supp}(\mathbf{z}) \subset S_{(k)} \rbrace$
%          		\EndWhile
%          		\State$\mathbf{x}^\ast = \mathbf{x}_{(k)}$
%            \EndProcedure
%        \end{algorithmic} 
%        \label{alg:OMP}
%\end{algorithm}

\subsection{Test of ICA on synthetic data}
In this section the fast ICA algorithm is tested on on...


