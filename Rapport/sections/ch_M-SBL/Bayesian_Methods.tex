....\\
INTRODUCTION\\
....\\
\\ 
Consider the multiple measurement vector model (MMV)
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
where the unknown matrices $\mathbf{A} \in \mathbb{R}^{M \times N}$ and $\mathbf{X} \in \mathbb{R}^{N \times L}$ are wished recovered from the known measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$ in the case of $M < N$. In chapter \ref{ch:Cov-DL} the mixing matrix $\mathbf{A}$ was found. In this chapter, a method to recover the source matrix $\mathbf{X}$ is sought. The approach is to find the support set $S$ of $\textbf{X}$ providing the non-zeros rows of $\mathbf{X}$ which corresponds to localisation of the active sources. By ... the active sources are identified in order to fully recover $\textbf{X}$.
\\
The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.
%\textit{
%\begin{itemize}
%\item $M \leq N$ (more sources than sensors)
%\item $M \leq k \leq N$ activations within the sources
%\item Instantaneous mixing -- no time delay between samples
%\item The conditions on the support set are:
%\begin{itemize}
%\item Orthogonality/uncorrelated of the active sources $k$.
%\item Constraint on the dictionary matrix $\mathbf{A}$
%\end{itemize}
%\end{itemize}
%}

\section{Maximum a Posterior Estimation}
%\textit{NOTES:
%\begin{itemize}
%\item Finding sparse solutions to our optimisation problem can be view in Bayesian framework. We can find a sparse X by finding its estimates with maximum a posterior (MAP) estimation with use of a fixed and sparsity induced prior. This is also called empirical Bayesian with used a parameterized prior to encourage sparsity.
%\item With a global SBL minimum we always achieve a maximal sparse solution.
%\item By assuming some prior belif that Y has been generated by sparse coefficeinet expansion such that most ellemens in X are zero -- called sparsity-induciong prior
%\item ALternative we can use empirical priors
%\item One problem -- they all ensuing the inverse problem from Y to X becoming non-linear.
%\end{itemize}
%}
With the knowledge of $\mathbf{A}$ and $\mathbf{Y}$ it is possible to find maximum likelihood estimate of $\textbf{X}$. By maximising the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ an estimate of $\mathbf{X}$ can be achieved in the case of more sensors than sources, $M > N$. But in the desired case where $M < N$ the estimation becomes complicate as the MMV model becomes under-determined and potentially an infinitely number of solutions exist with equal likelihoods.
%The maximum a posterior (MAP) estimate of $\textbf{X}$, including a prior which induce the wanted sparsity of $\mathbf{X}$... is one approach?
\\
As the optimisation problem of the MMV model is NP-hard another estimation method must be used.
\\ \\
Bayesian probability \todo{indsæt her hvad tanken bag Bayesian egenligt er} ... \\
Within this Bayesian framework the source matrix $\mathbf{X}$ is seen as a variable which is drawn from some distribution $p(\mathbf{X})$ such that it is possible to narrow down the infinitely solution space. Assuming a prior belief that $\textbf{Y}$ is generated from a sparse coefficient matrix, that is the distribution from where $\textbf{X}$ is drawn has a sharp, possibly infinite, spike at zero surrounded by fat tails.     
\\
%%5
%E.g. $\mathbf{X}$ could be drawn from a Gaussian prior with zero-mean and covariance $\sigma_X^2 \mathbf{I}$ where the additional noise $\mathbf{E}$ is independently Gaussian with covariance $\sigma_E^2 \mathbf{I}$. The MAP estimate of $\textbf{X}$ is then given as
%\begin{align*}
%\hat{\mathbf{X}} = \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) = \mathbf{A}^T (\lambda \mathbf{I} + \mathbf{AA}^T)^{-1} \mathbf{Y},
%\end{align*}
%with $\lambda = \sigma_E^2 / \sigma_X^2$. With this distribution the estimate of the source matrix $\hat{\mathbf{X}}$ would have a large number of small non-zero coefficients.
%%%% can we remove this example??  
\\ \\
By applying an exponential function $\exp(- (\cdot))$ transformation onto our optimisation problem a Gaussian likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ with a $\lambda$-dependent variance is achieved:
\begin{align*}
p(\mathbf{Y} \vert \mathbf{X}) \propto \exp \left( - \frac{1}{\gamma} \Vert \mathbf{Y} - \mathbf{AX} \Vert_2^2 \right),
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \exp(- \Vert \mathbf{X} \Vert_0)$ \cite[p. 137]{phd_wipf}. 
The optimisation problem is then rewritten by Bayers formula 
%The MAP estimation problem is then rewritten to
\begin{align*}
\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
\end{align*}
From this it it possible to view the optimization problem as a MAP estimation challenge.

\section{Empirical Bayesian Estimation}
Different MAP estimation approaches exists separated by the choice of sparsity inducing prior and optimization method. Some problems have shown to occur when using a fixed and algorithm-dependent prior as the posterior is not sparse enough if a prior is not as sparse leading to a non-recovery. 
Another issue is that a combinatorial number of suboptimal local solutions can occur.  
\\
By use of automatic relevance determination (ARD) the problems related to the sparse prior can be avoided. ARD is a method where a prior is introduced to determine the relevance of a parameter. This prior is modulated by a vector of hyperparameters affecting the prior variance of each row in $X$.
%The rest will become zero. 
This can also be view as a regularisation of the solution space which is narrowed to consist only of relevant information \cite{ARD}.
\\
An empirical prior can be used with ARD as the empirical prior is flexible and depends on the unknown hyperparameter $\boldsymbol{\gamma}$ and therefore more data-dependent -- such prior can be controlled to induce sparsity.
\\ \\
Let the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ be Gaussian, with known noise variance $\sigma^2$. Then for each column in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written as
\begin{align*}
p(\mathbf{y}_{j} \vert \mathbf{x}_{j}) &= \mathcal{N}(\mathbf{Ax}_{j}, \sigma^2 \mathbf{I}) \\
&= (2 \pi \sigma^2)^{-N/2} \exp \left( - \frac{1}{2 \sigma^2} \Vert \mathbf{y}_{j} - \mathbf{A} \mathbf{x}_{j} \Vert_2^2 \right).
\end{align*}
With the use of ARD the $i$-th row of the sources matrix $\mathbf{X}$, $\mathbf{x}_{i \bullet}$, is assigned an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by $\gamma_i$ which is unknown:
\begin{align*}
p (\mathbf{x}_{i \bullet} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I}).
\end{align*}
By combing the row priors
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{j=1}^L p (\mathbf{x}_{i \bullet} ; \gamma_i),
\end{align*}
a full prior of $\mathbf{X}$ is achieved modulated by the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_M]^T$. By combining the full prior and the likelihood $p(\mathbf{y}_{j} \vert \mathbf{x}_{j})$ the posterior of the $j$-th column of the source matrix $\mathbf{X}$ becomes
\begin{align*}
p(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) = \frac{p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{j}} = \mathcal{N}(\boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}),
\end{align*}
with mean and covariance given as
\begin{align}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \nonumber \\
\mathcal{M} &= [\boldsymbol{\mu}_{1 \bullet}, \dots, \boldsymbol{\mu}_{ L \bullet}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}, \label{eq:moments}
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$ and $\boldsymbol{\Sigma}_y = \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T$. Let now the posterior mean serve as the point estimate for $\mathbf{X}$ without involving the support set $S$.
\\
It is clear that row sparsity is achieved whenever $\gamma_i = 0$. From this the posterior must satisfy the following 
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
which ensure that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \bullet}$, will be zero. Now, instead of estimating the support set?/sparsity profile of our source matrix $\mathbf{X}$ it is sufficient to estimate the hyperparameters $\gamma_i$\cite[p. 147]{phd_wipf}.
\\
Each different hyperparameter $\boldsymbol{\gamma}$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determination of $\boldsymbol{\gamma}$ is seen as a model selection for which an empirical Bayesian strategy can be used. By the empirical Bayesian strategy the unknown weights, making the source matrix $\textbf{X}$, are treated as nuisance parameters and are integrated out.
\\
By integrating the likelihood of $\textbf{Y}$ with respect to the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$, $p (\mathbf{Y} ; \boldsymbol{\gamma})$ is achieved \cite[p. 146]{phd_wipf}. By applying the $-2 \log (\cdot)$ transformation the marginal likelihood function is transformed to a cost function
\begin{align*}
\mathcal{l}(\boldsymbol{\gamma}) &= -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma}))\\
&= \log ( \vert \boldsymbol{\Sigma}_y \vert) + \frac{1}{L} \sum_{j=1}^L \mathbf{y}_{j}^T \boldsymbol{\Sigma}_y ^{-1} \mathbf{y}_{j}
\end{align*}
To minimise the marginal log likelihood $\mathcal{l}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the evidence maximisation (EM) algorithm can be used. The E-step of the EM algorithm is to compute the posterior moments using \eqref{eq:moments} while the M-step is the following update rule of $\gamma_i$:
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \bullet} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, M.
\end{align*}
The M-step is very slow on large data. Instead one could use a fixed point update to fasten the convergence on large data, however convergence is no longer ensured. The fixed point updating step is achieved by taking the derivative of the marginal log likelihood $\mathcal{l}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. This lead to the following update equation which can replace the above M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \bullet} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, M.
\end{align*}
Empirically this alternative update rule have shown use full in highly under-determined large scale cases by driving many hyper parameters toward zero allowing for the corresponding weight in the source matrix to be discarded. For simultaneous sparse approximation problems this is the process referred to as multiple sparse Bayesian learning, M-SBL.\\
From the resulting $\boldsymbol{\gamma}^\ast$ the support set $S$ of the source matrix $\textbf{X}$ can be extracted, 
\begin{align*}
S = \{ i \vert \hat{\gamma}_i \neq 0 \},
\end{align*}
concluding the localisation of active sources within $\textbf{X}$. In practise some arbitrary small threshold can be used such that that any sufficiently small hyperparameter is discarded.\\
For identification of the active sources the estimate of the source matrix $\textbf{X}$ is given as $\textbf{X}^\ast=\mathcal{M}^\ast \approx \textbf{X}$, with $\mathcal{M}^\ast = \mathbb{E}[\textbf{X}\vert \textbf{Y};\boldsymbol{\gamma}^\ast]$. This leads to the following estimate  

\begin{align*}
\mathbf{X}^\ast = 
\begin{cases}
\mathbf{x}_{i\bullet} = \boldsymbol{\mu}_{i \bullet}^\ast, & i \in S \\
\mathbf{x}_{i\bullet} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}


\begin{algorithm}[H]
\caption{M-SBL}
\begin{itemize}
\item[1.] Given $\mathbf{Y}$ and a dictionary matrix $\mathbf{A}$.
\item[2.] Initialise $\boldsymbol{\gamma}$, e.g $\boldsymbol{\gamma} = \mathbf{1}$.
\item[3.] Compute the posterior moments $\boldsymbol{\Sigma}$ and $\mathcal{M}$.
\item[4.] Update $\boldsymbol{\gamma}$ using EM or fixed-point.
\item[5.] Repeat step 3 and 4 until convergence to a fixed point $\boldsymbol{\gamma}^\ast$.
\item[6.] Update the posterior moments $\boldsymbol{\Sigma}^\ast$ and $\mathcal{M}^\ast$ with $\gamma^\ast$.
\item[7.] Extract support set $S$
\item[8.] Set source matrix estimate $\hat{\mathbf{X}}^\ast = \mathcal{M}^\ast_S$
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
\begin{itemize}
\item Bayesian replace the troublesome prior with a distribution that, while still encouraging sparsity, is somehow more computationally convenient. Bayesian approaches to the sparse approximation problem that follow this route have typically been divided into two categories: (i) maximum a posteriori (MAP) estimation using a fixed, computationally tractable family of priors and, (ii) empirical Bayesian approaches that employ a flexible, parameterized prior that is ‘learned’ from the data
\end{itemize}