INTRODUCTION!!!!
\\ \\
For the multiple measurement vector model (MMV)
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
the unknown matrices $\mathbf{A} \in \mathbb{R}^{M \times N}$ and $\mathbf{X} \in \mathbb{R}^{N \times L}$ are wished recovered from the known measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$ with $M < N$. In chapter XX\todo{COV ref} the mixing matrix $\mathbf{A}$ was found. In this chapter, a method to recover the source matrix $\mathbf{X}$ is sought. A way to do this is to find the support set $S$ of the non-zeros rows $k$ (the active sources) of $\mathbf{X}$ which corresponds to localisation of the active and non-active sources.
\\
The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.
%\textit{
%\begin{itemize}
%\item $M \leq N$ (more sources than sensors)
%\item $M \leq k \leq N$ activations within the sources
%\item Instantaneous mixing -- no time delay between samples
%\item The conditions on the support set are:
%\begin{itemize}
%\item Orthogonality/uncorrelated of the active sources $k$.
%\item Constraint on the dictionary matrix $\mathbf{A}$
%\end{itemize}
%\end{itemize}
%}

\section{Maximum a Posterior Estimation}
%\textit{NOTES:
%\begin{itemize}
%\item Finding sparse solutions to our optimisation problem can be view in Bayesian framework. We can find a sparse X by finding its estimates with maximum a posterior (MAP) estimation with use of a fixed and sparsity induced prior. This is also called empirical Bayesian with used a parameterized prior to encourage sparsity.
%\item With a global SBL minimum we always achieve a maximal sparse solution.
%\item By assuming some prior belif that Y has been generated by sparse coefficeinet expansion such that most ellemens in X are zero -- called sparsity-induciong prior
%\item ALternative we can use empirical priors
%\item One problem -- they all ensuing the inverse problem from Y to X becoming non-linear.
%\end{itemize}
%}
A possible solution to recover $\mathbf{X}$ with the knowledge of $\mathbf{A}$ and $\mathbf{Y}$ is by finding its estimate, e.g. from the maximum a posterior (MAP) with a prior which induce the wanted sparsity of $\mathbf{X}$. By maximising the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ a solution for an estimate of $\mathbf{X}$ is achieved when more sources than sensors are presented in the MMV model. But it  is not always the case where $\mathbf{X}$ has a smaller dimensionality than $\mathbf{Y}$ because of the wanted sparsity of $\mathbf{X}$ leading to a matrix of greater dimensionality because of the added zeros. Hence the estimation becomes complicate as the MMV model becomes under-determined -- an infinitely number of solutions with equal likelihoods.
\\
As the optimisation problem of the MMV model is NP-hard another estimation method must be used.
\\ \\
Bayesian probability \todo{indsæt her hvad tanken bag Bayesian egenligt er} ... With this Bayesian framework the source matrix $\mathbf{X}$ seen as a variable can be drawn from some distribution $p(\mathbf{X})$ such that the infinitely solution space is narrowed. 
\\
E.g. $\mathbf{X}$ could be drawn from a Gaussian prior with zero-mean and covariance $\sigma_X^2 \mathbf{I}$ where the additional noise $\mathbf{E}$ is independently Gaussian with covariance $\sigma_E^2 \mathbf{I}$. The MAP estimator could then be rewritten to
\begin{align*}
\hat{\mathbf{X}} = \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) = \mathbf{A}^T (\lambda \mathbf{I} + \mathbf{AA}^T)^{-1} \mathbf{Y},
\end{align*}
with $\lambda = \sigma_E^2 / \sigma_X^2$. With this distribution the estimate of the source matrix $\hat{\mathbf{X}}$ would have a large number of small non-zero coefficients.
\\ \\
By applying an exponential function $\exp(- (\cdot))$ transformation onto our optimisation problem a Gaussian likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ with a $\lambda$-dependent variance is achieved:
\begin{align*}
p(\mathbf{Y} \vert \mathbf{X}) \propto \exp \left( - \frac{1}{\gamma} \Vert \mathbf{Y} - \mathbf{AX} \Vert_2^2 \right),
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \exp(- \Vert \mathbf{X} \Vert_0)$ \cite[p. 137]{phd_wipf}. The MAP estimation problem is then rewritten to
\begin{align*}
\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
\end{align*}

\section{Empirical Bayesian Estimation}
From earlier MAP estimation approaches some problems occurs when using a fixed and algorithm-dependent prior as the posterior is not sparse enough if a prior is not as sparse leading to a non-recovery . In other cases the prior can become to sparse and then lead to a combinatorial problem when looking for the global optima.
\\
By using automatic relevance determination (ARD) the problem of using sparse prior can be overcome. ARD is a method where a prior is introduced to determine the relevance of a parameter. The rest will become zero. This can also be view as a regularisation of the solution space which has been narrowed because of the prior and only consist of the relevance information \cite{ARD}.
\\
An empirical prior can be used with ARD as the empirical prior is flexible and dependent on the unknown hyperparameter $\gamma$ and therefore more data-dependent -- the prior can be controlled to induce sparsity.
\\ \\
Let $p(\mathbf{Y} \vert \mathbf{X})$ be a Gaussian prior with a known noise variance $\sigma^2$. Then for each columns in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written as
\begin{align*}
p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{.j}, \sigma^2 \mathbf{I}) \\
&= (2 \pi \sigma^2)^{-N/2} \exp \left( - \frac{1}{2 \sigma^2} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right).
\end{align*}
With the used of ARD the $i$-th row of the sources matrix $\mathbf{X}$, $\mathbf{x}_{i \cdot}$, is assigned an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by $\gamma_i$ which is unknown:
\begin{align*}
p (\mathbf{x}_{i \cdot} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I}).
\end{align*}
By combing the row priors 
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{j=1}^L p (\mathbf{x}_{ \cdot j} ; \gamma_i),
\end{align*}
a full prior of $\mathbf{X}$ is achieved with the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_M]^T$. By combining the full prior and the likelihood $p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j})$ the posterior of the $j$-th column of the source matrix $\mathbf{X}$ is defined as
\begin{align*}
p(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \frac{p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} = \mathcal{N}(\boldsymbol{\mu}_{\cdot j}, \boldsymbol{\Sigma}),
\end{align*}
with the mean and covariance given as
\begin{align}\label{eq:moments}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \left( \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \\
\mathcal{M} &= [\boldsymbol{\mu}_{\cdot 1}, \dots, \boldsymbol{\mu}_{\cdot L}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \left( \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{Y},
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$. The posterior mean is the point estimate for $\mathbf{X}$ without involving the support set $S$.
\\
The row sparsity is achieved whenever $\gamma_i = 0$ leading to that the posterior must have the following probability
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
which ensure that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, will be zero. Instead of estimating our source matrix $\mathbf{X}$ we instead estimate the hyperparameter $\gamma_i$.
\\ \\
Each of the hyperparameters $\gamma_i$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determining of $\gamma_i$ must be seen as a model selection in with we can use the empirical Bayesian stragetry. This evolve the task of treating the unknown source matrix $\mathbf{X}$ as nuisance\todo{pestilens, plage, gene -- de brugte dette ord i phden} parameters and integrating them out.
\\
By integrating the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$, $p (\mathbf{Y} ; \boldsymbol{\gamma})$ is achieved \cite[p. 146]{phd_wipf}. By applying the $-2 \log (\cdot)$ transformation the marginal likelihood function is transformed to the cost function
\begin{align*}
\mathcal{L}(\boldsymbol{\gamma}) &= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma})) = -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= \log ( \vert \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T \vert) + \frac{1}{L} \sum_{j=1}^L \mathbf{y}_{.j}^T \left( \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{y}_{.j}
\end{align*}
To minimise the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the evidence maximisation (EM) algorithm can be used. The E-step of the EM algorithm is to compute the posterior moments as mention in \eqref{eq:moments} while the M-step is a update rule of $\gamma_i$:
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, M.
\end{align*}
The M-step is very slow on large data. Instead one could use a fixed point update to fasten the convergence on large data. The fixed point updating step is achieved by taking the derivative of the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. This lead to the updating equation which can replace the one from M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, M.
\end{align*}
After convergence the support set $S$ is extracted from the solution of $\hat{\gamma}$
\begin{align*}
S = \{ i, \hat{\gamma}_i \neq 0 \},
\end{align*}
which give the non-zero indexes of the final posterior mean $\mathcal{M}^\ast$. The estimate for $\mathbf{X}$ is constructed which used of the non-zero indices of $\mathcal{M}^\ast$:
\begin{align*}
\hat{\mathbf{X}}^\ast = 
\begin{cases}
\mathbf{x}_{i\cdot} = \boldsymbol{\mu}_i^\ast, & i \in S \\
\mathbf{x}_{i\cdot} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}


\begin{algorithm}[H]
\caption{M-SBL}
\begin{itemize}
\item[1.] Given $\mathbf{Y}$ and a dictionary matrix $\mathbf{A}$.
\item[2.] Initialise $\boldsymbol{\gamma}$, e.g $\boldsymbol{\gamma} = \mathbf{1}$.
\item[3.] Compute the posterior moments $\boldsymbol{\Sigma}$ and $\mathcal{M}$.
\item[4.] Update $\boldsymbol{\gamma}$ using EM or fixed-point.
\item[5.] Repeat step 3 and 4 until convergence to a fixed point $\boldsymbol{\gamma}^\ast$.
\item[6.] Update the posterior moments $\boldsymbol{\Sigma}^\ast$ and $\mathcal{M}^\ast$ with $\gamma^\ast$.
\item[7.] Extract support set $S$
\item[8.] Set source matrix estimate $\hat{\mathbf{X}}^\ast = \mathcal{M}^\ast_S$
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
\begin{itemize}
\item Bayesian replace the troublesome prior with a distribution that, while still encouraging sparsity, is somehow more computationally convenient. Bayesian approaches to the sparse approximation problem that follow this route have typically been divided into two categories: (i) maximum a posteriori (MAP) estimation using a fixed, computationally tractable family of priors and, (ii) empirical Bayesian approaches that employ a flexible, parameterized prior that is ‘learned’ from the data
\end{itemize}