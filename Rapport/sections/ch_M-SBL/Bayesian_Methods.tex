As described in earlier chapters, the support set of sources is wish recovered. A way to recover the set is to use sparse bayesian learning (M-SBL) on the MMV model. To insure full recovery some sufficient condition must be applied on the dictionary matrix $\mathbf{A}$ and the sources. 
\\
The chapter is inspired by \cite{??} \todo{indsæt ref for bayesian phd}.
\textit{
\begin{itemize}
\item $M \leq N$ (more sources than sensors)
\item $k \leq N$ activations within the sources
\item Instantaneous mixing -- no time delay between samples
\item The conditions on the support set are:
\begin{itemize}
\item Orthogonality/uncorrelated of the active sources $k$.
\item Constraint on the dictionary matrix $\mathbf{A}$
\end{itemize}
\end{itemize}
}


\section{M-SBL Algorithm}
\textit{NOTES:
\begin{itemize}
\item Finding sparse solutions to our optimisation problem can be view in Bayesian framework. We can find a sparse X by finding its estimates with maximum a posterior (MAP) estimation with use of a fixed and sparsity induced prior. This is also called empirical Bayesian with used a parameterized prior to encourage sparsity.
\item With a global SBL minimum we always achieve a maximal sparse solution.
\item By assuming some prior belif that Y has been generated by sparse coefficeinet expansion such that most ellemens in X are zero -- called sparsity-induciong prior
\item ALternative we can use empirical priors
\item One problem -- they all ensuing the inverse problem from Y to X becoming non-linear.
\end{itemize}
}
As mention in section \ref{sec:MMV} a multiple measurement model (MMV) $\mathbf{Y} \in \mathbb{R}^{M \times L}$
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
models the mixing of a unknown source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ with a mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and some additional noise $\mathbf{E} \in \mathbb{R}^{M \times L}$ for $L$ snapshots in time. The mixing matrix $\mathbf{A}$ is in this section known and is found with the Cov-DL as described in section XX \todo{Sæt reference ind til Cov-DL}. The non-zeros rows of $\mathbf{X}$ is referred to as the active sources and there are at most $k \leq M$ activations.
\\
The goal is to find a source matrix $\mathbf{X}$ which is as sparse as possible to ensure a full recovery of our MMV \eqref{eq:MMV_model}.
\\ \\
One way to find the source matrix $\mathbf{X}$ is by finding its estimate, e.g. from the maximum a posterior (MAP) which have priors which induce sparsity of $\mathbf{X}$ given the measurement matrix $\mathbf{Y}$ and mixing matrix $\mathbf{A}$. By maximising the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ and effective solution for an estimate of $\mathbf{X}$ is achieved when $M \ll N$. It is not always the case where $\mathbf{X}$ has a smaller dimensionality than $\mathbf{Y}$ because of the wanted sparsity of $\mathbf{X}$ leading to a matrix of greater dimensionality because of the added zeros. Hence the estimation becomes complicate as the MMV model becomes under-determined -- an infinitely number of solutions with equal likelihoods.
\\
As the optimisation problem of the MMV model is NP-hard another estimation method must be used.
\\ \\
Bayesian probability \todo{indsæt her hvad tanken bag Bayesian egenligt er} ... With this Bayesian framework the source matrix $\mathbf{X}$ seen as a variable can be drawn from some distribution $p(\mathbf{X})$ such that the infinitely solution space is narrowed. 
\\
E.g. $\mathbf{X}$ could be drawn from a Gaussian prior with zero-mean and covariance $\sigma_X^2 \mathbf{I}$ where the additional noise $\mathbf{E}$ is independently Gaussian with covariance $\sigma_E^2 \mathbf{I}$. The MAP estimator could then be rewritten to
\begin{align*}
\hat{\mathbf{X}} = \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) = \mathbf{A}^T (\lambda \mathbf{I} + \mathbf{AA}^T)^{-1} \mathbf{Y},
\end{align*}
with $\lambda = \sigma_E^2 / \sigma_X^2$. With this distribution the estimate of the source matrix $\hat{\mathbf{X}}$ would have a large number of small non-zero coefficients.
\\ \\

\todo{Kommet her til}
By apply an exponential function $\exp(- (\cdot))$ transformation onto our optimisation problem a Gaussian likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ with a $\lambda$-dependent variance is achieved:
\begin{align*}
p(\mathbf{Y} \vert \mathbf{X}) \propto \exp \left( - \frac{1}{\gamma} \Vert \mathbf{Y} - \mathbf{AX} \Vert_2^2 \right),
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \exp(- \Vert \mathbf{X} \Vert_0)$. The optimisation problem is the rewritten to
\begin{align*}
\mathbf{X} (\gamma) = \hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
\end{align*}
$\mathbf{X}$ can then be estimated by the used of maximum a posterior (MAP) estimation \cite[p. 137]{??}.

\subsection{Empirical Bayesian Algorithm}
Let $p(\mathbf{Y} \vert \mathbf{X})$ be a Gaussian prior with a known noise variance $\lambda$. Then for each columns in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written as
\begin{align*}
p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{.j}, \lambda^2 \mathbf{I}) \\
&= (2 \pi \lambda)^{-N/2} \exp \left( - \frac{1}{2 \lambda} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right).
\end{align*}
The $i$-th row of the sources matrix $\mathbf{X}$, $\mathbf{x}_{i \cdot}$, has an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by $\gamma_i$ which is unknown:
\begin{align*}
p (\mathbf{x}_{i \cdot} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I})
\end{align*}
By combing the two row priors 
\begin{align*}
p (\mathbf{X} \vert \boldsymbol{\gamma}) &= \prod_{j=1}^L p (\mathbf{x}_{ \cdot j} \vert \gamma_i),
\end{align*}
a full prior is achieved with the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_M]^T$. By combining the full prior and the likelihood the posterior of the $j$-th column of the source matrix $\mathbf{X}$ is defined as
\begin{align*}
p(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \frac{p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} = \mathcal{N}(\boldsymbol{\mu}_{\cdot j}, \boldsymbol{\Sigma}),
\end{align*}
with the mean and covariance given as
\begin{align}\label{eq:moments}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \\
\mathcal{M} &= [\boldsymbol{\mu}_{\cdot 1}, \dots, \boldsymbol{\mu}_{\cdot L}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{Y},
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$. 
\\ \\
The row sparsity is achieved whenever $\gamma_i = 0$ leading to that the posterior must have the following probability
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
which ensure that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, will be zero. Instead of estimating our source matrix $\mathbf{X}$ we instead estimate the hyperparameter $\gamma_i$.
\\
Each of the hyperparameters $\gamma_i$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determining of $\gamma_i$ must be seen as a model selection in with we can use the empirical Bayesian stragetry. This evolve the task of treating the unknown source matrix $\mathbf{X}$ as nuisance\todo{pestilens, plage, gene -- de brugte dette ord i phden} parameters and integrating them out.
\\
By integrating the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$, $p (\mathbf{Y} ; \gamma)$ is achieved \cite[p. 146]{??}. By applying the $-2 \log (\cdot)$ transformation the marginal likelihood function is transformed to the cost function
\begin{align*}
\mathcal{L}(\gamma) &= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma})) = -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= L \log ( \vert \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T \vert) + \sum_{j=1}^L \mathbf{y}_{.j}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{y}_{.j}
\end{align*}
To minimise the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the evidence maximisation (EM) algorithm can be used. The E-step of the EM algorithm is to compute the posterior moments as mention in \eqref{eq:moments} while the M-step is a update rule of $\gamma_i$:
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, M.
\end{align*}
The M-step is very slow on large data. Instead one could use a fixed point update to fasten the convergence on large data. The fixed point updating step is achieved by taking the derivative of the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. This lead to the updating equation which can replace the one from M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, M.
\end{align*}
After convergence the support set $\hat{S}$ is extracted from the solution $\hat{\gamma}$ by $\hat{S} = \{ i, \hat{\gamma}_i \neq 0 \}$.


\begin{algorithm}[H]
\caption{M-SBL -- See page 148 in PHD}
\begin{itemize}
\item[1.] Given $\mathbf{Y}$ and a dictionary matrix $\mathbf{A}$.
\item[2.] Initialise $\boldsymbol{\gamma}$, e.g $\boldsymbol{\gamma} = \mathbf{1}$.
\item[3.] Compute the posterior moments $\boldsymbol{\Sigma}$ and $\mathcal{M}$.
\item[4.] Update $\boldsymbol{\gamma}$ using EM or fixed-point.
\item[5.] Repeat step 3 and 4 until convergence to a fixed point $\boldsymbol{\gamma}^\ast$.
\item[6.] ...
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
\begin{itemize}
\item With M-SBL the \textbf{support set} of source can be recover for $k \geq M$, with some sufficient condition on the dictionary and sources. $M \leq k \leq N$ the support set can be recovered in the noiseless case.
\item We assume that mixing at the sensors is instantaneous (no time delay between sources and sensors) and the environment is anechoic. (M-SBL)
\item The sufficient conditions for exact support recovery for M-SBL in the regime k $\geq$ M are twofold: 1) orthogonality (uncorrelated) of the active sources, 2) The second condition imposes a constraint on the sensing dictionary A
\item Bayesian replace the troublesome prior with a distribution that, while still encouraging sparsity, is somehow more computationally convenient. Bayesian approaches to the sparse approximation problem that follow this route have typically been divided into two categories: (i) maximum a posteriori (MAP) estimation using a fixed, computationally tractable family of priors and, (ii) empirical Bayesian approaches that employ a flexible, parameterized prior that is ‘learned’ from the data
\end{itemize}