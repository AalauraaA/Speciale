As described in section \ref{sec:sol_met} the covariance-domain dictionary learning (Cov-DL) is not a fitted method to use in the recovering of the source matrix $\mathbf{X}$ as the found source matrix is not the true recovering. 
Instead a different method, multiple sparse Bayesian Learning (M-SBL), is used for the recovering process of the last element of the multiple measurement model (MMV) \ref{eq:MMV_model}.

This chapter will include a introduction to M-SBL and some theory of its Bayesian background. The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.

Let first consider the MMV model for a non-segmented case
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
with the mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ which is in this chapter a known matrix and source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ which is wished recovered from the known EEG measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$ in the case of $M < N$. 
The approach is to find the support set $S$ of $\textbf{X}$ providing the non-zeros rows of $\mathbf{X}$ which corresponds to localization of the active sources. The support set $S$ is constructed from a minimisation of a log-likelihood function defined from Bayesian framework.

\section{Bayesian Interference}
From Bayesian interference theory a posterior distribution is constructed to predict a new distribution of a unknown data sample such that a distribution of the unknown samples are return. The optimum sample estimate of a parameter -- for the posterior distribution -- can be found by e.g. maximum a posterior (MAP) estimation \todo{Kilde (WIKI): This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution}.

With the knowledge of $\mathbf{A}$ and $\mathbf{Y}$ it is possible to find maximum likelihood estimate of $\textbf{X}$. 
By maximising the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ an estimate of $\mathbf{X}$ can be achieved in the case of more sensors than sources, $M > N$. 
But in the desired case where $M < N$ the estimation becomes complicate as the MMV model becomes under-determined and potentially an infinitely number of solutions exist with equal likelihoods.
As the optimisation problem of the MMV model is NP-hard another estimation method must be used.

Within this Bayesian framework the source matrix $\mathbf{X}$ can be seen as a variable which is drawn from some distribution $p(\mathbf{X})$ such that it is possible to narrow down the infinitely solution space. 
Assuming a prior belief that $\textbf{Y}$ is generated from a sparse coefficient matrix, that is the distribution from where $\textbf{X}$ is drawn has a sharp, possibly infinite, spike at zero surrounded by fat tails. 

The MAP estimation of the MMV can then be view as 
\begin{align*}
\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \Vert \mathbf{X} \Vert_0$. From this it it possible to view the optimization problem as a MAP estimation challenge.

 
%By applying an exponential function $\exp(- (\cdot))$ transformation onto our optimisation problem a Gaussian likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ with a $\lambda$-dependent variance is achieved:
%\begin{align*}
%p(\mathbf{Y} \vert \mathbf{X}) \propto \exp \left( - \frac{1}{\gamma} \Vert \mathbf{Y} - \mathbf{AX} \Vert_2^2 \right),
%\end{align*}
%with a prior distribution $p(\mathbf{X}) \propto \exp(- \Vert \mathbf{X} \Vert_0)$ \cite[p. 137]{phd_wipf}. 
%The optimisation problem is then rewritten by Bayers formula 
%\begin{align*}
%\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
%&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
%&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
%\end{align*}
%From this it it possible to view the optimization problem as a MAP estimation challenge.

\section{Empirical Bayesian Estimation}
Different MAP estimation approaches exists separated by the choice of sparsity inducing prior and optimization method. 
Some problems have shown to occur when using a fixed and algorithm-dependent prior as the posterior is not sparse enough if a prior is not as sparse leading to a non-recovery. 
Another issue is that a combinatorial number of suboptimal local solutions can occur.  
By use of automatic relevance determination (ARD) the problems related to the sparse prior can be avoided. 
ARD is a method where a prior is introduced to determine the relevance of a parameter. 
This prior is modulated by a vector of hyperparameters affecting the prior variance of each row in $\mathbf{X}$.
This can also be view as a regularisation of the solution space which is narrowed to consist only of relevant information \cite{ARD}.
An empirical prior can be used with ARD as the empirical prior is flexible and depends on the unknown hyperparameter $\boldsymbol{\gamma}$ and therefore more data-dependent -- such prior can be controlled to induce sparsity.

Let the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ be Gaussian, with known noise variance $\sigma^2$. 
Then for each column in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written as
\begin{align*}
p(\mathbf{y}_{j} \vert \mathbf{x}_{j}) &= \mathcal{N}(\mathbf{Ax}_{j}, \sigma^2 \mathbf{I}) \\
&= (2 \pi \sigma^2)^{-N/2} \exp \left( - \frac{1}{2 \sigma^2} \Vert \mathbf{y}_{j} - \mathbf{A} \mathbf{x}_{j} \Vert_2^2 \right).
\end{align*}
With the use of ARD the $i$-th row of the source matrix $\mathbf{X}$, $\mathbf{x}_{i \cdot}$, is assigned an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by $\gamma_i$ which is unknown:
\begin{align*}
p (\mathbf{x}_{i \cdot} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I}).
\end{align*}
By combing the row priors
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{j=1}^L p (\mathbf{x}_{i \cdot} ; \gamma_i),
\end{align*}
a full prior of $\mathbf{X}$ is achieved modulated by the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_M]^T$. 
By combining the full prior and the likelihood $p(\mathbf{y}_{j} \vert \mathbf{x}_{j})$ the posterior of the $j$-th column of the source matrix $\mathbf{X}$ becomes \todo{Er ved at finde ud af hvorfor vi kan sætte dem sammen på denne måde}
\begin{align*}
p(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) = \frac{p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{j}} = \mathcal{N}(\boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}),
\end{align*}
with mean and covariance given as
\begin{align}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \nonumber \\
\mathcal{M} &= [\boldsymbol{\mu}_{1 \cdot}, \dots, \boldsymbol{\mu}_{ L \cdot}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}, \label{eq:moments}
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$ and $\boldsymbol{\Sigma}_y = \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T$ \todo{Er det nemt at se at de to sigmaer vi bruger ikke er den samme? Den ene er en kovarianse mens den anden er en variable}. 
Let the posterior mean $\mathcal{M}$ serve as the point estimate for $\mathbf{X}$ without involving the support set $S$.
It is clear that row sparsity is achieved whenever $\gamma_i = 0$. 
From this the posterior must satisfy the following 
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
which ensure that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, will be zero. Now, instead of estimating the sparsity profile of our source matrix $\mathbf{X}$ it is sufficient to estimate the hyperparameters $\gamma_i$\cite[p. 147]{phd_wipf}.
Each different hyperparameter $\boldsymbol{\gamma}$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determination of $\boldsymbol{\gamma}$ is seen as a model selection for which an empirical Bayesian strategy can be used. 
By the empirical Bayesian strategy the unknown weights, making the source matrix $\textbf{X}$, are treated as nuisance parameters and are integrated out.
By integrating the likelihood of $\textbf{Y}$ with respect to the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$, $p (\mathbf{Y} ; \boldsymbol{\gamma})$ is achieved \cite[p. 146]{phd_wipf}. 
By applying the $-2 \log (\cdot)$ transformation the marginal likelihood function is transformed to a cost function
\begin{align*}
\ell(\boldsymbol{\gamma}) &= -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma}))\\
&= \log ( \vert \boldsymbol{\Sigma}_y \vert) + \frac{1}{L} \sum_{j=1}^L \mathbf{y}_{j}^T \boldsymbol{\Sigma}_y ^{-1} \mathbf{y}_{j}
\end{align*}
To minimise the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the evidence maximisation (EM) \todo{Tjek lige om det er evidence eller expectation} algorithm can be used. 
The E-step of the EM algorithm is to compute the posterior moments using \eqref{eq:moments} while the M-step is the following update rule of $\gamma_i$:
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, M.
\end{align*}
The M-step is very slow on large data. 
Instead one could use a fixed point update to fasten the convergence on large data, however convergence is no longer ensured. 
The fixed point updating step is achieved by taking the derivative of the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. 
This lead to the following update equation which can replace the above M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, M.
\end{align*}
Empirically this alternative update rule have shown use full in highly under-determined large scale cases by driving many hyper parameters toward zero allowing for the corresponding weight in the source matrix to be discarded. 
For simultaneous sparse approximation problems this is the process referred to as multiple sparse Bayesian learning, M-SBL.
From the resulting $\boldsymbol{\gamma}^\ast$ the support set $S$ of the source matrix $\textbf{X}$ can be extracted, 
\begin{align*}
S = \{ i \vert \hat{\gamma}_i \neq 0 \},
\end{align*}
concluding the localisation of active sources within $\textbf{X}$. 
In practise some arbitrary small threshold can be used such that that any sufficiently small hyperparameter is discarded.
For identification of the active sources the estimate of the source matrix $\textbf{X}$ is given as $\textbf{X}^\ast=\mathcal{M}^\ast \approx \textbf{X}$, with $\mathcal{M}^\ast = \mathbb{E}[\textbf{X}\vert \textbf{Y};\boldsymbol{\gamma}^\ast]$. 
This leads to the following estimate  
\begin{align*}
\mathbf{X}^\ast = 
\begin{cases}
\mathbf{x}_{i\cdot} = \boldsymbol{\mu}_{i \cdot}^\ast, & i \in S \\
\mathbf{x}_{i\cdot} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}

\begin{algorithm}[H]
\caption{M-SBL}
\begin{algorithmic}[1]
\Procedure{M-SBL}{$\textbf{Y}, \textbf{A}, $ iterations}
\State $\boldsymbol{\gamma} = \mathbf{1} \in \mathbb{R}^{\text{iterations} + 2 \times N \times 1}$
\State $k = 0$
\While{$\boldsymbol{\gamma} \geq 10^{-16}$} 
	\State $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma}^k)$
	\For{$i = 1, \dots, N$}
		\State $\boldsymbol{\Sigma} = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}$
		\State $\mathcal{M} = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}$
		\State $\gamma_i^{(k+1)} = \dfrac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}$
	\EndFor
	\If{$k =$ iterations}
		\State Break
	\EndIf
	\State $k += 1$
\EndWhile
\State Return $\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$
\EndProcedure
\Procedure{Support}{$\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$, non\_zero}
\State Support = $\mathbf{0} \in \mathbb{R}^{\text{non\_zero}}$
\State $\boldsymbol{\gamma}_{\text{value}} = \boldsymbol{\gamma}^\ast (-2)$
\For{$j$ in range(non\_zero)}	
	\If{$\boldsymbol{\gamma}_{\text{value}} (\arg \max (\boldsymbol{\gamma}_{\text{value}}))$ != $0$}
		\State Support($j$) = $\arg \max (\boldsymbol{\gamma}_{\text{value}}$)
		\State $\boldsymbol{\gamma}_{\text{value}}(\arg \max (\boldsymbol{\gamma}_{\text{value}})) = 0$
	\EndIf

\EndFor
\State $\mathbf{X} = \mathbf{0} \in \mathbb{R}^{N \times L-2}$
\For{$i$ in Support}
	\State $\mathbf{X}(i) = \mathcal{M}^\ast(-1)(i)$
\EndFor
\State Return $\mathbf{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}