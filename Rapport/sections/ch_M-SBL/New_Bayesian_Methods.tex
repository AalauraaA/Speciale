%As described in section \ref{sec:sol_met} the covariance-domain dictionary learning (Cov-DL) is not a fitted method to use in the recovering of the source matrix $\mathbf{X}$ as the found source matrix is not the true recovering. 
%Instead a different method, multiple sparse Bayesian Learning (M-SBL), is used for the recovering process of the last element of the multiple measurement model (MMV) \ref{eq:MMV_model}.
In this chapter the Multiple sparse Bayesian Learning (M-SBL) method is described in details. As the method leverage a bayesian framework the generel conpect of Bayesian inference is shortly introduced prior to the the M-SBL method, with respect to the model of interest. The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.

Consider again the MMV model for a non-segmented case of EEG-measurements
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
with measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, sources matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ and mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$. Note that $\textbf{A}$ is known throughout the chapter, as it was found by Cov-DL in chapter \ref{ch:Cov-DL}.

The aim is to recover $\textbf{X}$ in the case of fewer measurements than active sources, $k>M$. In \cite{Balkan2014} is proven that exact localization of the active sources can be achieved by M-SBL for $k>M$, when two sufficient conditions satisfied.   
\todo{skal de sidster her med ellers skal det specificeres en lille smule}The basic approach is to find the support set $S$ of $\textbf{X}$ providing the non-zeros rows of $\mathbf{X}$ which corresponds to localization of the active sources. The support set $S$ is constructed from a minimisation of a log-likelihood function defined from Bayesian framework.

\section{Bayesian Inference}
Bayesian probability differs from the classical probability(\textit{the frequentist, relative
frequency with which A occurs}) by interpreting probability as reasonable expectation. 
In general the Bayesian framework builds upon the task of inferring what the model parameters most be, given the model and data.  
%%% jeg forstår ikke helt sammenhængen her
%From Bayesian inference theory a posterior distribution is constructed to predict a new distribution of an unknown data sample such that a distribution of the unknown samples are returned. The optimum sample estimate of a parameter -- for the posterior distribution -- can be found by e.g. maximum a posterior (MAP) estimation \todo{Kilde (WIKI): This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the variance of the predictive distribution}.
%%%
This is centred around Bayes' theorem from which the posterior distribution of some unobserved variables $\textbf{C}$ given some observed variables $\textbf{B}$
\begin{align*}
p(\textbf{C}|\textbf{B})=\frac{p(\textbf{B}|\textbf{C})p(\textbf{C})}{p(\textbf{B})}
\end{align*}  
where $p(\textbf{B}|\textbf{C})$ is the probability density function of $\text{B}$ given $\text{C}$, also referred to as the likelihood function of $\text{C}$(?), $p(\text{C})$ is a prior distribution of $\text{C}$ and $p(\text{B})$ is the distribution of $\text{B}$ serving as a normalizing parameter.  
By maximizing the posterior distribution with respect to $\textbf{C}$ the maximum a posteriori (MAP) estimate is achieved. 
\begin{align*}
\textbf{C}_{MAP} = \arg \max_{\mathbf{C}} \frac{p(\mathbf{B} \vert \mathbf{C}) p(\mathbf{C})}{p(\mathbf{B)}}
\end{align*}
That is the estimate of $\textbf{C}$ with the highest probability of causing the given variables $\textbf{B}$\todo{evt. noget mere over gang her}. 
%% go to our case 

With the basic concept of the frameworks settled, consider now the current MMV model, where we want to estimate the model parameter/source matrix $\textbf{X}$ given the measurements $\textbf{Y}$.   
  
%%%%%% kan dette pilles ud?
%With the knowledge of $\mathbf{A}$ and $\mathbf{Y}$ it is possible to find maximum likelihood estimate of $\textbf{X}$. 
%By maximising the likelihood $p(\mathbf{Y} \vert \mathbf{X})$ an estimate of $\mathbf{X}$ can be achieved in the case of more sensors than sources, $M > N$. 
%But in the desired case where $M < N$ the estimation becomes complicate as the MMV model becomes under-determined and potentially an infinitely number of solutions exist with equal likelihoods.
%As the optimisation problem of the MMV model is NP-hard another estimation method must be used.
%%%%%%

In the desired case where $M < N$ the MMV model makes an under-determined system and potentially an infinitely number of solutions exist with equal likelihoods.  
Within this Bayesian framework the source matrix $\mathbf{X}$ can be seen as a variable which is drawn from some distribution $p(\mathbf{X})$, as such it is possible to narrow down the infinitely solution space. 
Assuming a prior belief that $\textbf{Y}$ is generated from a sparse source matrix, gives a so-called sparsity inducing prior. That is $\textbf{X}$ is drawn from a distribution which has a sharp, possibly infinite, spike at zero surrounded by fat tails.
Now estimation of $\textbf{X}$ from the MMV model can be view as the following MAP estimation task\cite[p. 14]{phd_wipf} 
%\begin{align*}
%\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
%&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
%&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
%\end{align*}
\begin{align*}
\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}}.
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \exp \left( - \Vert \mathbf{X} \Vert_0\right)$\todo{vi skal da have exp her ikke, jo støtte 0-norm for mindre sandsynlighed, og kan vi nøjes med den ene linje ovenfor?}.

Different MAP estimation approaches exists separated by the choice of sparsity inducing prior and optimization method. 
However, regardless of the approach some problems have shown to occur when using a fixed and algorithm-dependent prior. One issue is the posterior not being sparse enough if a prior is not as sparse, leading to non-recovery. 
Another issue is that a combinatorial number of suboptimal local solutions can occur.
By use of automatic relevance determination (ARD) the problems related to the fixed sparse prior can be avoided\cite[p. 20]{phd_wipf}. 
ARD is a method where a prior is introduced to determine the relevance of a parameter\todo{tjek op på parameter/variable?}. The main asset of this alternative approach is the use of an empirical prior. That is an flexible prior distribution which depends on an unknown set of hyperparameters, which is to be learned from the data
\todo{citat: p. 9(pdf) sparse Bayesian learning (SBL) is an empirical Bayesian approaches, which use a parameterized prior to encourage sparsity through a process called evidence maximization}. 
\subsection{Empirical Bayesian Estimation}
%% nødvendigt i forhold til ovenstående?
%This prior is modulated by a vector of hyperparameters affecting the prior variance of each row in $\mathbf{X}$.
%This can also be view as a regularisation of the solution space which is narrowed to consist only of relevant information \cite{ARD}.
%An empirical prior can be used with ARD as the empirical prior is flexible and depends on the unknown hyperparameter $\boldsymbol{\gamma}$ and therefore more data-dependent -- such prior can be controlled to induce sparsity.
Assume the likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ is Gaussian, with known noise variance $\sigma^2$\todo{skal dette begrundes? det stammer fra det ordiginale optimeringsproblem recast in bayesian terms (I.10) i pdf -- omformuler til at vi vælger at antage dette.}. 
Then for each column in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written\todo{hvordan bliver variancen matricen bar en scalar?} as
\begin{align*}
p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{\cdot j}, \sigma^2 \mathbf{I}) \\
&= (2 \pi)^{-\frac{M}{2}}|\sigma^2\mathbf{I}|^{-\frac{1}{2}}\exp \left( -\frac{1}{2 \sigma^2\mathbf{I}} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right)\\
&= (2 \pi \sigma^2)^{-M/2} \exp \left( - \frac{1}{2 \sigma^2} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right).
\end{align*}   
The empirical prior is now defined by application of ARD. 
The $i$-th row of the source matrix $\mathbf{x}_{i \cdot}$, has an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by an unknown hyperparameter $\gamma_i$:
\begin{align*}
p (\mathbf{x}_{i \cdot} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I}).
\end{align*}
\textit{(one hole row is controlled by the same hyperparameter)}
By combing the row priors
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{i=1}^N p (\mathbf{x}_{i \cdot} ; \gamma_i),
\end{align*}
a full prior of $\mathbf{X}$ is achieved modulated by the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_N]^T$. 
By combining the full prior and the likelihood $p(\mathbf{y}_{j} \vert \mathbf{x}_{j})$ the posterior of the $j$-th column of the source matrix $\mathbf{X}$ becomes\todo{udledning okay? sidste linje er fra kilden}
\begin{align}
p(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) &= \frac{p(\textbf{y}_{\cdot j} | \textbf{x}_{\cdot j};\gamma)p(\textbf{x}_{\cdot j};\gamma)}{p(\textbf{y}_{\cdot j}|\gamma)}
 \nonumber \\
&= \frac{p(\textbf{y}_{\cdot j} | \textbf{x}_{\cdot j};\gamma)p(\textbf{x}_{\cdot j};\gamma)}{\int p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j})p(\mathbf{x}_{\cdot j};\gamma) \ d \mathbf{x}_{j}}
\nonumber \\ 
&= \frac{p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{j}, \mathbf{y}_{j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{j}} \nonumber \\ 
&= \mathcal{N}(\boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}),\label{eq:bay}
\end{align}
where the denominator is the marginal likelihood of $\textbf{y}_{\cdot j}$ also referred to as the evidence, this is elaborated in the next section. The mean and covariance of \eqref{eq:bay} is given as
\begin{align}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{j} \vert \mathbf{y}_{j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \nonumber \\
\mathcal{M} &= [\boldsymbol{\mu}_{1 \cdot}, \dots, \boldsymbol{\mu}_{ L \cdot}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}, \label{eq:moments}
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$ and $\boldsymbol{\Sigma}_y = \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T$. 

Let the posterior mean $\mathcal{M}$ serve as the point estimate for $\mathbf{X}$.
It is clear that row sparsity is achieved whenever $\gamma_i = 0$. 
From this the posterior must satisfy the following 
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
this ensures that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, become zero, whenever $\gamma_i = 0$ as desired.

From this it is evident that for estimating the support set of $\textbf{X}$ it is sufficient to estimate the hyperparameter $\boldsymbol{\gamma}$, from which the support set can be extracted. Further the point estimate is of $\textbf{X}$, providing the source signal estimate, is given by $\mathcal{M}$\cite[p. 147]{phd_wipf}. 
This leads to the actual M-SBL algorithm for which the aim is to estimate $\boldsymbol{\gamma}$ and the corresponding $\mathcal{M}$.

\section{M-SBL for estimation of $\textbf{X}$}
Different hyperparameters $\boldsymbol{\gamma}$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determination of $\boldsymbol{\gamma}$ is seen as a model selection.
Due to the empirical Bayesian strategy the unknown weights, making the source matrix $\textbf{X}$ are integrated out as follows.
By integrating the likelihood of $\textbf{Y}$ with respect to the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$ is achieved as $p (\mathbf{Y} ; \boldsymbol{\gamma})$ \cite[p. 146]{phd_wipf}. 
The resulting marginal likelihood is to be maximised with respect to $\boldsymbol{\gamma}$, that is the resulting ARD based M-SBL cost function.
The $-2 \log (\cdot)$ transformation is applied in order to the cost function to be minimized
\begin{align*}
\ell(\boldsymbol{\gamma}) &= -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma}))\\
&= \log ( \vert \boldsymbol{\Sigma}_y \vert) + \frac{1}{L} \sum_{j=1}^L \mathbf{y}_{j}^T \boldsymbol{\Sigma}_y ^{-1} \mathbf{y}_{j}
\end{align*}
To minimise the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the expectation maximisation (EM) algorithm can be used. 
The E-step of the EM algorithm is to compute the posterior moments using \eqref{eq:moments} while the M-step is the following update rule of $\gamma_i$\cite[p.147]{phd_wipf}
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, N.
\end{align*}
The M-step is very slow on large data. 
Instead one could use a fixed point update to fasten the convergence on large data, however convergence is no longer ensured. 
The fixed point updating step is achieved by taking the derivative of the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. 
This lead to the following update equation which can replace the above M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, N.
\end{align*}
Empirically this alternative update rule have shown use full in highly under-determined large scale cases by driving many hyper parameters toward zero allowing for the corresponding weight in the source matrix to be discarded. 
For simultaneous sparse approximation problems this is the process referred to as multiple sparse Bayesian learning, M-SBL.

From the resulting $\boldsymbol{\gamma}^\ast$ the support set $S$ of the source matrix $\textbf{X}$ can be extracted, 
\begin{align*}
S = \{ i \vert \hat{\gamma}_i \neq 0 \},
\end{align*}
concluding the localisation of active sources within $\textbf{X}$. 
In practise some arbitrary small threshold can be used such that that any sufficiently small hyperparameter is discarded.
For identification of the active sources the estimate of the source matrix $\textbf{X}$ is given as $\textbf{X}^\ast=\mathcal{M}^\ast \approx \textbf{X}$, with $\mathcal{M}^\ast = \mathbb{E}[\textbf{X}\vert \textbf{Y};\boldsymbol{\gamma}^\ast]$. 
This leads to the following estimate  
\begin{align*}
\mathbf{X}^\ast = 
\begin{cases}
\mathbf{x}_{i\cdot} = \boldsymbol{\mu}_{i \cdot}^\ast, & i \in S \\
\mathbf{x}_{i\cdot} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}

\subsection{psudo code for the M-SBL algorithm}

\begin{algorithm}[H]
\caption{M-SBL}
\begin{algorithmic}[1]
\Procedure{M-SBL}{$\textbf{Y}, \textbf{A}, $ iterations}
\State $\boldsymbol{\gamma} = \mathbf{1} \in \mathbb{R}^{\text{iterations} + 2 \times N \times 1}$
\State $k = 0$
\While{$\boldsymbol{\gamma} \geq 10^{-16}$} 
	\State $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma}^k)$
	\For{$i = 1, \dots, N$}
		\State $\boldsymbol{\Sigma} = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}$
		\State $\mathcal{M} = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}$
		\State $\gamma_i^{(k+1)} = \dfrac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}$
	\EndFor
	\If{$k =$ iterations}
		\State Break
	\EndIf
	\State $k += 1$
\EndWhile
\State Return $\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$
\EndProcedure
\Procedure{Support}{$\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$, non\_zero}
\State Support = $\mathbf{0} \in \mathbb{R}^{\text{non\_zero}}$
\State $\boldsymbol{\gamma}_{\text{value}} = \boldsymbol{\gamma}^\ast (-2)$
\For{$j$ in range(non\_zero)}	
	\If{$\boldsymbol{\gamma}_{\text{value}} (\arg \max (\boldsymbol{\gamma}_{\text{value}}))$ != $0$}
		\State Support($j$) = $\arg \max (\boldsymbol{\gamma}_{\text{value}}$)
		\State $\boldsymbol{\gamma}_{\text{value}}(\arg \max (\boldsymbol{\gamma}_{\text{value}})) = 0$
	\EndIf

\EndFor
\State $\mathbf{X} = \mathbf{0} \in \mathbb{R}^{N \times L-2}$
\For{$i$ in Support}
	\State $\mathbf{X}(i) = \mathcal{M}^\ast(-1)(i)$
\EndFor
\State Return $\mathbf{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Sufficient Conditions for Exact Source Localization}
In \cite{Balkan2014} it is proven that exact source localization is guaranteed in the under-determined case, $k>M$ when the following conditions on $\textbf{A}$ is fulfilled.
The theorem is based on a theoretical analysis of the minima where noise less conditions are considered, that is letting $\sigma^2\rightarrow 0$. Thus the following theorem applies to the noise less case.   
First, defined a function $f:\mathbb{R}^{M\times N} \rightarrow \mathbb{R}^{\frac{M(M+1)}{2}\times N}$, such that for $B = f(\textbf{A})$ the $i$-th column is given as $\textbf{b}_i = vec(\textbf{a}_i\textbf{a}_i^T)$. Here the function $\text{vec}(\cdot)$ corresponds to the function defined in section \ref{sec:cov}, being a vectorization of the lower triangular part of a matrix.	
\begin{theorem}
Given a dictionary matrix $\textbf{A}$ and a set of observed measurement $\textbf{Y}$, M-SBL recovers the support set of any size $k$ exactly in the noisefree case, if the following conditions are satisfied. 
\begin{enumerate}
\item The active sources $\textbf{X}_S$ are orthogonal. That is, $\textbf{X}_S\textbf{X}_S^T = \Lambda$, where $\Lambda$ is a diagonal matrix.
\item $\text{Rank}(f(\textbf{A}))= N$
\end{enumerate}
For the proof there is referred to \cite[p. 16]{Balkan2014}.



\end{theorem}  

