%As described in section \ref{sec:sol_met} the covariance-domain dictionary learning (Cov-DL) is not a fitted method to use in the recovering of the source matrix $\mathbf{X}$ as the found source matrix is not the true recovering. 
%Instead a different method, multiple sparse Bayesian Learning (M-SBL), is used for the recovering process of the last element of the multiple measurement model (MMV) \ref{eq:MMV_model}.
In this chapter the multiple sparse Bayesian learning (M-SBL) method is described in details.\todo{citat: p. 9(pdf) sparse Bayesian learning (SBL) is an empirical Bayesian approaches, which use a parameterized prior to encourage sparsity through a process called evidence maximization} As the method leverage a Bayesian framework the general concept of Bayesian inference is shortly introduced prior to the the M-SBL method, with respect to the model of interest \eqref{eq:MSBL_MMV}. The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.

Consider again the multiple measurement vector (MMV) model for a non-segmented case of EEG measurements
\begin{align}\label{eq:MSBL_MMV}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align}
with measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and noise matrix $\mathbf{E} \in \mathbb{R}^{M \times L}$. Note that $\mathbf{A}$ is known throughout the chapter, as it is found by Cov-DL in chapter \ref{ch:Cov-DL}.

The aim is to recover the source matrix $\mathbf{X}$ in the case of fewer measurements than active sources, $k > M$. 
In \cite{Balkan2014} it is proven that exact localization of the active sources can be achieved with M-SBL for $k > M$, when two sufficient conditions are satisfied.   
The basic approach of M-SBL is to find the support set $S$ providing the non-zeros rows of the source matrix $\mathbf{X}$ which corresponds to localization of the active sources. Finally the value of the localized active sources are estimated.  

\section{Bayesian Inference} 
General Bayesian statistics builds upon the task of inferring what the model parameters must be, given the model and data.  
This is centred around Bayes' theorem, which is a posterior distribution of some unobserved variable given some observed variable.

Consider now the current non-segmented MMV model \eqref{eq:MSBL_MMV} within the Bayesian framework, the model parameter -- the source matrix $\textbf{X}$ -- is wished estimated given the measurement matrix $\textbf{Y}$.   
Bayes' theorem becomes 
\begin{align*}
p(\mathbf{X}|\mathbf{Y}) = \frac{p(\mathbf{Y}|\mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y})},
\end{align*}  
where $p(\mathbf{Y}|\mathbf{X})$ is the probability density function of $\mathbf{Y}$ given $\mathbf{X}$, also referred to as a likelihood function, $p(\mathbf{X})$ is a prior distribution of $\mathbf{X}$ and $p(\mathbf{Y})$ is the distribution of $\mathbf{Y}$ serving as a normalizing parameter.
By maximizing the posterior distribution $p(\mathbf{X}|\mathbf{Y})$ with respect to $\mathbf{X}$, the maximum a posteriori (MAP) estimate, an estimate for the source matrix can be found as
\begin{align*}
\hat{\mathbf{X}}_{\text{MAP}} &= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}}.
\end{align*}
That is the estimate of $\mathbf{X}$, from the MMV model \eqref{eq:MSBL_MMV}, with the highest probability of causing the given variable $\mathbf{Y}$\todo{hvordan er dette ikke korrekt? 'rasmus'}. In the desired case where $M < N$ the MMV model \eqref{eq:MSBL_MMV} makes an under-determined system and potentially an infinitely number of solutions exist with equal likelihoods.  

Let the source matrix $\mathbf{X}$ be seen as a variable which is drawn from some distribution $p(\mathbf{X})$, as such it is possible to narrow down the infinitely solution space. 
Assuming a prior belief that $\mathbf{Y}$ is generated from a sparse source matrix, gives a so-called sparsity inducing prior. 
That is $\mathbf{X}$ is drawn from some distribution which has a sharp, possibly infinite, spike at zero surrounded by fat tails.
A specific example could be a prior distribution $p(\mathbf{X}) \propto \exp \left( - \Vert \mathbf{X} \Vert_0\right)$ \cite[p. 14]{phd_wipf}. 
However, for simplicity a Gaussian prior is to prefer. 
The use of a Gaussian distribution can (almost) be justified if a mixture of two Gaussian distributions are considered such that the variable is drawn from one of the two with equal likelihood. 
One where the variance of the distribution is close to zero, resembling the narrow spike around the mean at zero. 
And, one with high variance resembling the fat tails\todo{giver det mening med den mixture?}.       

Different MAP estimation approaches exists separated by the choice of sparsity inducing prior and optimization method. 
However, regardless of the approach some problems have shown to occur when using a fixed and algorithm-dependent prior. 
One issue is the posterior not being sparse enough if a prior is not as sparse, leading to non-recovery. 
Another issue is that a combinatorial number of suboptimal local solutions can occur.
By use of automatic relevance determination (ARD) the problems related to the fixed sparse prior can be avoided \cite[p. 20]{phd_wipf}. 
The main asset of this alternative approach is the use of an empirical prior. 
That is an flexible prior distribution which depends on an unknown set of hyperparameters, which is to be learned from the data.
 
\subsection{Empirical Bayesian Estimation}\label{seg:EBE}
Assume the likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ is Gaussian, with known noise variance $\sigma^2$.
Due to $\textbf{Y}$ containing samples from multiple sensors over time each entry of $\textbf{Y}$ are independent and equally distributed with likelihood
\begin{align*}
p(y_{ij}\vert x_{ij}) &\sim \mathcal{N}(\mathbf{A}_{i \cdot}\textbf{x}_{\cdot j}, \sigma^2) \\
& = \frac{1}{\sigma^2\sqrt{2\pi}}\exp\left( -\frac{1}{2}\left( \frac{y_{ij}- \mathbf{A}_{i \cdot}\textbf{x}_{\cdot j}}{\sigma}\right)^{2}\right)
\end{align*}
\textit{Tidligere definition a likelihood}
\begin{align*}
p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{\cdot j}, \sigma^2 \mathbf{I}) \\
 &= (2 \pi)^{-\frac{M}{2}}|\sigma^2\mathbf{I}|^{-\frac{1}{2}}\exp \left( -\frac{1}{2 \sigma^2\mathbf{I}} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right)
\end{align*}
Now the empirical prior is defined by application of ARD. Similar to the entries of $\textbf{Y}$ each parameter $x_{ij}$ are independent and equally distributed by a Gaussian distribution with zero mean and a variance controlled by an unknown hyperparameter $\gamma_i$:
\begin{align*}
p (x_{i j} ; \gamma_i) &\sim \mathcal{N}(0, \gamma_i).
\end{align*}
Note that every entry of row $i$ is controlled by the same hyperparameter $\gamma_i$, that is one source signal over time is controlled by one hyperparameter. 
By combining the prior of each parameter, the prior of $\textbf{X}$ is fully specified as follows 
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{i=1}^N p (\mathbf{x}_{i \cdot} ; \gamma_i),
\end{align*}
with the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_N]^T$. Note that one column of the unknown sources $\textbf{x}_{\cdot j}$ depends on the $\boldsymbol{\gamma}$. The prior can be composed as 
\begin{align*}
p\left(\textbf{x}_{\cdot j};\boldsymbol{\gamma}\right) = \prod_{i=1}^{N}p\left( x_{ij};\gamma_{i}\right).
\end{align*} 
Combining the prior and the likelihood $p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j})$ \todo{giver det mening? hvis 5.2 skulle skrive om så skal det hele være produkter ikke?} the posterior of the $j$-th column of the source matrix $\mathbf{X}$ becomes
\begin{align}
p(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) &= \frac{p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma})}{p(\mathbf{y}_{\cdot j} | \boldsymbol{\gamma})} \nonumber \\
&= \frac{p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} \nonumber \\ 
&\propto p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma})\\
&\sim \mathcal{N}(\boldsymbol{\mu}_{\cdot j}, \boldsymbol{\Sigma}),\label{eq:bay}
\end{align}
%&= \frac{p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} \nonumber \\ 
where the denominator is the marginal likelihood of $\mathbf{y}_{\cdot j}$ also referred to as the evidence. The marginalization is elaborated in the next section. The mean and covariance of \eqref{eq:bay} is given as
\begin{align}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \label{eq:moments1} \\
\mathcal{M} &= [\boldsymbol{\mu}_{\cdot 1}, \dots, \boldsymbol{\mu}_{\cdot L}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}, \label{eq:moments2} 
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$ and $\boldsymbol{\Sigma}_y = \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T$. The derivation of the posterior mean and covariance if found in appendix \ref{app_sec:mean_cov}.   

Let the posterior mean $\mathcal{M}$ serve as the point estimate for the source matrix $\mathbf{X}$.
It is clear that row sparsity is achieved whenever $\gamma_i = 0$. 
From this the posterior must satisfy the following 
\begin{align*}
\mathbb{P}(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1.
\end{align*}
This ensures that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, become zero, whenever $\gamma_i = 0$ as desired.

From this it is evident that for estimating the support set of $\mathbf{X}$ it is sufficient to estimate the hyperparameter $\boldsymbol{\gamma}$, from which the support set $S$ can be extracted. Furthermore, the point estimate of $\mathbf{X}$, providing the source signal estimate, is given by $\mathcal{M}$ \cite[p. 147]{phd_wipf}. 
This leads to the actual M-SBL algorithm for which the aim is to estimate $\boldsymbol{\gamma}$ and the corresponding $\mathcal{M}$.
\input{sections/ch_M-SBL/extra_derivation.tex}

\section{M-SBL for estimation of $\textbf{X}$}\label{seg:M_sblalg}
The M-SBL algorithm is now specified in order to estimate the hyperparameter $\boldsymbol{\gamma}$ and then the corresponding unknown sources $\textbf{X}$.
%The specific support of the hyperparameter $\boldsymbol{\gamma}$ can be seen as one hypotheses for the prior distribution of the underlying generation of $\mathbf{Y}$. 
%Hence the determination of $\boldsymbol{\gamma}$ is seen as a model selection.
Due to the empirical Bayesian strategy the unknown variables, making the source matrix $\mathbf{X}$ are integrated out, also referred to as marginalized.
By integrating the posterior with respect to the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$ is achieved \cite[p. 146]{phd_wipf} 
\begin{align*}
\mathcal{L}(\boldsymbol{\gamma};\textbf{Y}) &= \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \\
&= p (\mathbf{Y} \vert \boldsymbol{\gamma})
\end{align*}
\todo{when to use ; instead of $\vert$}The resulting marginal likelihood of $\boldsymbol{\gamma}$ is to be maximised with respect to $\boldsymbol{\gamma}$, that is the maximum likelihood estimate (MLE) which is considered the resulting ARD based M-SBL cost function. 
The $-2 \log (\cdot)$ transformation is applied in order for the cost function to be minimized, and factors not depending on $\textbf{Y}$ is removed. Resulting in the following log likelihood.\todo{ops på L faktor i sidste linje.. } 
\begin{align}
\ell(\boldsymbol{\gamma};\textbf{Y})&= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma}))\nonumber \\ 
&= -2\log \left( 2\pi^{\frac{M}{2}}\vert \boldsymbol{\Sigma}_{y}\vert^{\frac{1}{2}}\exp \left( - \frac{1}{2} \sum_{j=1}^L \textbf{y}_{\cdot j}^T \boldsymbol{\Sigma}_{y}^{-1} \textbf{y}_{\cdot j} \right) \right)\nonumber \\
&= L \log ( \vert \boldsymbol{\Sigma}_y \vert ) + \sum_{j=1}^L \mathbf{y}_{\cdot j}^T \boldsymbol{\Sigma}_y ^{-1} \mathbf{y}_{\cdot j}.\label{eq:likelihood}
\end{align}
It is not expected that an explicit solution to the minimization problem can be found by differentiating and letting the expression equal to zero, hence it has to be solved iteratively based on a initial parameter guess $\boldsymbol{\gamma}^{(0)}$.
One iterative method is the expectation maximisation (EM) algorithm.
In general each iteration consist of an expectation (E) step, where a function determines the expectation of the likelihood function given the currently estimated parameters. The E-step is followed by an maximization (M) step which computes the parameters by maximizing the expected likelihood found in the E-step.
In this case the E-step is to compute the posterior moments using \eqref{eq:moments1} and \eqref{eq:moments2} while the M-step is the following update rule of $\gamma_i$ \cite[p.147]{phd_wipf}
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \boldsymbol{\Sigma}_{ii}, \quad \forall i = 1, \dots, N.
\end{align*}
The M-step is in general very slow on large data. 
An alternative is to use a fixed point update rule to fasten convergence on large data, however convergence is no longer ensured. 
The fixed point updating step is achieved by taking the derivative of the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. 
This lead to the following update rule which can replace the above M-step in the EM-algorithm
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \boldsymbol{\Sigma}_{ii}}, \quad \forall i = 1, \dots, N.
\end{align*}
\todo{mangler at udlede denne regl}
Empirically this alternative update rule have shown use full in highly under-determined large scale cases by driving many hyper parameters toward zero allowing for the corresponding weight in the source matrix to be discarded. 
For simultaneous sparse approximation problems this is the process referred to as multiple sparse Bayesian learning, M-SBL.

From the resulting $\boldsymbol{\gamma}^\ast$ the support set $S$ of the source matrix $\mathbf{X}$ can be extracted, 
\begin{align*}
S = \{ i \vert \hat{\gamma}_i \neq 0 \},
\end{align*}
concluding the localization of active sources within $\mathbf{X}$. 
In practise some arbitrary small threshold can be used such that that any sufficiently small hyperparameter is discarded.
For identification of the active sources the estimate of the source matrix $\mathbf{X}$ is given as $\mathbf{X}^\ast = \mathcal{M}^\ast $, with $\mathcal{M}^\ast = \mathbb{E}[\mathbf{X}\vert \mathbf{Y} ; \boldsymbol{\gamma}^\ast]$. 
This leads to the following estimate  
\begin{align*}
\mathbf{X}^\ast = 
\begin{cases}
\mathbf{x}_{i\cdot} = \boldsymbol{\mu}_{i \cdot}^\ast, & i \in S \\
\mathbf{x}_{i\cdot} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}

\subsection{Pseudo Code for the M-SBL Algorithm}
\begin{algorithm}[H]
\caption{M-SBL}
\begin{algorithmic}[1]
\Procedure{M-SBL}{$\textbf{Y}, \textbf{A}, $ iterations}
\State $\boldsymbol{\gamma} = \mathbf{1} \in \mathbb{R}^{\text{iterations} + 2 \times N \times 1}$
\State iter $= 0$
\While{$\boldsymbol{\gamma} \geq 10^{-16}$} 
	\State $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma}^\text{iter})$
	\For{$i = 1, \dots, N$}
		\State $\boldsymbol{\Sigma} = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}$
		\State $\mathcal{M} = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}$
		\State $\gamma_i^{(\text{iter} + 1)} = \dfrac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (\text{iter})} \Sigma_{ii}}$
	\EndFor
	\If{iter = iterations}
		\State Break
	\EndIf
	\State iter $+= 1$
\EndWhile
\State Return $\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$
\EndProcedure
\Procedure{Support}{$\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$, $k$}
\State Support = $\mathbf{0} \in \mathbb{R}^{k}$
\State $\boldsymbol{\gamma}_{\text{value}} = \boldsymbol{\gamma}^\ast (-2)$
\For{$j$ in range($k$)}	
	\If{$\boldsymbol{\gamma}_{\text{value}} (\arg \max (\boldsymbol{\gamma}_{\text{value}}))$ != $0$}
		\State Support($j$) = $\arg \max (\boldsymbol{\gamma}_{\text{value}}$)
		\State $\boldsymbol{\gamma}_{\text{value}}(\arg \max (\boldsymbol{\gamma}_{\text{value}})) = 0$
	\EndIf
\EndFor
\State $\mathbf{X} = \mathbf{0} \in \mathbb{R}^{N \times L-2}$
\For{$i$ in Support}
	\State $\mathbf{X}(i) = \mathcal{M}^\ast(-1)(i)$
\EndFor
\State Return $\mathbf{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Sufficient Conditions for Exact Source Localization}
In \cite{Balkan2014} it is proven that exact source localization is guaranteed in the under-determined case, $k > M$ when the following conditions on $\mathbf{A}$ is fulfilled.
The theorem is based on a theoretical analysis of the minima where noise-free conditions are considered, that is letting $\sigma^2 \rightarrow 0$. Thus the following theorem applies to the noise less case.   
First, defined a function $f:\mathbb{R}^{M \times N} \rightarrow \mathbb{R}^{\frac{M(M+1)}{2}\times N}$, such that for $B = f(\mathbf{A})$ the $j$-th column is given as $\mathbf{b}_{\cdot j} = \text{vec}(\mathbf{a}_{\cdot j}\mathbf{a}_{\cdot j}^T)$. Here the function $\text{vec}(\cdot)$ corresponds to the function defined in section \ref{sec:cov}, being a vectorization of the lower triangular part of a matrix.	
\begin{theorem}
Given a dictionary matrix $\mathbf{A}$ and a set of observed measurement $\mathbf{Y}$, M-SBL recovers the support set of any size $k$ exactly in the noise-free case, if the following conditions are satisfied. 
\begin{enumerate}
\item The active sources $\mathbf{X}_S$ are orthogonal. That is, $\mathbf{X}_S \mathbf{X}_S^T = \boldsymbol{\Lambda}$, where $\boldsymbol{\Lambda}$ is a diagonal matrix and $S$ the support set.
\item $\text{Rank}(f(\mathbf{A}))= N$.
\end{enumerate}
The proof can be found in \cite[p. 16]{Balkan2014}.


\label{th:conditions}
\end{theorem}  

