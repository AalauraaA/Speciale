%As described in section \ref{sec:sol_met} the covariance-domain dictionary learning (Cov-DL) is not a fitted method to use in the recovering of the source matrix $\mathbf{X}$ as the found source matrix is not the true recovering. 
%Instead a different method, multiple sparse Bayesian Learning (M-SBL), is used for the recovering process of the last element of the multiple measurement model (MMV) \ref{eq:MMV_model}.
In this chapter the multiple sparse Bayesian learning (M-SBL) method is described in details.\todo{citat: p. 9(pdf) sparse Bayesian learning (SBL) is an empirical Bayesian approaches, which use a parameterized prior to encourage sparsity through a process called evidence maximization} As the method leverage a Bayesian framework the generel concept of Bayesian inference is shortly introduced prior to the the M-SBL method, with respect to the model of interest \eqref{eq:MSBL_MMV}. The chapter is inspired by \cite{phd_wipf} and the articles \cite{article_wipf}, \cite{Balkan2014}.

Consider again the multiple measurement vector (MMV) model for a non-segmented case of EEG measurements
\begin{align}\label{eq:MSBL_MMV}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align}
with measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, sources matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and noise matrix $\mathbf{E} \in \mathbb{R}^{M \times L}$. Note that $\textbf{A}$ is known throughout the chapter, as it is found by Cov-DL in chapter \ref{ch:Cov-DL}.

The aim is to recover the source matrix $\mathbf{X}$ in the case of fewer measurements than active sources, $k > M$. 
In \cite{Balkan2014} it is proven that exact localization of the active sources can be achieved with M-SBL for $k > M$, when two sufficient conditions are satisfied.   
The basic approach of M-SBL is to find the support set $S$ providing the non-zeros rows of the source matrix $\mathbf{X}$ which corresponds to localization of the active sources. Finally the value of the localized active sources are estimated.  

\section{Bayesian Inference} 
In general Bayesian statistics builds upon the task of inferring what the model parameters must be, given the model and data.  
This is centred around Bayes' theorem, which is a posterior distribution of some unobserved variable given some observed variable.

Consider now the current non-segmented MMV model \eqref{eq:MSBL_MMV} within the Bayesian framework, where we want to estimate the model parameter -- the source matrix $\textbf{X}$ -- given the measurement matrix $\textbf{Y}$.   
Bayes' theorem becomes 
\begin{align*}
p(\mathbf{X}|\mathbf{Y}) = \frac{p(\mathbf{Y}|\mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y})},
\end{align*}  
where $p(\mathbf{Y}|\mathbf{X})$ is the probability density function of $\mathbf{Y}$ given $\mathbf{X}$, also referred to as a likelihood function, $p(\mathbf{X})$ is a prior distribution of $\mathbf{X}$ and $p(\mathbf{Y})$ is the distribution of $\mathbf{Y}$ serving as a normalizing parameter.
By maximizing the posterior distribution $p(\mathbf{X}|\mathbf{Y})$ with respect to $\mathbf{X}$ the maximum a posteriori (MAP) estimate of is defined  by
\begin{align*}
\hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}}.
\end{align*}
That is the estimate of $\mathbf{X}$, from the MMV model \eqref{eq:MSBL_MMV}, with the highest probability of causing the given variable $\mathbf{Y}$\todo{hvordan er dette ikke korrekt? 'rasmus'}.
In the desired case where $M < N$ the MMV model \eqref{eq:MSBL_MMV} makes an under-determined system and potentially an infinitely number of solutions exist with equal likelihoods.  
The source matrix $\mathbf{X}$ can be seen as a variable which is drawn from some distribution $p(\mathbf{X})$, as such it is possible to narrow down the infinitely solution space. 
Assuming a prior belief that $\mathbf{Y}$ is generated from a sparse source matrix, gives a so-called sparsity inducing prior. 
That is $\mathbf{X}$ is drawn from some distribution which has a sharp, possibly infinite, spike at zero surrounded by fat tails.
A specific example could be a prior distribution $p(\mathbf{X}) \propto \exp \left( - \Vert \mathbf{X} \Vert_0\right)$ \cite[p. 14]{phd_wipf}. However for simplicity a Gaussian prior is to prefer. The use of a Gaussian distribution can (almost) be justified if a mixture of two Gaussian distributions are considered which is such that the variable is drawn from one of the two with equal likelihood. One where the variance of the distribution is assumed to approach zero, resembling the narrow spike around the mean at zero. And, one with high variance resembling the fat tails\todo{giver det mening med den mixture?}.       

Different MAP estimation approaches exists separated by the choice of sparsity inducing prior and optimization method. 
However, regardless of the approach some problems have shown to occur when using a fixed and algorithm-dependent prior. 
One issue is the posterior not being sparse enough if a prior is not as sparse, leading to non-recovery. 
Another issue is that a combinatorial number of suboptimal local solutions can occur.
By use of automatic relevance determination (ARD) the problems related to the fixed sparse prior can be avoided \cite[p. 20]{phd_wipf}. 
The main asset of this alternative approach is the use of an empirical prior. 
That is an flexible prior distribution which depends on an unknown set of hyperparameters, which is to be learned from the data.
 
\subsection{Empirical Bayesian Estimation}
Assume the likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ is Gaussian, with known noise variance $\sigma^2$.
Due to $\textbf{Y}$ containing samples from multiple sensors over time each entry of $\textbf{Y}$ are independent and equally distributed with likelihood
\begin{align*}
p(y_{ij}\vert x_{ij}) &\sim \mathcal{N}(\mathbf{A}_{i \cdot}\textbf{x}_{\cdot j}, \sigma^2) \quad  \textit{tidligere } p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{\cdot j}, \sigma^2 \mathbf{I}) \\
& = \frac{1}{\sigma^2\sqrt{2\pi}}\exp\left( -\frac{1}{2}\left( \frac{y_{ij}- \mathbf{A}_{i \cdot}\textbf{x}_{\cdot j}}{\sigma}\right)^{2}\right) \quad  \textit{tidligere } &= (2 \pi)^{-\frac{M}{2}}|\sigma^2\mathbf{I}|^{-\frac{1}{2}}\exp \left( -\frac{1}{2 \sigma^2\mathbf{I}} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right)
\end{align*}
Now the empirical prior is defined by application of ARD. Similar to the entries of $\textbf{Y}$ each parameter $x_{ij}$ are independent and equally distributed by a Gaussian distribution with zero mean and a variance controlled by an unknown hyperparameter $\gamma_i$:
\begin{align*}
p (x_{i j} ; \gamma_i) &\sim \mathcal{N}(0, \gamma_i).
\end{align*}
\todo{note that the row i is controlled by the same hyperparameter $\gamma_i$, leading to the same support over time}
By combining the prior of each parameter, the prior of $\textbf{X}$ is fully specified

........
\begin{align*}
p (\mathbf{X} ; \boldsymbol{\gamma}) &= \prod_{i=1}^N p (\mathbf{x}_{i \cdot} ; \gamma_i),
\end{align*}
with the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_N]^T$. 
By combining the full prior and the likelihood $p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j})$ the posterior of the $j$-th column of the source matrix $\mathbf{X}$ becomes
\begin{align}
p(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) &= \frac{p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma})}{p(\mathbf{y}_{\cdot j} | \boldsymbol{\gamma})} \nonumber \\
&= \frac{p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{y}_{\cdot j} | \mathbf{x}_{\cdot j}) p(\mathbf{x}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} \nonumber \\ 
&= \frac{p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} \nonumber \\ 
&= \mathcal{N}(\boldsymbol{\mu}_{\cdot j}, \boldsymbol{\Sigma}),\label{eq:bay}
\end{align}
where the denominator is the marginal likelihood of $\mathbf{y}_{\cdot j}$ also referred to as the evidence, this is elaborated in the next section. The mean and covariance of \eqref{eq:bay} is given as
\begin{align}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \label{eq:moments1} \\
\mathcal{M} &= [\boldsymbol{\mu}_{\cdot 1}, \dots, \boldsymbol{\mu}_{\cdot L}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}, \label{eq:moments2} 
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$ and $\boldsymbol{\Sigma}_y = \sigma^2 \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T$. \todo{uddyb udledning af udtrykkende her}

Let the posterior mean $\mathcal{M}$ serve as the point estimate for the source matrix $\mathbf{X}$.
It is clear that row sparsity is achieved whenever $\gamma_i = 0$. 
From this the posterior must satisfy the following 
\begin{align*}
\mathbb{P}(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
this ensures that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, become zero, whenever $\gamma_i = 0$ as desired.

From this it is evident that for estimating the support set of $\mathbf{X}$ it is sufficient to estimate the hyperparameter $\boldsymbol{\gamma}$, from which the support set $S$ can be extracted. Furthermore, the point estimate of $\mathbf{X}$, providing the source signal estimate, is given by $\mathcal{M}$ \cite[p. 147]{phd_wipf}. 
This leads to the actual M-SBL algorithm for which the aim is to estimate $\boldsymbol{\gamma}$ and the corresponding $\mathcal{M}$.

\section{M-SBL for estimation of $\textbf{X}$}
Different hyperparameters $\boldsymbol{\gamma}$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. 
Therefore the determination of $\boldsymbol{\gamma}$ is seen as a model selection.
Due to the empirical Bayesian strategy the unknown variables, making the source matrix $\mathbf{X}$ are integrated out as follows.
By integrating the likelihood of $\mathbf{Y}$ with respect to the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$ is achieved as $p (\mathbf{Y} ; \boldsymbol{\gamma})$ \cite[p. 146]{phd_wipf}. 
The resulting marginal likelihood is to be maximised with respect to $\boldsymbol{\gamma}$, that is the resulting ARD based M-SBL cost function.
The $-2 \log (\cdot)$ transformation is applied in order to the cost function to be minimized
\begin{align*}
\ell(\boldsymbol{\gamma}) &= -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma}))\\
&= \log ( \vert \boldsymbol{\Sigma}_y \vert) + \frac{1}{L} \sum_{j=1}^L \mathbf{y}_{\cdot j}^T \boldsymbol{\Sigma}_y ^{-1} \mathbf{y}_{\cdot j}.
\end{align*}
To minimise the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the expectation maximisation (EM) algorithm can be used. 
The E-step of the EM algorithm is to compute the posterior moments using \eqref{eq:moments1} and \eqref{eq:moments2} while the M-step is the following update rule of $\gamma_i$ \cite[p.147]{phd_wipf}
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \boldsymbol{\Sigma}_{ii}, \quad \forall i = 1, \dots, N.
\end{align*}
The M-step is very slow on large data. 
Instead one could use a fixed point update to fasten the convergence on large data, however convergence is no longer ensured. 
The fixed point updating step is achieved by taking the derivative of the marginal log likelihood $\ell(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. 
This lead to the following update equation which can replace the above M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \boldsymbol{\Sigma}_{ii}}, \quad \forall i = 1, \dots, N.
\end{align*}
Empirically this alternative update rule have shown use full in highly under-determined large scale cases by driving many hyper parameters toward zero allowing for the corresponding weight in the source matrix to be discarded. 
For simultaneous sparse approximation problems this is the process referred to as multiple sparse Bayesian learning, M-SBL.

From the resulting $\boldsymbol{\gamma}^\ast$ the support set $S$ of the source matrix $\mathbf{X}$ can be extracted, 
\begin{align*}
S = \{ i \vert \hat{\gamma}_i \neq 0 \},
\end{align*}
concluding the localization of active sources within $\mathbf{X}$. 
In practise some arbitrary small threshold can be used such that that any sufficiently small hyperparameter is discarded.
For identification of the active sources the estimate of the source matrix $\mathbf{X}$ is given as $\mathbf{X}^\ast = \mathcal{M}^\ast \approx \mathbf{X}$, with $\mathcal{M}^\ast = \mathbb{E}[\mathbf{X}\vert \mathbf{Y} ; \boldsymbol{\gamma}^\ast]$. 
This leads to the following estimate  
\begin{align*}
\mathbf{X}^\ast = 
\begin{cases}
\mathbf{x}_{i\cdot} = \boldsymbol{\mu}_{i \cdot}^\ast, & i \in S \\
\mathbf{x}_{i\cdot} = \mathbf{0}, & i \not \in S
\end{cases}
\end{align*}

\subsection{Pseudo Code for the M-SBL Algorithm}
\begin{algorithm}[H]
\caption{M-SBL}
\begin{algorithmic}[1]
\Procedure{M-SBL}{$\textbf{Y}, \textbf{A}, $ iterations}
\State $\boldsymbol{\gamma} = \mathbf{1} \in \mathbb{R}^{\text{iterations} + 2 \times N \times 1}$
\State iter $= 0$
\While{$\boldsymbol{\gamma} \geq 10^{-16}$} 
	\State $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma}^\text{iter})$
	\For{$i = 1, \dots, N$}
		\State $\boldsymbol{\Sigma} = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{A} \boldsymbol{\Gamma}$
		\State $\mathcal{M} = \boldsymbol{\Gamma} \mathbf{A}^T \boldsymbol{\Sigma}_y^{-1} \mathbf{Y}$
		\State $\gamma_i^{(\text{iter} + 1)} = \dfrac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (\text{iter})} \Sigma_{ii}}$
	\EndFor
	\If{iter = iterations}
		\State Break
	\EndIf
	\State iter $+= 1$
\EndWhile
\State Return $\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$
\EndProcedure
\Procedure{Support}{$\mathcal{M}^\ast, \boldsymbol{\gamma}^\ast$, $k$}
\State Support = $\mathbf{0} \in \mathbb{R}^{k}$
\State $\boldsymbol{\gamma}_{\text{value}} = \boldsymbol{\gamma}^\ast (-2)$
\For{$j$ in range($k$)}	
	\If{$\boldsymbol{\gamma}_{\text{value}} (\arg \max (\boldsymbol{\gamma}_{\text{value}}))$ != $0$}
		\State Support($j$) = $\arg \max (\boldsymbol{\gamma}_{\text{value}}$)
		\State $\boldsymbol{\gamma}_{\text{value}}(\arg \max (\boldsymbol{\gamma}_{\text{value}})) = 0$
	\EndIf
\EndFor
\State $\mathbf{X} = \mathbf{0} \in \mathbb{R}^{N \times L-2}$
\For{$i$ in Support}
	\State $\mathbf{X}(i) = \mathcal{M}^\ast(-1)(i)$
\EndFor
\State Return $\mathbf{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Sufficient Conditions for Exact Source Localization}
In \cite{Balkan2014} it is proven that exact source localization is guaranteed in the under-determined case, $k > M$ when the following conditions on $\mathbf{A}$ is fulfilled.
The theorem is based on a theoretical analysis of the minima where noise-free conditions are considered, that is letting $\sigma^2 \rightarrow 0$. Thus the following theorem applies to the noise less case.   
First, defined a function $f:\mathbb{R}^{M \times N} \rightarrow \mathbb{R}^{\frac{M(M+1)}{2}\times N}$, such that for $B = f(\mathbf{A})$ the $j$-th column is given as $\mathbf{b}_{\cdot j} = \text{vec}(\mathbf{a}_{\cdot j}\mathbf{a}_{\cdot j}^T)$. Here the function $\text{vec}(\cdot)$ corresponds to the function defined in section \ref{sec:cov}, being a vectorization of the lower triangular part of a matrix.	
\begin{theorem}
Given a dictionary matrix $\mathbf{A}$ and a set of observed measurement $\mathbf{Y}$, M-SBL recovers the support set of any size $k$ exactly in the noise-free case, if the following conditions are satisfied. 
\begin{enumerate}
\item The active sources $\mathbf{X}_S$ are orthogonal. That is, $\mathbf{X}_S \mathbf{X}_S^T = \boldsymbol{\Lambda}$, where $\boldsymbol{\Lambda}$ is a diagonal matrix and $S$ the support set.
\item $\text{Rank}(f(\mathbf{A}))= N$.
\end{enumerate}
The proof can be found in \cite[p. 16]{Balkan2014}.



\end{theorem}  

