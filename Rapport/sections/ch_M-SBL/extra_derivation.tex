\section{Derivation of Posterior Mean $\mathcal{M}$ and Covariance $\boldsymbol{\Sigma}$}\label{app_sec:mean_cov}
The purpose of this section is to derive the mean $\mathcal{M}$ and covariance $\boldsymbol{\Sigma}$ of the posterior distribution
\begin{align*}
p\left( \mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma} \right) \sim \mathcal{N}(\boldsymbol{\mu}_{\cdot j} , \boldsymbol{\Sigma}),
\end{align*}
from \eqref{eq:bay} in section \ref{seg:EBE}. 

Let $\mathbf{x}_{\cdot j} = \mathbf{x}$ and $\mathbf{y}_{\cdot j} = \mathbf{y}$ to ease the notation throughout the derivation. The prior and likelihood is then defined as
\begin{align*}
p(\mathbf{x} ; \boldsymbol{\gamma}) &\sim \mathcal{N}(\mathbf{0}, \boldsymbol{\gamma} \mathbf{I} ), \\
p(\mathbf{y} \vert \mathbf{x}) &\sim \mathcal{N}(\mathbf{A}\mathbf{x}, \sigma^2 \mathbf{I}).
\end{align*}
From the known SMV model of $\mathbf{y}$ the above implies
\begin{align*}
\mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e}, \quad \mathbf{e} \sim \mathcal{N}(\mathbf{0},\sigma^2 \mathbf{I}) 
\end{align*}
%Now define one Gaussian random variable  
%\begin{align*}
%\mathbf{z} = \begin{bmatrix}
%\mathbf{x} \\ 
%\mathbf{y}\vert \mathbf{x} 
%\end{bmatrix} \in \mathbb{R}^{N + M}.	
%\end{align*}  
%The mean and covariance of the Gaussian variable $\mathbf{z}$ can then be partitioned into 
%\begin{align*}
%\boldsymbol{\Sigma}_{\mathbf{z}} = 
%\begin{bmatrix}
%\boldsymbol{\Sigma}_{\mathbf{x}\mathbf{x}} & \boldsymbol{\Sigma}_{\mathbf{x}\mathbf{y}} \\ 
%\boldsymbol{\Sigma}_{\mathbf{y}\mathbf{x}} & \boldsymbol{\Sigma}_{\mathbf{y}\mathbf{y}}
%\end{bmatrix} 
%\in \mathbb{R}^{N+M \times N+M}
%\end{align*}
%and
%\begin{align*}
%\boldsymbol{\mu}_{\mathbf{z}} = 
%\begin{bmatrix}
%\boldsymbol{\mu}_{\mathbf{x}} \\ 
%\boldsymbol{\mu}_{\mathbf{y} \vert \mathbf{x}}
%\end{bmatrix} 
%= 
%\begin{bmatrix}
%\mathbf{0}  \\ 
%\mathbf{A} \mathbf{x}
%\end{bmatrix}.
%\end{align*}
From \cite{conditional_cov} the conditional covariance is given by
\begin{align*}
\boldsymbol{\Sigma} &= \text{cov}(\mathbf{x} , \mathbf{x} \vert \mathbf{y}) = \boldsymbol{\Sigma}_{\mathbf{xx}} - \boldsymbol{\Sigma}_{\mathbf{xy}} \boldsymbol{\Sigma}_{\mathbf{yy}}^{-1} \boldsymbol{\Sigma}_{\mathbf{yx}} \\
\boldsymbol{\mu} &= \boldsymbol{\mu}_{\mathbf{x}} + \boldsymbol{\Sigma}_{\mathbf{xy}} \boldsymbol{\Sigma}_{\mathbf{yy}}^{-1} (\mathbf{y} - \boldsymbol{\mu}_{\mathbf{y}})
\end{align*}
Each of the covariances within the expressing will now be found.

\paragraph{Covariance $\boldsymbol{\Sigma}_{\mathbf{xx}}$}
The covariance of $\mathbf{x}$ comes directly from the distribution  
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{xx}} = \boldsymbol{\gamma} \mathbf{I}
\end{align*} 

\paragraph{Covariance $\boldsymbol{\Sigma}_{\mathbf{xy}}$}
The covariance between $\mathbf{x}$ and $\mathbf{y}$ is found by using the linearity of covariance and $\mathbf{y} = \mathbf{Ax} + \mathbf{e}$:
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{yx}} &= \text{cov} \left(\mathbf{x} , \mathbf{Ax} + \mathbf{e} \right) \\ 
&= \text{cov} \left(\mathbf{x} , \mathbf{Ax} \right) + \text{cov} \left(\mathbf{x} , \mathbf{e} \right) \\
&= \boldsymbol{\Sigma}_{\mathbf{xx}} \mathbf{A}^T \\
&= \boldsymbol{\gamma} \mathbf{I} \mathbf{A}^T
\end{align*}
where $\text{cov} \left(\mathbf{x} , \mathbf{e} \right) = 0$ because $\textbf{x}$ and $\textbf{e}$ are uncorrelated.

\paragraph{Covariance $\boldsymbol{\Sigma}_{\mathbf{yx}}$}
The covariance between $\mathbf{y}$ and $\mathbf{x}$ is defined by the transpose of $\boldsymbol{\Sigma}_{\mathbf{yx}}$
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{xy}} &= \left( \boldsymbol{\gamma} \mathbf{I} \mathbf{A}^T\right)^T\\
&= \textbf{A}\boldsymbol{\gamma}\mathbf{I}
\end{align*}

\paragraph{Covariance $\boldsymbol{\Sigma}_{\mathbf{yy}}$}
Lastly the covariance of $\mathbf{y}$ and $\mathbf{y}$ is similarly found using again the linearity of covariance:
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{yy}} &= \text{cov} \left(\mathbf{Ax} + \mathbf{e} , \mathbf{Ax} + \mathbf{e} \right) \\ 
&= \text{cov}\left( \mathbf{Ax},\mathbf{Ax} \right) + \text{cov}\left(\mathbf{Ax},  \mathbf{e} \right) + \text{cov}\left( \mathbf{e}, \mathbf{Ax} \right) + \text{cov}\left( \mathbf{e}, \mathbf{e} \right)\\
&= \textbf{A}\boldsymbol{\Sigma}_{\mathbf{xx}}\textbf{A}^T + \boldsymbol{\Sigma}_{\mathbf{ee}} \\
&= \textbf{A}\boldsymbol{\gamma} \mathbf{I}\textbf{A}^T + \sigma^2 \mathbf{I}
\end{align*} 
By combining all the found covariances the resulting covariance becomes 
\begin{align*}
\boldsymbol{\Sigma} = \boldsymbol{\gamma} \mathbf{I} - \boldsymbol{\gamma} \mathbf{I} \mathbf{A}^T (\textbf{A}\boldsymbol{\gamma} \mathbf{I}\textbf{A}^T + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \boldsymbol{\gamma} \mathbf{I}.
\end{align*}
The resulting mean under the assumption that $\boldsymbol{\mu}_{\mathbf{y}} = \mathbf{0}$ becomes
\begin{align*}
\boldsymbol{\mu}_{\cdot j} &= \mathbf{0} + \boldsymbol{\gamma} \mathbf{I} \mathbf{A}^T \left( \boldsymbol{\Sigma}_{\mathbf{yy}} \right)^{-1} \left(\mathbf{y} - \mathbf{0}\right) \\
&= \boldsymbol{\gamma} \mathbf{I} \mathbf{A}^T \left( \boldsymbol{\Sigma}_{\mathbf{yy}} \right)^{-1} \mathbf{y}.
\end{align*}  
