\section{Derivation of posterior  mean and covariance}\label{app_sec:mean_cov}
The purpose is here to derive the mean and covariance of the posterior distribution
\begin{align*}
p\left( \textbf{x}_{\cdot j} \vert \textbf{y}_{\cdot j}; \boldsymbol{\gamma}\right) \sim \mathcal{N}(\boldsymbol{\mu} _{\cdot j},\boldsymbol{\Sigma}).
\end{align*}
from \eqref{eq:bay} in section \ref{seg:EBE}. Let now $\textbf{x}_{\cdot j} = \textbf{x}$ and $\textbf{y}_{\cdot j} = \textbf{y}$

We have 
$$
p(\textbf{x};\boldsymbol{\gamma}) \sim \mathcal{N}(0,\boldsymbol{\gamma} \textbf{I} )
$$
and 
$$
p(\textbf{y}\vert \textbf{x}) \sim \mathcal{N}(\textbf{A}\textbf{x}, \boldsymbol{\sigma}^2\textbf{I}).
$$ 
Now define one Gaussian random variable  
$$
\textbf{z} = \begin{bmatrix}
\textbf{x} \\ 
\textbf{y}\vert \textbf{x} 
\end{bmatrix} \in \mathbb{R}^{N + M},	
$$   
then the mean and covariance of $\textbf{z}$ can be partitioned into 
$$
\boldsymbol{\Sigma}_{\textbf{z}} = \begin{bmatrix}
\boldsymbol{\Sigma}_{\textbf{x}\textbf{x}} & \boldsymbol{\Sigma}_{\textbf{x}\textbf{y}} \\ 
\boldsymbol{\Sigma}_{\textbf{y}\textbf{x}} & \boldsymbol{\Sigma}_{\textbf{y}\textbf{y}}
\end{bmatrix} \in \mathbb{R}^{N+M \times N+M}
$$
$$
\boldsymbol{\mu}_{\textbf{z}}  = \begin{bmatrix}
\boldsymbol{\mu}_{\textbf{x}}  \\ 
 \boldsymbol{\mu}_{\textbf{y}\vert \textbf{x}}
\end{bmatrix} = 
 \begin{bmatrix}
0  \\ 
\textbf{A}\textbf{x}
\end{bmatrix}.
$$
Then, from \cite{conditional_cov}, the posterior distribution of $\textbf{x}$ given $\textbf{y}$ is defined as
\begin{align*}
\boldsymbol{\Sigma} &= \text{cov}(\textbf{x},\textbf{x}\vert \textbf{y}) =  \boldsymbol{\Sigma}_{\textbf{xx}} - \boldsymbol{\Sigma}_{\textbf{xy}}\boldsymbol{\Sigma}_{\textbf{yy}}^{-1}\boldsymbol{\Sigma}_{\textbf{yx}} \\
\boldsymbol{\mu} &= \boldsymbol{\mu}_{\textbf{x}}+\boldsymbol{\Sigma}_{\textbf{xy}}\boldsymbol{\Sigma}_{\textbf{yy}}^{-1}(\textbf{y}-\boldsymbol{\mu}_{\textbf{y}})
\end{align*}
Consider first the conditional covariance. Each covariance within the expression is now found:

The covariance of $\textbf{x}$ comes directly from the distribution  
\begin{align*}
\boldsymbol{\Sigma}_{\textbf{xx}} = \boldsymbol{\gamma}\textbf{I}
\end{align*} 

The covariance between $\textbf{x}$ and $\textbf{y}\vert\textbf{x}$ is found by using the linearity of covariance and $\textbf{y}=\textbf{Ax}+\textbf{e}$.
\begin{align*}
\boldsymbol{\Sigma}_{\textbf{yx}} &= \text{cov}\left(\textbf{x},\textbf{Ax}+\textbf{e} \right)\\ 
&= \text{cov}\left(\textbf{x},\textbf{Ax}\right)+\text{cov}\left(\textbf{x},\textbf{e}\right)\\
&= \boldsymbol{\Sigma}_{\textbf{xx}}\textbf{A}^T \\
&= \boldsymbol{\gamma}\textbf{I}\textbf{A}^T
\end{align*}
where $\text{cov}\left(\textbf{X},\textbf{E}\right)=0$ because they are uncorrelated.
\todo[inline]{next Rasmus missed cov(yy). either we define if like above wiht y=Ax but is x not given then so y|x for which var in known to be sigma. or it is just cov(yy) that we should use i main, se jan rettelse også, han mener det er fint sådan her. 
if we let it be, we assume that cov(yy) is just sigmaI}
Lastly the covariance of $\textbf{y}\vert \textbf{x}$ is similarly found by the definition of conditional covariance as follows 
\begin{align*}
\boldsymbol{\Sigma}_{\textbf{yy} \vert \mathbf{x}} =  \text{cov}(\textbf{y},\textbf{y}\vert \textbf{x}) &= \boldsymbol{\Sigma}_{\textbf{yy}} - \boldsymbol{\Sigma}_{\textbf{yx}}\boldsymbol{\Sigma}_{\textbf{xx}}^{-1}\boldsymbol{\Sigma}_{\textbf{xy}} \\
&= \boldsymbol{\sigma}^2 \textbf{I} - \boldsymbol{\gamma}\textbf{I}\textbf{A}^T (\boldsymbol{\gamma}\textbf{I})^{-1} \textbf{A}\boldsymbol{\gamma}\textbf{I} \\
&= \boldsymbol{\sigma}^2 \textbf{I} - \textbf{A}\boldsymbol{\gamma}\textbf{I}\textbf{A}^T
\end{align*} 
The resulting conditional covariance becomes 
\begin{align*}
\boldsymbol{\Sigma} = \boldsymbol{\gamma}\textbf{I} - \boldsymbol{\gamma}\textbf{I}\textbf{A}^T (\boldsymbol{\Sigma}_{\textbf{yy}\vert \textbf{x}})^{-1} \textbf{A} \boldsymbol{\gamma}\textbf{I}.
\end{align*}
Consider now the mean of $\textbf{x}$ conditional on $\textbf{y}$
\begin{align*}
\boldsymbol{\mu} &= \boldsymbol{\mu}_{\textbf{x}}+\boldsymbol{\Sigma}_{\textbf{xy}}\boldsymbol{\Sigma}_{\textbf{yy}}^{-1}\left( \textbf{y}-\boldsymbol{\mu}_{\textbf{y}}\right) \\
&= 0 + \boldsymbol{\gamma}\textbf{I}\textbf{A}^T \left( \boldsymbol{\Sigma}_{\textbf{yy}}\right)^{-1}\left(\textbf{y}-\textbf{Ax}\right)
\end{align*}  
\todo{in the result we have used, the mean of y is subtracted.?  }