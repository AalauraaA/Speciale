\section{MSB}\label{sec:M-SBL}
See chapter 2 in PHD.
\\ \\
As described in earlier sections, the support set of sources is wish recovered. A way to recover the set is to use sparse bayesian learning (M-SBL) on the MMV model. To insure full recovery some sufficient condition must be applied on the dictionary matrix $\mathbf{A}$ and the sources. 
\\ \\
Lets first sketch the case. For the MMV model we assume that we have more sources than sensors $M \leq N$ and the activations inside the sources $k$ are less than the sensors $k \leq N$. At last we assume that the mixing happen instantaneous meaning that no time delay occur -- we will work in the time domain.
\\
We will look at two sufficient conditions for exact recovery of the support set $S$: orthogonality/uncorrelated of the active sources $k$ and constraint on the dictionary matrix $\mathbf{A}$.

\subsection{M-SBL Algorithm}
\begin{itemize}
\item Finding sparse solutions to our optimisation problem can be view in Bayesian framework. We can find a sparse X by finding its estimates with maximum a posterior (MAP) estimation with use of a fixed and sparsity induced prior. This is also called empirical Bayesian with used a parameterized prior to encourage sparsity.
\item With a global SBL minimum we always achieve a maximal sparse solution.
\item We want to estimate X given Y and A
\item One estimator could be the one which maximise the likelihood of the data p(Y|X) which also is equivalent to the least square solution. 
\item When M << N where the dimensionality of X is smalle to the Y dimension then the maximum likelihood (ML) solution is effective.
\item But in some case where the dimensionality of X exceeds the dimension of Y the opposite is true -- the mapping from Y to X is under-determined and we will then have infinietly number of solution to the ML which equal likelihood.
\item By introducing Bayesian such as X is drawn from a distribution p(X) allows us to narrow the candidates solutions space.
\item This could e.g. be a zero-mean Gaussian prior with covariance $\sigma_X^2 I$ while E is independently Guassian with covariance $\sigma_e^2 I$. The MAP estimator is the given as
\begin{align*}
\hat{X} = \arg \max_X p(Y|X)p(X) = A^T (\lambda I + AA^T)^{-1} Y,
\end{align*}
with $\lambda = \sigma_e^2 / \sigma_x^2$. This lead to that $\hat{X}$ has a large number of smalle non-zero coefficents.
\item By assuming some prior belif that Y has been generated by sparse coefficeinet expansion such that most ellemens in X are zero -- called sparsity-induciong prior
\item ALternative we can use empirical priors
\item One problem -- they all ensuing the inverse problem from Y to X becoming non-linear.
\end{itemize}

As mention in section XX a multiple measurement model (MMV) $\mathbf{Y} \in \mathbb{R}^{M \times L}$
\begin{align*}
\mathbf{Y} = \mathbf{AX} + \mathbf{E},
\end{align*}
models the mixing of a unknown source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ with the known (see section \ref{sec:Cov_DL}) mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ with some additional noise $\mathbf{E} \in \mathbb{R}^{M \times L}$ for $L$ snapshots in time. The goal is to estimate the source matrix $\mathbf{X}$ as sparse as possible.
\\
The non-zero rows ($k \leq M$) of the source matrix $\mathbf{X}$ is referred to as the active sources and most be as small as possible to induce the sparsity of the matrix.
\\ \\
As the optimisation problem of the MMV model is NP-hard and not guarantee a global solution we instead rewrite our MMV model in Bayesian framework. By apply an exponential function $\exp(- (\cdot))$ transformation onto our optimisation problem a Gaussian likelihood function $p(\mathbf{Y} \vert \mathbf{X})$ with a $\lambda$-dependent variance is achieved:
\begin{align*}
p(\mathbf{Y} \vert \mathbf{X}) \propto \exp \left( - \frac{1}{\gamma} \Vert \mathbf{Y} - \mathbf{AX} \Vert_2^2 \right),
\end{align*}
with a prior distribution $p(\mathbf{X}) \propto \exp(- \Vert \mathbf{X} \Vert_0)$. The optimisation problem is the rewritten to
\begin{align*}
\mathbf{X} (\gamma) = \hat{\mathbf{X}} &= \arg \max_{\mathbf{X}} p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X}) \\
&= \arg \max_{\mathbf{X}} \frac{p(\mathbf{Y} \vert \mathbf{X}) p(\mathbf{X})}{p(\mathbf{Y)}} \quad (\text{Bayers Formular}) \\
&= \arg \max_{\mathbf{X}} p(\mathbf{X} \vert \mathbf{Y}).
\end{align*}
$\mathbf{X}$ can then be estimated by the used of maximum a posterior (MAP) estimation \cite[p. 137]{??}.

\subsection{Empirical Bayesian Algorithm}
Let $p(\mathbf{Y} \vert \mathbf{X})$ be a Gaussian prior with a known noise variance $\lambda$. Then for each columns in $\mathbf{Y}$ and $\mathbf{X}$ the likelihood is written as
\begin{align*}
p(\mathbf{y}_{\cdot j} \vert \mathbf{x}_{\cdot j}) &= \mathcal{N}(\mathbf{Ax}_{.j}, \lambda^2 \mathbf{I}) \\
&= (2 \pi \lambda)^{-N/2} \exp \left( - \frac{1}{2 \lambda} \Vert \mathbf{y}_{\cdot j} - \mathbf{A} \mathbf{x}_{\cdot j} \Vert_2^2 \right).
\end{align*}
The $i$-th row of the sources matrix $\mathbf{X}$, $\mathbf{x}_{i \cdot}$, has an $L$-dimensional independent Gaussian prior with zero mean and a variance controlled by $\gamma_i$ which is unknown:
\begin{align*}
p (\mathbf{x}_{i \cdot} ; \gamma_i) &= \mathcal{N}(0, \gamma_i \mathbf{I})
\end{align*}
By combing the two row priors 
\begin{align*}
p (\mathbf{X} \vert \boldsymbol{\gamma}) &= \prod_{j=1}^L p (\mathbf{x}_{ \cdot j} \vert \gamma_i),
\end{align*}
a full prior is achieved with the hyperparameter vector $\boldsymbol{\gamma} = [\gamma_1, \dots, \gamma_M]^T$. By combining the full prior and the likelihood the posterior of the $j$-th column of the source matrix $\mathbf{X}$ is defined as
\begin{align*}
p(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \frac{p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma})}{\int p(\mathbf{x}_{\cdot j}, \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) \ d \mathbf{x}_{\cdot j}} = \mathcal{N}(\boldsymbol{\mu}_{\cdot j}, \boldsymbol{\Sigma}),
\end{align*}
with the mean and covariance given as
\begin{align}\label{eq:moments}
\boldsymbol{\Sigma} &= \text{Cov}(\mathbf{x}_{\cdot j} \vert \mathbf{y}_{\cdot j} ; \boldsymbol{\gamma}) = \boldsymbol{\Gamma} - \boldsymbol{\Gamma} \mathbf{A}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{A} \boldsymbol{\Gamma}, \quad \forall j = 1, \dots, L \\
\mathcal{M} &= [\boldsymbol{\mu}_{\cdot 1}, \dots, \boldsymbol{\mu}_{\cdot L}] = \mathbb{E}[\mathbf{X} \vert \mathbf{Y} ; \boldsymbol{\gamma}] = \boldsymbol{\Gamma} \mathbf{A}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{Y},
\end{align}
where $\boldsymbol{\Gamma} = \text{diag}(\boldsymbol{\gamma})$. 
\\ \\
The row sparsity is achieved whenever $\gamma_i = 0$ leading to that the posterior must have the following probability
\begin{align*}
P(\mathbf{x}_{i \cdot} = \mathbf{0} \vert \mathbf{Y} ; \gamma_i = 0) = 1,
\end{align*}
which ensure that the posterior mean $\mathcal{M}$ of the $i$-th row, $\boldsymbol{\mu}_{i \cdot}$, will be zero. Instead of estimating our source matrix $\mathbf{X}$ we instead estimate the hyperparameter $\gamma_i$.
\\
Each of the hyperparameters $\gamma_i$ correspond to different hypothesis for the prior distribution of the underlying generation of $\mathbf{Y}$. Therefore the determining of $\gamma_i$ must be seen as a model selection in with we can use the empirical Bayesian stragetry. This evolve the task of treating the unknown source matrix $\mathbf{X}$ as nuisance\todo{pestilens, plage, gene -- de brugte dette ord i phden} parameters and integrating them out.
\\
By integrating the unknown sources $\mathbf{X}$ the marginal likelihood of the observed mixed data $\mathbf{Y}$, $p (\mathbf{Y} ; \gamma)$ is achieved \cite[p. 146]{??}. By applying the $-2 \log (\cdot)$ transformation the marginal likelihood function is transformed to the cost function
\begin{align*}
\mathcal{L}(\gamma) &= - 2 \log(p (\mathbf{Y} ; \boldsymbol{\gamma})) = -2 \log \left( \int p (\mathbf{Y}  \vert \mathbf{X}) p (\mathbf{X} ; \boldsymbol{\gamma}) \ d\mathbf{X} \right) \\
&= L \log ( \vert \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T \vert) + \sum_{j=1}^L \mathbf{y}_{.j}^T \left( \lambda \mathbf{I} + \mathbf{A} \boldsymbol{\Gamma} \mathbf{A}^T\right)^{-1} \mathbf{y}_{.j}
\end{align*}
To minimise the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ the evidence maximisation (EM) algorithm can be used. The E-step of the EM algorithm is to compute the posterior moments as mention in \eqref{eq:moments} while the M-step is a update rule of $\gamma_i$:
\begin{align*}
\gamma_i^{(k+1)} = \frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2 + \Sigma_{ii}, \quad \forall i = 1, \dots, M.
\end{align*}
The M-step is very slow on large data. Instead one could use a fixed point update to fasten the convergence on large data. The fixed point updating step is achieved by taking the derivative of the marginal likelihood $\mathcal{L}(\boldsymbol{\gamma})$ with respect to $\boldsymbol{\gamma}$ and equating it with zero. This lead to the updating equation which can replace the one from M-step in the EM-algorithm:
\begin{align*}
\gamma_i^{(k+1)} = \frac{\frac{1}{L} \Vert \boldsymbol{\mu}_{i \cdot} \Vert_2^2}{1 - \gamma_i^{-1 (k)} \Sigma_{ii}}, \quad \forall i = 1, \dots, M.
\end{align*}
After convergence the support set $\hat{S}$ is extracted from the solution $\hat{\gamma}$ by $\hat{S} = \{ i, \hat{\gamma}_i \neq 0 \}$.


\begin{algorithm}[H]
\caption{M-SBL -- See page 148 in PHD}
\begin{itemize}
\item[1.] Given $\mathbf{Y}$ and a dictionary matrix $\mathbf{A}$.
\item[2.] Initialise $\boldsymbol{\gamma}$, e.g $\boldsymbol{\gamma} = \mathbf{1}$.
\item[3.] Compute the posterior moments $\boldsymbol{\Sigma}$ and $\mathcal{M}$.
\item[4.] Update $\boldsymbol{\gamma}$ using EM or fixed-point.
\item[5.] Repeat step 3 and 4 until convergence to a fixed point $\boldsymbol{\gamma}^\ast$.
\item[6.] ...
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
\begin{itemize}
\item With M-SBL the \textbf{support set} of source can be recover for $k \geq M$, with some sufficient condition on the dictionary and sources. $M \leq k \leq N$ the support set can be recovered in the noiseless case.
\item We assume that mixing at the sensors is instantaneous (no time delay between sources and sensors) and the environment is anechoic. (M-SBL)
\item The sufficient conditions for exact support recovery for M-SBL in the regime k $\geq$ M are twofold: 1) orthogonality (uncorrelated) of the active sources, 2) The second condition imposes a constraint on the sensing dictionary A
\item Bayesian replace the troublesome prior with a distribution that, while still encouraging sparsity, is somehow more computationally convenient. Bayesian approaches to the sparse approximation problem that follow this route have typically been divided into two categories: (i) maximum a posteriori (MAP) estimation using a fixed, computationally tractable family of priors and, (ii) empirical Bayesian approaches that employ a flexible, parameterized prior that is ‘learned’ from the data
\end{itemize}