Through this chapter an introduction to the concept compressive sensing is described with associated theory which later on will be used in the development of the algorithm with used methods known from compressive sensing to estimate the mixing matrix $A$ and the sparse source matrix $X$.

\section{Compressive Sensing}
Compressive sensing is the theory of efficient recover/reconstruct a signal from minimal measurements. This recovery is often described as a linear model/system 
\begin{align*}
\mathbf{y} = \mathbf{Ax},
\end{align*}
which consist of observed data $\mathbf{y} \in \mathbb{R}^M$, a matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ which models the linear measurements and signal $\mathbf{x} \in \mathbb{R}^N$. In compressive sensing terminology, $\mathbf{y}$ is the signal of interest that is wish recovered from minimal measurements meaning that the signal $\mathbf{x}$ must be sparse.
\\
A signal is said to be $k$-sparse if the signal has at most $k$ non-zeros coefficient
\begin{align*}
\Vert \mathbf{x} \Vert_0 = \text{card}(\text{supp}(\mathbf{x})) \leq k,
\end{align*}
where the $\ell_0$-norm is used. The function $\text{card}$ is the cardinality of the support of $\mathbf{x}$. The support of $\mathbf{x}$ is giving as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set \cite[p. 41]{FR}. The set of all $k$-sparse signals is denoted as
\begin{align*}
\Sigma_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}
It is of interest to recover the signal $\mathbf{y}$ when $M << N$ and $k < M$ \cite[p. 8]{CS}. This lead to that the matrix $\mathbf{A}$ becomes rank-deficient and therefore have a non-empty null-space \cite[p. ix]{CS}.
\\ \\
The linear model and finding the sparse signal $\mathbf{x}$ can be written as an optimisation problem
\begin{align*}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az},
\end{align*}
where $\mathbf{z}$ is all possible candidates to an $k$-sparse signal $\mathbf{x}$.
\\
Unfortunately, this optimisation problem is non-convex because of $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. Instead by replacing the $\ell_0$-norm with its convex approximation, the $\ell_1$-norm, the optimisation problem become computational feasible \cite[p. 27]{CS}
\begin{align}\label{eq:p1}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az},
\end{align} 
and instead we find the best $k$-term approximation of the signal $\mathbf{x}$.

\subsection{Conditions on the Mixing Matrix}
To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated on the matrix $\mathbf{A}$ must be satisfied.

\subsubsection{Null Space Conditions}
The null space property (NSP) is some necessary and sufficient condition for exact recovery.
The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(A) = \{ z \ : \ Az = 0 \}.
\end{align*} 




%$\ell_p$ norm is given as
%\begin{align*}
%\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right)^{1/p}, \quad p \in [1, \infty)
%\end{align*}
%the norm is used as an measure of the strenght of a signal or as an error.

\subsubsection{Restricted Isometry Conditions}
NSP do not take account for noise and we must therefore look at some stronger conditions which incoperate noise, the following restricted isometry property (RIP) %\cite[p. 19]{•}.

\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}
holds for all $x \in \Sigma_k$
\end{definition}


If a matrix $A$ satisfy RIP then it will also satisfy the NSP as RIP is strictly stronger than NSP.
\begin{theorem}
If $A$ satisfies the RIP of order $2k$ with the constant $\delta_{2k} < \sqrt{2} -1$. Then
\begin{align*}
C = \frac{2}{1 - (1 + \sqrt{2}) \delta_{2k}}
\end{align*}
\end{theorem} 

\subsubsection{Coherence}
The NSP provide a unique solution to the optimisation problem, \eqref{eq:p1}, but is unfortunately complicated to investigate. Instead an alternative measure used for sparsity is presented.
\\%\cite[p. 24]{•}.
Coherence is a measure of quality and determine if the matrix $A$ is a good choice for the optimisation problem \eqref{eq:p1}. A small coherence describe the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
\begin{definition}[Coherence]
Coherence of the matrix $A \in \mathbb{R}^{M \times N}$, denoted as $\mu (A)$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
\begin{align*}
\mu (A) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}.
\end{align*}
\end{definition}