\section{Linear Algebra}
Some observed measurement vector $\mathbf{y}$ can be described as a linear combinations of a coefficient matrix $\mathbf{A}$ and some vector $\mathbf{x}$ such that
\begin{align}\label{eq:SMV_model}
\mathbf{y} &= \mathbf{Ax},
\end{align}
where $\mathbf{y} \in \mathbb{R}^M$ is the observed measurement vector consisting of $M$ measurements, $\mathbf{x} \in \mathbb{R}^N$ is an unknown source vector of $N$ elements, and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a coefficient matrix which models the linear measurement process column-wise. The liner model makes a system of linear equations with $M$ equations and $N$ unknowns.
\\ \\
In the case of $\mathbf{A}$ being a square matrix, $M = N$, a solution can be found to the linear model if $\mathbf{A}$ has full rank -- $\mathbf{A}$ consist of linearly independent columns or rows.
% For $M > N$ the matrix said to have full rank when the columns are linearly independent. For $M < N$ the matrix has full rank when the rows are linearly independent. 
A linear systems with $M = N$ is called determined, $M > N$ overdetermined and $M < N$ under-determined. 
When full rank do not occur the matrix is then called rank-deficient.
\\ \\
By inverting $\mathbf{A}$ from \eqref{eq:SMV_model} the unknown vector $\mathbf{x}$ can be achieved. Square matrix is invertible if an only if its has full rank or its determinant $\det(\mathbf{A}) \neq 0$. For rectangular matrices, $M > N$ and $M < N$, left-sided and right-sided inverse exists.
%\begin{itemize}
%\item Left-sided inverse ($M > N$): $(\mathbf{A} \mathbf{A})^{-1} \mathbf{A}^T \mathbf{A} = \mathbf{A}_{\text{left}}^{-1} = \mathbf{I}_N$ 
%\item Right-sided inverse ($M < N$): $\mathbf{A} \mathbf{A}^T (\mathbf{A} \mathbf{A}^T)^{-1} = \mathbf{A}_{\text{right}}^{-1} = \mathbf{I}_M$ 
%\end{itemize}
With the left-inverse the least norm solution of \eqref{eq:SMV_model} can be found.\todo{least norm solution?}
\\ \\
For an determined system there will exist a unique solution. For an overdetermined system there do not exist a solution and for under-determined systems there exist infinitely many solutions.
\\
For rank-deficient matrices there do not exist an inverse and therefore \eqref{eq:SMV_model} can not be solve by inverting the model. But rank-deficient matrices  meaning have non-empty null space leading to infinitely solutions to \eqref{eq:SMV_model} \cite[p. ix]{CS}\todo{evt kan dette afsnit fjernes?}.
\\ \\
As described in chapter \ref{ch:motivation} the linear model of interest consist of $M$ sensors which is known and $N$ sources which is unknown. Furthermore, we also have that $M < N$ -- an under-determined system. It is therefore of interest to find a solution in the infinitely solution set.

\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery or reconstruction of a signal from a minimal number of observed measurements. It is build upon empirical observations assuring that many signals can be approximated by remarkably sparser signals.   
 Assume linear acquisition of the original measurements, then the relation between the measurements and the signal to be recovered can be described by the linear model \eqref{eq:SMV_model} \cite{FR}.  
\\ 
%\begin{align}\label{eq:model}
%\mathbf{y} = \mathbf{Ax}.
%\end{align}
%Here $\mathbf{y} \in \mathbb{R}^M$ is the measured data consisting of $M$ observations, $\mathbf{x} \in \mathbb{R}^N$ is the original signal consisting of $N$ possible sources. $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a matrix which models the linear measurement process or in other words it projects each of the $N$ possible sources on to the $M$ observations. $\textbf{A}$ is  referred to as a dictionary or a mixing matrix .\\
In compressive sensing terminology, $\mathbf{x}\in \mathbb{R}^{N}$ is the signal of interest which is sought recovered from the measurements $\textbf{y}\in \mathbb{R}^{M}$ by solving the linear system \eqref{eq:SMV_model}. The coefficient matrix $\textbf{A}$ is in the context of compressive sensing referred to as the mixing matrix or the dictionary.\\    
 In the typical compressive sensing case the system is under-determined, $M < N$,  and there exist infinitely many solutions, provided that a solution exist.
% Such system is also referred to as over-complete \textit{(as the number of column basis vectors is greater than the dimension of the input)}.
\\
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal, hence the term sparse signal recovery \cite{FR}.
%Yet another argument; If $M<<N$, it leads to the matrix $\mathbf{A}$ being rank-deficient\textit{(but not necessarily?)} which imply that $\mathbf{A}$ has a non-empty null space and this leads to infinitely many signals which yield the same solution $\mathbf{y} = \mathbf{Ax} = \mathbf{Ax'}$ \cite[p. ix]{CS}. Thus it is necessary to limit the solution space to a specific class of signals $\mathbf{x}$, for this a certain constraint on sparseness is introduced.   

\subsection{Sparseness} 
A signal is said to be $k$-sparse if the signal has at most $k$ non-zero coefficients. For the purpose of counting the non-zero entries of a vector representing a signal the $\ell_0$-norm is defined
\begin{align*}
\Vert \mathbf{x} \Vert_0 := \text{card}(\text{supp}(\mathbf{x})).
\end{align*}
The function $\text{card}(\cdot)$ gives the cardinality of the input and the support vector of $\mathbf{x}$ is given as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ is a set of integers $\{1,2,\hdots,N\}$ \cite[p. 41]{FR}. The set of all $k$-sparse signals is denoted as
\begin{align*}
\Omega_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}

\subsection{Optimisation Problem}\label{sec:opti}
To find a solution to the linear model \eqref{eq:SMV_model} assuming the solution is $k$-sparse with $k < M$\todo{første gang vi møder k<M, er det for tidligt?}, it can be view as an optimisation problem. An optimisation problem is defined as
\begin{align*}
\min f_0 (\mathbf{x}) \quad \text{s.t} \quad f_i (\mathbf{x}) \leq b_i, \quad i \in [n],
\end{align*}
where $f_0 \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ is an objective function and $f_i \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ are the constraint functions. 
\\
To find the $k$-sparse solution our optimisation problem can be written as
\begin{align*}
\textbf{x}^\ast = \arg \min_{\textbf{x}\in \boldsymbol{\Omega_{k}}} \Vert \mathbf{x} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax},
\end{align*}
The objective function is given by an $\ell_0$ norm with the constraint function being the linear model \eqref{eq:SMV_model}. Unfortunately, this optimisation problem is non-convex due to the definition of $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. Instead, by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimisation problem can be approximated and hence become computational feasible \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\textbf{x}^\ast = \arg \min_{\textbf{x}\in  \boldsymbol{\Omega_{k}}} \Vert \mathbf{x} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax}.
\end{align} 
With this optimisation problem we find the best $k$-sparse approximation of the signal $\mathbf{x}^\ast$. This method is referred to as Basis Pursuit. 
\\
The following theorem justifies that the $\ell_1$ optimisation problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}
A mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined with columns $\textbf{A} = [\textbf{a}_1, \dots, \textbf{a}_N]$. By assuming uniqueness of a solution $\textbf{x}^{\ast}$ to
$$
\min_{\textbf{x} \in \mathbb{R}^N} \Vert \textbf{x} \Vert_1 \quad \text{s.t} \quad \textbf{Ax} = \textbf{y},
$$
the system $\{ \textbf{a}_j, j \in \text{supp} ( \textbf{x}^{\ast})\}$ is linearly independent, and in particular
$$
\Vert \textbf{x}^{\ast} \Vert_0 = \text{card}(\text{supp} (\textbf{x}^{\ast})) \leq M.
$$
\end{theorem}
To prove this theorem we need to realise that the set $\{\textbf{a}_j, j \in S \} \leq M$, with $S = \text{supp}(\textbf{x}^\ast)$, can not have more than $M$ linearly independence columns. So when $M \ll N$ we automatically achieve a sparse signal\todo{gør vi det, det er vel bare et kort signal, det er måske en sparse repræsentation?}.
\begin{proof}
For the case of contradiction, assume that the set $\{\textbf{a}_j, j \in S \}$ is linearly dependent. Thus there exists a non-zero vector $\textbf{v} \in \mathbb{R}^N$ supported on $S$ such that $\textbf{Av} = \textbf{0}$. Then, for any $t \neq 0$,
$$
\Vert \textbf{x}^{\ast} \Vert_1 < \Vert \textbf{x}^{\ast} + t \textbf{v} \Vert_1 = \sum_{j \in S} \vert x_j^{\ast} + t v_j \vert = \sum_{j \in S} \text{sgn}(x_j^{\ast} + t v_j )(x_j^{\ast} + t v_j ).
$$
If $|t|$ is small enough, namely, $|t| < \min_{j \in S} \vert x_j^{\ast}\vert / \Vert \textbf{v} \Vert_{\infty}$, then
$$
\text{sgn}(x_j^{\ast} + t v_j) = \text{sgn}(x_j^{\ast}), \quad \forall j \in S.
$$
It the follows that
$$
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{j \in S} \text{sgn}(x_j^{\ast})(x_j^{\ast} + t v_j ) = \sum_{j \in S} \text{sgn}(x_j^{\ast})x_j^{\ast} + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j.
$$
This is a contradiction, because we can always choose a small $t \neq 0$ such that $t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j \leq 0$ and therefore the set $\{\textbf{a}_j, j \in S \}$ must be linearly independent\todo{kan vi lige gennemgå dette. }.
\end{proof}
The Basis Pursuit makes the foundation of several algorithms solving alternative versions of \eqref{eq:SMV_p1} where noise is incorporated. An alternative solution method includes greedy algorithms such as the Orthogonal Matching Pursuit(OMP) \cite[P. 65]{FR}. At each iteration of the OMP algorithm an index set $S$ is updated by adding the index corresponding to the column in $\textbf{A}$ that best describes the residual, hence greedy. That is the part of $\textbf{y}$ that is not yet explained by $\textbf{Ax}$ is included. Then $\textbf{x}$ is updated as the vector, supported by $S$, which minimize the residual, that is also the orthogonal projection  of $\textbf{y}$ onto the span$\{a_j \ \vert \ j \in S \}$. 
\begin{algorithm}[H]
\caption{Orthogonal Matching Pursuit (OMP)}
\begin{algorithmic}[1]
			\State$k = 0$			
			\State$\text{Initialize} \quad S_{(0)} =\emptyset$ 
			\State$\text{Initialize} \quad x_{(0)} =\textbf{0}$
            \Procedure{OMP}{$\textbf{A},\textbf{y}$}    
            \While{$\text{stopping criteria not meet}$} 
                \State$k = k + 1$
				\State$j_{(k)} = \arg \max_{j \in [N]} \{ \vert (\mathbf{A}^\ast (\mathbf{y} - \mathbf{Ax}_{(k-1)}))_j\vert \}$				
				\State$S_{(k)} = S_{(k-1)} \cup \lbrace j_{(k)} \rbrace$ 
				\State$\textbf{x}_{(k)} = \arg \min_{\mathbf{z} \in \mathbb{C}^N} \{ \Vert \mathbf{y} - \mathbf{Az} \Vert_2 \  \vert \ \text{supp}(\mathbf{z}) \subset S_{(k)} \}$
          		\EndWhile
          		\State$\mathbf{x}^\ast = \mathbf{x}_{(k)}$
            \EndProcedure
        \end{algorithmic} 
        \label{alg:OMP}
\end{algorithm}


      
\subsection{Conditions on the Dictionary}\label{sec:dic_conditions}
In section \ref{sec:opti} the mixing matrix $\mathbf{A}$ was assumed known, in order to solve the optimisation problem \eqref{eq:SMV_p1}. However, in practise it is only the measurement vector $\textbf{y}$ which is known. In this case $\textbf{A}$ is often thought of as a dictionary matrix\todo{vi skrev tidligere at vi bruger en dictionary til at finde den rigtige A?}. To ensure exact or approximately reconstruction of the sparse signal $\mathbb{x}$, the dictionary matrix must be constructed with certain conditions in mind. 
% $\mathbf{x}^\ast$ from the optimisation problem \eqref{eq:SMV_p1} 


\subsubsection{Null Space Condition}
The the null space property is a necessary and sufficient condition to $\mathbf{A}$ for exact reconstruction of every sparse signal $\textbf{x}$ that solves the optimisation problem \eqref{eq:SMV_p1}\cite[p. 77]{FR}. The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(\mathbf{A}) = \{ \mathbf{z} \ : \ \mathbf{Az} = \textbf{0} \}.
\end{align*} 
The null space property is defined as
\begin{definition}[Null Space Property]
A matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is said to satisfy the null space property relative to a set $S \subset [N]$ if
\begin{align}
\Vert \textbf{v}_S \Vert_1 < \Vert \textbf{v}_{\overline{S}} \Vert_1 \quad \text{for all} \quad \textbf{v} \in \text{ker}(\mathbf{A} \setminus \lbrace \textbf{0} \rbrace),
\end{align}
where the vector $\mathbf{v}_S$ is the restriction of $\mathbf{v}$ to the indices in $S$, and $\bar{S}$ is the set $[N]\setminus S$. 
\end{definition}
\todo{hvorfor ker? ikke null?}
\begin{theorem}
Given a matrix $\textbf{A} \in \mathbb{R}^{M \times N}$, every vector $\textbf{x} \in \mathbb{R}^N$ with $\text{supp}(\textbf{x})\subset S$ is the unique solution of \eqref{eq:SMV_p1} with $\textbf{y} = \textbf{Ax}$ if and only if $\textbf{A}$ satisfies the null space property relative to $S$.
\end{theorem}
\begin{proof}
$\Rightarrow:$ Given a fixed index set $S \subseteq [N]$, assume first that every vector $\textbf{x} \in \mathbb{R}^N$ with support $\text{supp}(\textbf{x})\subset S$ is the unique minimiser of $\Vert \textbf{z} \Vert_1$ subject to $\textbf{Az} = \textbf{Ax}$. Thus, for any $\textbf{v} \in \text{ker}(\mathbf{A}) \setminus \{\textbf{0} \}$, the vector $\textbf{v}_S$ is the unique minimizer $\Vert \textbf{z} \Vert_1$ subject to $\textbf{Az} = \textbf{Av}_S$. But we have that $\textbf{0} = \textbf{A}(\textbf{v}_S + \textbf{v}_{\overline{S}}) \implies \textbf{Av}_S = \textbf{A}(-\textbf{v}_{\overline{S}})$ and $-\textbf{v}_{\overline{S}} \neq \textbf{v}_S$ or else $\textbf{v} = \textbf{0}$. We conclude that $\Vert \textbf{v}_S \Vert_1 < \Vert \textbf{v}_{\overline{S}} \Vert_1$. This establishes the null space property relative to $S$.
\\ \\
$\Leftarrow:$ Conversely, assume that the null space property relative to $S$ holds. Then, given $S \subseteq [N]$ with null space property and a vector $\textbf{x} \in \mathbb{R}^N$ with$\text{supp}(\textbf{x})\subset S$ and a vector $\textbf{z} \in \mathbb{R}^N$, $\textbf{z} \neq \textbf{x}$, satisfying $\textbf{Az} = \textbf{Ax}$, we consider the vector $\textbf{v} := \textbf{x} - \textbf{z} \in \text{ker}(\textbf{A}) \setminus \{ \textbf{0} \}$. In view of the null space property, we obtain
\begin{align*}
\Vert \textbf{x} \Vert_1 \leq \Vert \textbf{x} - \textbf{z}_S \Vert_1 &= \Vert \textbf{v}_S \Vert_1 + \Vert \textbf{z}_S \Vert_1 \\
&< \Vert \textbf{v}_{\overline{S}} \Vert_1 + \Vert \textbf{z}_S \Vert_1 \\
&= \Vert -\textbf{z}_{\overline{S}} \Vert_1 + \Vert \textbf{z}_S \Vert_1 = \Vert \textbf{z} \Vert_1
\end{align*}
This establishes the required minimality of $\Vert \textbf{x} \Vert_1$.
\end{proof}
\todo{vi ligger meget op a FR p. 79 text her}Unfortunately, this is a property/condition which is hard to check in practice.

\subsubsection{Coherence}
The null space property provide a unique solution to the optimisation problem \eqref{eq:SMV_p1}, but it is unfortunately complicated to investigate. Instead an alternative measure used for sparsity is presented\todo{er null propertien et mål for sparsity?}.
\\
Coherence is a measure of quality, it determines whether a matrix $\mathbf{A}$ is a good choice for the optimisation problem \eqref{eq:SMV_p1}. A small coherence describes the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
\begin{definition}[Coherence]
Coherence of the matrix $A \in \mathbb{R}^{M \times N}$, denoted as $\mu (\mathbf{A})$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
\begin{align*}
\mu (\mathbf{A}) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}.
\end{align*}
\end{definition}

\subsubsection{Restricted Isometry Condition}
Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix $\mathbf{A}$.
\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}%holds for all $x \in \Sigma_k$
\end{definition}

\begin{theorem}
Suppose that the $2s$-th restricted isometry constant of the matrix $\textbf{A} \in \mathbb{R}^{M \times N}$ satisfies
\begin{align*}
    \delta_{2s} < \frac{1}{3}.
\end{align*}
Then every $s$-sparse vector $\textbf{x}^\ast \in \mathbb{R}^N$ is the unique solution of
$$
\min_{\textbf{z} \in \mathbb{R}^N} \Vert \textbf{z} \Vert_1 \quad \text{subject to} \quad \textbf{Az} = \textbf{Ax}.
$$
\end{theorem}
\begin{proof}
To proof the theorem we only need to show the null space condition:
$$
\Vert \textbf{v} \Vert_1 < \frac{1}{2} \Vert \textbf{v} \Vert_1, \quad \forall \ \textbf{v} \in \text{ker}(\textbf{A})\setminus \{\textbf{0}\}, \ S \subseteq [N], \ \text{card}(S) \leq s.
$$
Cf. Cauchy-Schwarz or $\Vert \textbf{v}_S \Vert_1 \leq \Vert \textbf{v}_S \Vert_2 \sqrt{s}$, we only need to show
\begin{align*}
\Vert \textbf{v}_S \Vert_2 &\leq \frac{\rho}{2 \sqrt{s}} \Vert \textbf{v} \Vert_1 \\
\rho &= \dfrac{2 \delta_{2s}}{1 - \delta_{2s}} < 1,
\end{align*}
whenever $\delta_{2s} < 1/3$. Given $\textbf{v} \in$ ker$(\textbf{A})\setminus \{\textbf{0}\}$, it is enough to consider an index set $S = S_0$ of $s$ largest absolute entries of the vector $\textbf{v}$. The complement $\overline{S_0}$ of $S_0$ in $[N]$ is partition as $S_0 = S_1 \cup S_2 \cup \cdots$, where
\begin{align*}
    S_1 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0}, \\
    S_2 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0 \cup S_1}.
\end{align*}
With $\textbf{v} \in$ ker$(\textbf{A})$:
$$
\textbf{A}(\textbf{v}_{S_0}) = \textbf{A}(-\textbf{v}_{S_1} - \textbf{v}_{S_2} - \cdots),
$$
so that
\begin{align}\label{eq:L9_6}
\Vert \textbf{v}_{S_0} \Vert_2^2 &\leq \frac{1}{1 - \delta_{2s}} \Vert \textbf{A}(\textbf{v}_{S_0}) \Vert_2^2 = \frac{1}{1 - \delta_{2s}} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_1}) + \textbf{A}(-\textbf{v}_{S_2}) + \cdots \rangle \nonumber \\
    &= \frac{1}{1 - \delta_{2s}} \sum_{k \geq 1} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle.
\end{align}
According to Proposition \ref{prop:L9_1}, we also have
\begin{align}\label{eq:L9_7}
    \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle. \leq \delta_{2s} \Vert \textbf{v}_{S_0} \Vert_2 \Vert \textbf{v}_{S_k} \Vert_2.
\end{align}
Substituting \eqref{eq:L9_7} into \eqref{eq:L9_6} and dividing by $\Vert \textbf{v}_{S_0} \Vert_2 > 0$
\end{proof}

%The construction of the matrix $\mathbf{A}$ is of course essential for the solution of the optimisation problem. So far no one has manage to construct a matrix which is proved to be optimal for some compressive sensing set up. However some certain constructions have shown sufficient recovery guarantee.\\ \\
%To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated to the matrix $\mathbf{A}$ must be satisfied.\\
%This includes at first the null space condition, a property of the $A$ matrix which is hard to check in practise. 
%The Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix. Furthermore the coherence of a matrix is a measure of quality and is used to determine whether the matrix $A$ is a good choice for the optimisation problem.\\    

\subsection{Multiple Measurement Vector Model}\label{sec:MMV}
The linear model \eqref{eq:SMV_model} is also referred to as a single measurement vector (SMV) model. In order to adapt the model \eqref{eq:SMV_model} to a practical use the model is expanded to include multiple measurement vectors and take noise into account.
\\ \\
A multiple measurement vector (MMV) model consist of the observed measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$, the dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and the noise vector $\textbf{E} \in \mathbb{R}^{M \times L}$:
\begin{align}\label{eq:MMV_model}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align}
$L$ denote the number of observed measurement vectors each consisting of $M$ measurements. For $L = 1$ the linear model will just be the SMV model \eqref{eq:SMV_model}. 
\\
The matrix $\mathbf{X}$ consist of $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ sparse vectors which has been stack column-wise leading to that $\mathbf{X}$ consist of at most $k$ non-zero rows\todo{vi skal lige have styr på om vi kun antager identisk sparsness i alle søjler, og i så fald at det data vi arbejde med har den form}. As for the SMV model \eqref{eq:SMV_model} the MMV model \eqref{eq:MMV_model} is under-determined with $M \ll N$ and the number of samples is less than $M$, $L < M$\todo{hvorfor L<M?} \cite[p. 42]{CS}.
\\ \\
The support of $\mathbf{X}$ denote the index set of non-zero rows of $\mathbf{X}$ and $\mathbf{X}$ is said to be row-sparse. As the columns in $\mathbf{X}$ are $k$-sparse and as mention before $\mathbf{X}$ has at most $k$ non-zero rows, the non-zero values occur in common location. By using this joint information it is possible to recover $\mathbf{X}$ from fewer measurements.
\\
By using the rank of $\mathbf{X}$ which give us information of the amount of linearly independent rows or columns and the spark of $\mathbf{A}$ the minimum set of linearly dependent columns it is possible to set some condition on the measurement to ensure recovery.
\\
When the support of $\mathbf{X}$ is equal to the amount of non-zero rows in $\mathbf{X}$\todo{antal, og er det ikke altid det?}, $\vert \text{supp}(\mathbf{X})\vert = k$ then the rank of $\mathbf{X}$ would be $\text{rank}(\mathbf{X}) \leq k$. If rank$(\mathbf{X}) = 1$ then are the $k$-sparse vectors $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ multiple of each other the joint information cannot be taken advantage of. But for large rank we can exploit the diversity of the columns in $\mathbf{X}$. This can be defined in a sufficient and necessary condition of the MMV model \eqref{eq:MMV_model} which must have
\begin{align*}
\vert \text{supp}(\mathbf{X}) \vert < \frac{\text{Spark} (\mathbf{A}) - 1 + \text{rank}(\mathbf{X})}{2}
\end{align*}
such that $\mathbf{X}$ can uniquely be determined.
\\
This result lead to that row-sparse matrix $\mathbf{X}$ with large rank can be recovered from fewer measurement \cite[p. 43]{CS}.

%\subsection{Dictionary learning}\label{sec:dictionarylearning}
%In cases where the dictionary is unknown it is possible to learn the dictionary from the observed measurement provided that several observations are available $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$. 
%In dictionary learning framework the inverse problem is defined as
%\begin{align*}
%\min_{\mathbf{A,X}} = \frac{1}{2} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_F^2 + \gamma \sum_{i=1}^{L} g(\mathbf{x}_i),
%\end{align*}
%where the function $g(\cdot)$ promotes sparsity of the source vectors at sample $i$ \cite[p. 4]{phd}. $\Vert \cdot \Vert_F$ is the Frobenius norm which is a vector norm defined as 
%\begin{align*}
%\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
%\end{align*}
%With the MMV model defined as a optimisation problem different dictionary learning algorithm can be used to learn the dictionary $\mathbf{A}$. One of them is the K-SVD algorithm.

\section{Dictionary learning}\label{sec:dictionarylearning}
As clarified in section \ref{sec:dic_conditions} the choice of dictionary matrix $\textbf{A}$ is essential to achieve the best recovery of a sparse signal $\textbf{x}$ from the measurements $\textbf{y}$. Pre-constructed dictionaries do exist which in many cases results in simple and fast algorithms for reconstruction of $\textbf{x}$\cite{Elad_book}. Pre-constructed dictionaries are typically fitted to a specific kind of data, for instance the discrete Fourier transform or the discrete wavelet transform are used especially for sparse representation of images\cite{Elad_book}. Hence the results of using such dictionaries depend on how well they fit the data of interest, which is creating a certain limitation. An alternative is to consider an adaptive dictionary based on a set of training data that resembles the data of interest. For this purpose learning methods are considered to empirically construct a fixed dictionary which can take part in the application. Different dictionary learning algorithms exist, one is the K-SVD which is to be elaborated in this section. The K-SVD algorithm was presented in 2006 by Elad et al. and found to outperform pre-constructed dictionaries when computational cost is of secondary interest\cite{Elad2006}. \\
\\
Consider now $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$, $\textbf{y}_i\in \mathbb{R}^{M}$ as a training database, created by $\textbf{y}_i=\textbf{A}\textbf{x}_i$ for which we want to learn the best suitable dictionary $\textbf{A}$ and sparse representation $\textbf{X}=\left[ \textbf{x}_1, \dots ,\textbf{x}_L \right]$, $\textbf{x}_i\in \mathbb{R}^{N}$. For a known sparsity constraint $k$ this can be defined by an optimisation problem similar to the general compressive sensing problem of multiple measurements \cite{Elad_book}
\begin{align}
\min_{\mathbf{A,X}} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_2^2 \quad st. \ \Vert \textbf{x}_i\Vert_0, \ 1\leq i \leq L.\label{eq:SVD1}
\end{align}  
The learning consist of jointly solving the optimization problem on $\textbf{X}$ and $\textbf{A}$. The uniqueness of $\textbf{A}$ depends on the recovery sparsity condition. As clarified earlier recovery is only possible if $k < M$\cite{phd2015}. Furthermore, consider $\textbf{A}_0$ such that every training signal can be represented by $k_0 < \text{spark}(\textbf{A}_0)/2$ columns of $\textbf{A}_0$, then $\textbf{A}_0$ is a unique dictionary, up to scaling and permutation of columns\cite{Elad_book}\todo{fungerer disse to uniqueness parameter sammen?}. Again the $\ell_0$-norm lead to an NP-hard problem an heuristic methods are need.     

\subsection{K-SVD}
The dictionary learning algorithm K-SVD is a generalisation of the well known K-means clustering also referred to as vector quantization. In K-means clustering a set of K vectors is learned referred to as mean vectors, each signal sample is then represented by its nearest mean vector. That corresponds to the case with sparsity constrict $k=1$ and the representation reduced to a binary scalar $x={1,0}$. Further instead of computing the mean of $K$ sub-sets the K-SVD algorithm computes the SVD factorisation of the K different sub-matrices that correspond to the K columns of $\textbf{A}$\todo{maybe this should come in the end}.\\
\\
The dictionary learning algorithm K-SVD provide an update rule which is applied to each column of $\textbf{A}_0 = \left[ \textbf{a}_0, \hdots , \textbf{a}_N \right] $. Updating first $\textbf{a}_i$ and then the corresponding coefficients in $\textbf{X}$ which it is multiplied with, that is the $i^{\text{th}}$ row in $\textbf{X}$ denoted by $\textbf{x}_i^T$.\\
Let $\textbf{a}_{i_{0}}$ be the column to be updated and let the remaining columns be fixed. By rewriting the objective function in \eqref{eq:SVD1} using matrix notation it is possible to isolate the contribution from $\textbf{a}_{i_{0}}$.
\begin{align}
\Vert \textbf{Y} - \textbf{AX} \Vert_{F}^{2} 
&= \left\| \textbf{Y} - \sum_{i=1}^{M} \textbf{a}_i \textbf{x}_i^{T} \right\|_{F}^{2}\nonumber\\
&= \left\| \left( \textbf{Y}- \sum_{i\neq i_{i}}^{M} \textbf{a}_i\textbf{x}_i^{T}\right) - \textbf{a}_{i_{0}}\textbf{x}_{i_{0}}^{T} \right\| _{F}^{2},\label{eq:SVD2} 
\end{align}
where $F$ is the Frobenius norm that works on matrices
\begin{align*}
\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
\end{align*} 
In \eqref{eq:SVD2} the term in the parenthesis makes the an error matrix $\textbf{E}_{i_0}$ without the contribution from $i_{0}$, hence minimising \eqref{eq:SVD2} with respect to $\textbf{a}_{i_{0}}$ and $\textbf{x}_{i_{0}}^{T}$ leads to the optimal contribution from $i_{0}$ (can I say it this way..?). 
\begin{align}
\min_{\textbf{a}_{i_{0}},\textbf{x}_{i_0}^{T}}\left\|\textbf{ E}_{i_{0}}-\textbf{a}_{i_{0}},\textbf{x}_{i_0}^{T} \right\|_{F}^{2}\label{eq:SVD3}
\end{align} 
The optimal solution to \eqref{eq:SVD3} is known to be the rank-1 approximation of $\textbf{E}_{i_{0}}$. This comes from the Eckart–Young–Mirsky theorem\cite{?} saying that a partial single value decomposition(SVD) makes the best low-rank approximation of a matrix such as $\textbf{E}_{i_0}$.\\
That is specifically that for $\textbf{E}_{i_0}=\textbf{U}\boldsymbol{\Sigma}\textbf{V}^T\in \mathbb{R}^{M\times N},\ M \leq N$ with 
\begin{align*}
\textbf{U}=\left[\textbf{u}_1, \hdots, \textbf{u}_M\right] \in \mathbb{R}^{M\times M}, \quad \boldsymbol{\Sigma}=\text{diag}\left[\sigma_1, \hdots , \sigma_m \right] \in \mathbb{R}^{M\times N}, \quad \textbf{V}=\left[\textbf{v}_1, \hdots, \textbf{v}_N\right] \in \mathbb{R}^{N\times N} 
\end{align*}  
where $\textbf{U}$ and $\textbf{V}$ are unitary matrices, i.e. $\textbf{U}^T\textbf{U}=\textbf{UU}^T=\textbf{I}$, and $\sigma_j$ is the non-negative singular values of $\textbf{E}_{i_0}$ such that $\sigma_1\geq \sigma_2 \geq \hdots \geq 0$. The best $k$-rank approximation to $\textbf{E}_{i_0}$, with $k< rank(\textbf{E}_{i_0})$ is then given by\cite{Wiki..} 
\begin{align*}
\textbf{E}_{i_{0}}^{(k)}= \sum_{j=1}^{k}\sigma_j\textbf{u}_{j}\textbf{v}_{j}^T.
\end{align*} 
Since the outer product always have rank-1 letting $\textbf{a}_{i_0}=\textbf{u}_1$ and $\textbf{x}_{i_0}^T = \sigma_{i}\textbf{v}_{1}^T$ solves the optimisation problem \eqref{eq:SVD3}.
However in order to preserve the sparsity in $\textbf{X}$ while optimising, only the non-zero entries in $\textbf{x}_{i_0}^T$ are allowed to vary. For this purpose only a subset of columns in $\textbf{E}_{i_0}$ is considered, those which correspond to the non-zero entries of $\textbf{x}_{i_0}^T$. A matrix $\textbf{P}_{i_0}$ is defined such that $\textbf{x}_{i_0}^{T^{(R)}}=\textbf{x}_{i_0}^T\textbf{P}_{i_0} $ is restricted to contain only the $M_{j_0}$ non-zero entries of $\textbf{x}_{i_0}^T$. By applying SVD to the  sub-matrix $\textbf{E}_{i_0}\textbf{P}_{i_0}$ and updating $\textbf{a}_{i_0}$ and $\textbf{x}_{i_0}^{T^{(R)}}$ the rank-1 approximation is found and the original representation vector is updated as $\textbf{x}_{i_0}^{T}=\textbf{x}_{i_0}^{T^{(R)}}\textbf{P}_{i_0}^{T}$.  \\ \\
The main steps of K-SVD is described in algorithm \ref{alg:K_SVD}. 

\begin{algorithm}[H]
\caption{K-SVD}
\begin{algorithmic}[1]
			\State$k = 0$			
			\State$\text{Initialize random} \quad  \textbf{A}_{(0)}$            
			\State$\text{Initialize} \quad \textbf{X}_{(0)}=\mathbf{0}$
			\State
            \Procedure{K-SVD}{$\textbf{A}_{(0)}$}    
            \State$\text{normilize columns of} \ \textbf{A}_{(0)}$
            \While{$error \geq limit$} 
                \State $j = j+1$
                \For{$j \gets 1,2,\hdots, L$} \Comment{updating each col. in $\textbf{X}_{(k)}$}
                	\State$\hat{\textbf{x}}_{j} = \min_{\textbf{x}} \|\textbf{y}_j -\textbf{A}_{(k-1)}\textbf{x}_{j}\| \quad s.t. \quad \|\textbf{x}_{j}\| \leq k_0 $
				\EndFor
				\State$\textbf{X}_{(k)}=\{\hat{\textbf{x}}_{j}\}_{j=1}^{L}$
				\For{$i_0 \gets 1,2,\hdots, N$}
					\State$\Omega_{i_0}=\{j|1\leq j \leq L , \textbf{X}_{(k)}[i_0,j]\neq 0\}$
					\State$\text{From} \ \Omega_{i_0} \ \text{define} \ \textbf{P}_{i_0} $
					\State$\textbf{E}_{i_0} =  \textbf{Y}- \sum_{i\neq i_{0}}^{M} \textbf{a}_i\textbf{x}_i^{T}$
					\State$\textbf{E}_{i_0}^R = \textbf{E}_{i_0}\textbf{P}_{i_0}$
					\State$\textbf{E}_{i_0}^R=\textbf{U}\boldsymbol{\Sigma}\textbf{V}^T$ \Comment{perform SVD}
					\State$\textbf{a}_{i_0}\gets \textbf{u}_{1}$ \Comment{update the $i_0$ col. in $\textbf{A}_{(k)}$}
					\State$\left( \textbf{x}_{i_0}^T\right)^R \gets \sigma_{1}\textbf{v}_{1}$
					\State$\textbf{x}_{i_0}^T \gets \left( \textbf{x}_{i_0}^T\right)^R \textbf{P}_{i_0}^T $ \Comment{update the $i_0$ row in $\textbf{X}_{(k)}$}
				\EndFor
				\State$error =\| \textbf{Y}-\textbf{A}_{(k)}\textbf{X}_{(k)}\|_{F}^2 $
          		\EndWhile
            \EndProcedure
        \end{algorithmic} 
        \label{alg:K_SVD}
\end{algorithm}

