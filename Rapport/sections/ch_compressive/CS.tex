\section{Linear Algebra}
A relation between some observed measurements $\mathbf{y}$ can be described as a linear combinations between a coefficient matrix $A$ and some vector $\mathbf{x}$ such that
\begin{align}\label{eq:model}
\mathbf{y} &= \mathbf{Ax},
\end{align}
where $\mathbf{y} \in \mathbb{R}^M$ is the observed measurement vector consisting of $M$ measurements, $\mathbf{x} \in \mathbb{R}^N$ is a vector of $N$ elements, and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a coefficient matrix which models the linear measurement process column-wise. The linear model consist of $M$ equations and $N$ unknown.
\\ \\
For the case of $\mathbf{A}$ been a square matrix ($M = N$) a solution can be found to the linear model if the $\mathbf{A}$ has full rank -- $\mathbf{A}$ consist of linearly independent columns or rows. For $M > N$ the matrix said to have full rank when the columns are linearly independent. For $M < N$ the matrix has full rank when the rows are linearly independent. For linearly systems/model with $M = N$ is called determined, $M > N$ overdetermined and $M < N$ underdetermined. 
\\
When full rank do not occur the matrix is then called rank-deficient.
\\ \\
By inverting $\mathbf{A}$ from \eqref{eq:model} the unknowing vector $\mathbf{x}$ can be achieved. Square matrix is invertible if an only if its has full rank or its determinant $\det(\mathbf{A}) \neq 0$. For rectangular matrices ($M > N$ and $M < N$) left-sided and right-sided inverse exists:
\begin{itemize}
\item Left-sided inverse ($M > N$): $(\mathbf{A} \mathbf{A})^{-1} \mathbf{A}^T \mathbf{A} = \mathbf{A}_{\text{left}}^{-1} = \mathbf{I}_N$ 
\item Right-sided inverse ($M < N$): $\mathbf{A} \mathbf{A}^T (\mathbf{A} \mathbf{A}^T)^{-1} = \mathbf{A}_{\text{right}}^{-1} = \mathbf{I}_M$ 
\end{itemize}
With the left-inverse the least norm solution of \eqref{eq:model} can be found.
\\ \\
For an determined system there will exist a unique solution. For an overdetermined system there do not exist a solution and for underdetermined systems there exist infinitely many solutions.
\\
For rank-deficient matrices there do not exist an inverse and therefore \eqref{eq:model} can not be solve by inverting the model. But rank-deficient matrices  meaning have non-empty null space leading to infinitely solutions to \eqref{eq:model} \cite[p. ix]{CS}.
\\ \\
As described in chapter \ref{ch:motivation} the linear model of interest consist of $M$ sensors which is known and $N$ sources which is unknown. Furthermore, we also have that $M < N$ -- an underdetermined system. It is therefore of interest to find a solution in the infinitely solution set.

\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery or reconstruction of a signal from a minimal number of observed measurements. Assuming linear acquisition of the original information the relation between the measurements and the signal to be recovered is described by the linear model giving in \eqref{eq:model} \cite{FR}.  \\ \\
%\begin{align}\label{eq:model}
%\mathbf{y} = \mathbf{Ax}.
%\end{align}
%Here $\mathbf{y} \in \mathbb{R}^M$ is the measured data consisting of $M$ observations, $\mathbf{x} \in \mathbb{R}^N$ is the original signal consisting of $N$ possible sources. $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a matrix which models the linear measurement process or in other words it projects each of the $N$ possible sources on to the $M$ observations. $\textbf{A}$ is  referred to as a dictionary or a mixing matrix .\\
In compressive sensing terminology, $\mathbf{x}$ is the signal of interest which is sought recovered by solving the linear system \eqref{eq:model}. In the typical compressive sensing case where $M < N$ the system becomes underdetermined and there are infinitely many solutions, provided that a solution exist. Such system is also referred to as overcomplete \textit{(as the number of column basis vectors is greater than the dimension of the input)}.\\
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal \cite{FR}, hence the term sparse signal recovery.
%Yet another argument; If $M<<N$, it leads to the matrix $\mathbf{A}$ being rank-deficient\textit{(but not necessarily?)} which imply that $\mathbf{A}$ has a non-empty null space and this leads to infinitely many signals which yield the same solution $\mathbf{y} = \mathbf{Ax} = \mathbf{Ax'}$ \cite[p. ix]{CS}. Thus it is necessary to limit the solution space to a specific class of signals $\mathbf{x}$, for this a certain constraint on sparseness is introduced.   

\subsection{Sparseness} 
A signal is said to be $k$-sparse if the signal has at most $k$ non-zero coefficients. For the purpose of counting the non-zero entries of a vector representing a signal the $\ell_0$-norm is defined
\begin{align*}
\Vert \mathbf{x} \Vert_0 := \text{card}(\text{supp}(\mathbf{x})).
\end{align*}
The function $\text{card}(\cdot)$ gives the cardinality of the input and the support vector of $\mathbf{x}$ is given as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set of integers $\{1,2,\hdots,N\}$ \cite[p. 41]{FR}. The set of all $k$-sparse signals is denoted as
\begin{align*}
\Omega_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}

\subsection{Optimisation Problem} 
The signal of interest is assumed to be $k$-sparse with $k < M$. \\
From the desire of finding a $k$-sparse solution $\mathbf{x}$ the following optimisation problem is considered
\\ \\
\begin{align*}
\min_{\Omega_k} \Vert \mathbf{x} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax},
\end{align*}
where $\mathbf{x}$ is all possible candidates to a $k$-sparse signal $\mathbf{x}^\ast$.
Unfortunately, this optimisation problem is non-convex due to the definition of $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. Instead by replacing the $\ell_0$-norm with its convex approximation, the $\ell_1$-norm, the optimisation problem become computational feasible \cite[p. 27]{CS}
\begin{align}\label{eq:p1}
\min_{\mathbf{x}} \Vert \mathbf{x} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax},
\end{align} 
and instead we find the best $k$-term approximation of the signal $\hat{\mathbf{x}}^\ast$.
This method is referred to as Basis Pursuit and makes the foundation of several algorithms solving alternative versions of \eqref{eq:p1} where noise is incorporated. A different type of solution method includes greedy algorithm such as the Orthogonal Matching Pursuit.    
      
\subsection{Conditions on the dictionary}
The construction of the matrix $\mathbf{A}$ is of course essential for the solution of the optimisation problem. So far no one has manage to construct a matrix which is proved to be optimal for some compressive sensing set up. However some certain constructions have shown sufficient recovery guarantee.\\ \\
To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated to the matrix $\mathbf{A}$ must be satisfied.\\
This includes at first the null space condition, a property of the $A$ matrix which is hard to check in practise. The Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix. Furthermore the coherence of a matrix is a measure of quality and is used to determine whether the matrix $A$ is a good choice for the optimisation problem.\\    
\textit{(Is it necessary describe these conditions on A in details?)}

%\subsubsection{Null Space Conditions}
%The null space property (NSP) is some necessary and sufficient condition for exact recovery.
%The null space of the matrix $A$ is defined as
%\begin{align*}
%\mathcal{N}(A) = \{ z \ : \ Az = 0 \}.
%\end{align*} 
%
%
%
%
%%$\ell_p$ norm is given as
%%\begin{align*}
%%\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right)^{1/p}, \quad p \in [1, \infty)
%%\end{align*}
%%the norm is used as an measure of the strenght of a signal or as an error.
%
%\subsubsection{Restricted Isometry Conditions}
%NSP do not take account for noise and we must therefore look at some stronger conditions which incoperate noise, the following restricted isometry property (RIP) %\cite[p. 19]{•}.
%
%\begin{definition}[Restricted Isometry Property]
%A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
%\begin{align*}
%(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
%\end{align*}
%holds for all $x \in \Sigma_k$
%\end{definition}
%
%
%If a matrix $A$ satisfy RIP then it will also satisfy the NSP as RIP is strictly stronger than NSP.
%\begin{theorem}
%If $A$ satisfies the RIP of order $2k$ with the constant $\delta_{2k} < \sqrt{2} -1$. Then
%\begin{align*}
%C = \frac{2}{1 - (1 + \sqrt{2}) \delta_{2k}}
%\end{align*}
%\end{theorem} 
%
%\subsubsection{Coherence}
%The NSP provide a unique solution to the optimisation problem, \eqref{eq:p1}, but is unfortunately complicated to investigate. Instead an alternative measure used for sparsity is presented.
%\\%\cite[p. 24]{•}.
%Coherence is a measure of quality and determine if the matrix $A$ is a good choice for the optimisation problem \eqref{eq:p1}. A small coherence describe the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
%\begin{definition}[Coherence]
%Coherence of the matrix $A \in \mathbb{R}^{M \times N}$, denoted as $\mu (A)$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
%\begin{align*}
%\mu (A) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}.
%\end{align*}
%\end{definition}
%

\subsubsection{Dictionary learning}\label{sec:dictionarylearning}
In cases where the dictionary is unknown it is possible to learn the dictionary from the observed measurement provided that several observations are available $\textbf{Y}=\left[ \textbf{y}_1, \hdots ,\textbf{y}_n\right]$. 
In dictionary learning framework the inverse problem is defined as
\begin{align*}
\min_{A,X} = \frac{1}{2} \sum_{s=1}^{N_d} \Vert \mathbf{Y} - \mathbf{AX} \Vert_F^2 + \gamma \sum_{s=1}^{N_d} g(\mathbf{x}_s),
\end{align*}
where the function $g(\cdot)$ promotes sparsity of the source vector at time $t$ 
The true dictionary $\mathbf{A}$ is recovered if the sources $\mathbf{x}_s$ are sparse ($k_s < M$).
\\  \\
Include specific dictionary learning algorithm here...


\subsection{Multiple Measurement Vector Model}
The linear model \eqref{eq:model} is also referred to as a single measurement model. In order to adapt the model to practical use the model is expanded to include multiple measurements and take noise into account.\\ 
A multiple measurement vector (MMV) model consist of the observation matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ which have $k \leq M$ rows that are non-zero and a dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$:
\begin{align*}
\mathbf{Y} = \mathbf{AX}+\textbf{E},
\end{align*}
where $L$ denote the number of observed samples each consisting of $M$ measurements and $\textbf{E} \in \mathbb{R}^{M \times L}$. From the MMV model the non-zero rows of the source matrix $\mathbf{X}$ are the ones of interest, which one want to recover \cite[p. 11]{phd2015}.
