Through this chapter an introduction to the concept compressive sensing is described with associated theory which later on will be used in the development of the algorithm with used methods known from compressive sensing to estimate the mixing matrix $A$ and the sparse source matrix $X$.

\section{Compressive Sensing}
Compressive sensing is the theory of efficient recover/reconstruct a signal from minimal measurements. This recovery is often described as a linear model/system 
\begin{align*}
\mathbf{y} = \mathbf{Ax},
\end{align*}
which consist of observed data $\mathbf{y} \in \mathbb{R}^M$, a matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ which models the linear measurements and signal $\mathbf{x} \in \mathbb{R}^N$. In compressive sensing terminology, $\mathbf{y}$ is the signal of interest that is wish recovered from minimal measurements meaning that the signal $\mathbf{x}$ must be sparse.
\\
A signal is said to be $k$-sparse if the signal has at most $k$ non-zeros coefficient: 
\begin{align*}
\Vert \mathbf{x} \Vert_0 = \text{card}(\text{supp}(\mathbf{x}) \leq k,
\end{align*}
where $\ell_0$ norm is used. The function $\text{card}$ is the cardinality of the support of $\mathbf{x}$. The support of $\mathbf{x}$ is giving as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set \cite[p. 41]{FR}. The set of all $k$-sparse signal is denoted as
\begin{align*}
\Sigma_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}
We want to recover the signal when there is $M << N$ and $k < M$ \cite[p. 8]{CS}. This lead to that the matrix $\mathbf{A}$ becomes rank-deficient and therefore have a non-empty null-space \cite[p. XX]{CS}.
\\ \\
The linear model and finding the sparse signal $\mathbf{x}$ can be written as an optimisation problem
\begin{align*}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az},
\end{align*}
where $\mathbf{z}$ is all possible candidates to an $k$-sparse signal $\mathbf{x}$.
\\
Unfortunately, this optimisation problem is non-convex because of $\ell_0$ norm and is therefore very difficult to solve leading it to be a NP-hard problem. Instead by replacing the $\ell_0$ norm with its convex approximation, the $\ell_1$ norm, the optimisation problem become computational feasible \cite[p. 27]{CS}:
\begin{align*}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az}.
\end{align*} 

\subsection{Conditions on the Mixing Matrix}
To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated on the matrix $\mathbf{A}$ must be satisfied.

\subsubsection{Null Space Conditions}
The null space property (NSP) is some necessary and sufficient condition for exact recovery.
The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(A) = \{ z \ : \ Az = 0 \}.
\end{align*} 




$\ell_p$ norm is given as
\begin{align*}
\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right)^{1/p}, \quad p \in [1, \infty)
\end{align*}
the norm is used as an measure of the strenght of a signal or as an error.

Then $\ell_0$ norm is np hard to calculate and therefore we seek for an approximation within the $\ell_1$ norm. Therefore,  we instead find the best $k$-term approximation of the signal


\subsubsection{Restricted Isometry Conditions}
NSP do not take account for noise and we must therefore look at some stronger conditions which incoperate noise, the following restricted isometry property (RIP) \cite[p. 19]{•}.

\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}
holds for all $x \in \Sigma_k$
\end{definition}


If a matrix $A$ satisfy RIP then it will also satisfy the NSP as RIP is strictly stronger than NSP.
\begin{theorem}
If $A$ satisfies the RIP of order $2k$ with the constant $\delta_{2k} < \sqrt{2} -1$. Then
\begin{align*}
C = \frac{2}{1 - (1 + \sqrt{2}) \delta_{2k}}
\end{align*}
\end{theorem} 

\subsubsection{Coherence}
Another measure used for sparsity is coherence \cite[p. 24]{•}.
\begin{definition}[Coherence]
Coherence of the matrix $A$, denoted as $\mu (A)$, is the largest absolute value between two columns $a_i$ and $a_j$ from $A$:
\begin{align*}
\mu (A) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}
\end{align*}
\end{definition}