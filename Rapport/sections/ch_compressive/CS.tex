\section{Linear Algebra}
Some measurement vector $\mathbf{y}$ can be described as a linear combinations of a coefficient matrix $\mathbf{A}$ and some vector $\mathbf{x}$ such that
\begin{align}\label{eq:SMV_model}
\mathbf{y} = \mathbf{Ax},
\end{align}
where $\mathbf{y} \in \mathbb{R}^M$ is the observed measurement vector consisting of $M$ measurements, $\mathbf{x} \in \mathbb{R}^N$ is an unknown vector of $N$ elements, and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a coefficient matrix which models the linear measurement process column-wise. 
The linear model makes a system of linear equations with $M$ equations and $N$ unknowns.
\\ \\
In the case of $\mathbf{A}$ being a square matrix, $M = N$, a solution can be found to the linear model if $\mathbf{A}$ has full rank -- $\mathbf{A}$ consist of linearly independent columns or rows.
% For $M > N$ the matrix said to have full rank when the columns are linearly independent. For $M < N$ the matrix has full rank when the rows are linearly independent. 
A linear systems with $M = N$ is called determined, $M > N$ over-determined and $M < N$ under-determined. 
When full rank do not occur the matrix is then called rank-deficient.
\\ \\
By inverting $\mathbf{A}$ from \eqref{eq:SMV_model} the unknown vector $\mathbf{x}$ can be achieved. 
Square matrix is invertible if an only if its has full rank or its determinant $\det(\mathbf{A}) \neq 0$. 
For rectangular matrices, $M > N$ and $M < N$, left-sided and right-sided inverse exists.
%\begin{itemize}
%\item Left-sided inverse ($M > N$): $(\mathbf{A} \mathbf{A})^{-1} \mathbf{A}^T \mathbf{A} = \mathbf{A}_{\text{left}}^{-1} = \mathbf{I}_N$ 
%\item Right-sided inverse ($M < N$): $\mathbf{A} \mathbf{A}^T (\mathbf{A} \mathbf{A}^T)^{-1} = \mathbf{A}_{\text{right}}^{-1} = \mathbf{I}_M$ 
%\end{itemize}
%With the left-inverse the least norm solution of \eqref{eq:SMV_model} can be found\todo{least norm solution?}.
\\ \\
For an determined system there will exist a unique solution.
For an over-determined system there do not exist a solution and for under-determined systems there exist infinitely many solutions \cite[p. ix]{CS}.
\\ 
As described in chapter \ref{ch:motivation} the linear model of interest consist of $M$ sensors which makes the observed measurements $\textbf{y}$ and $N$ sources which makes the unknown vector $\textbf{x}$. 
Here it is of interest to find a solution to the case where the system consist of more sources than sensors -- hence a solution has to be found within the infinitely solution set.

\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery or reconstruction of a signal from a minimal number of observed measurements. 
It is build upon empirical observations assuring that many signals can be approximated by remarkably sparser signals.   
Assume linear acquisition of the original measurements, then the relation between the measurements and the signal to be recovered can be described by the linear model \eqref{eq:SMV_model} \cite{FR}.  
\\ 
%\begin{align}\label{eq:model}
%\mathbf{y} = \mathbf{Ax}.
%\end{align}
%Here $\mathbf{y} \in \mathbb{R}^M$ is the measured data consisting of $M$ observations, $\mathbf{x} \in \mathbb{R}^N$ is the original signal consisting of $N$ possible sources. $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a matrix which models the linear measurement process or in other words it projects each of the $N$ possible sources on to the $M$ observations. $\textbf{A}$ is  referred to as a dictionary or a mixing matrix .\\
In compressive sensing terminology, $\mathbf{x} \in \mathbb{R}^N$ is the signal of interest which is sought recovered from the measurements $\mathbf{y} \in \mathbb{R}^M$ by solving the linear system \eqref{eq:SMV_model}. 
The coefficient matrix $\mathbf{A}$ is in the context of compressive sensing referred to as the mixing matrix or the dictionary matrix.
\\    
In the typical compressive sensing case the system is under-determined, $M < N$,  and there exist infinitely many solutions, provided that a solution exist.
% Such system is also referred to as over-complete \textit{(as the number of column basis vectors is greater than the dimension of the input)}.
\\
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal, hence the term sparse signal recovery \cite{FR}.
%Yet another argument; If $M<<N$, it leads to the matrix $\mathbf{A}$ being rank-deficient\textit{(but not necessarily?)} which imply that $\mathbf{A}$ has a non-empty null space and this leads to infinitely many signals which yield the same solution $\mathbf{y} = \mathbf{Ax} = \mathbf{Ax'}$ \cite[p. ix]{CS}. Thus it is necessary to limit the solution space to a specific class of signals $\mathbf{x}$, for this a certain constraint on sparseness is introduced.   

\subsection{Sparseness} 
A signal is said to be $k$-sparse if the signal has at most $k$ non-zero coefficients. 
For the purpose of counting the non-zero entries of a vector representing a signal the $\ell_0$-norm is defined
\begin{align*}
\Vert \mathbf{x} \Vert_0 := \text{card}(\text{supp}(\mathbf{x})).
\end{align*}
The function $\text{card}(\cdot)$ gives the cardinality of the input and the support vector of $\mathbf{x}$ is given as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ is a set of integers $\lbrace 1, 2, \hdots, N \rbrace$ \cite[p. 41]{FR}. 
The set of all $k$-sparse signals is denoted as
\begin{align*}
\Omega_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}

\subsection{Optimisation Problem}\label{sec:opti}
To find a solution to the linear model \eqref{eq:SMV_model} assuming the solution is $k$-sparse, it can be viewed as an optimisation problem. 
The optimisation problem is defined as
%\begin{align*}
%\min f_0 (\mathbf{x}) \quad \text{subject to} \quad f_i (\mathbf{x}) \leq b_i, \quad i = 1,2, \hdots, n ,
%\end{align*}
%where $f_0 \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ is an objective function and $f_i \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ are the constraint functions. 
%\\
%To find the $k$-sparse solution the optimisation problem can be written as
\begin{align*}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \boldsymbol{\Omega_{k}}} \Vert \mathbf{x} \Vert_0 \quad \text{subject to} \quad \mathbf{y} = \mathbf{Ax}.
\end{align*}
\begin{align*}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \boldsymbol{\Omega_{k}}} \Vert \mathbf{y} - \mathbf{Ax} \Vert_0 \quad \text{subject to} \quad \Vert \mathbf{x} \Vert_0 \leq k.
\end{align*}
The objective function is given by an $\ell_0$ norm with the constraint function being the linear model \eqref{eq:SMV_model}. 
Unfortunately, this optimisation problem is non-convex due to the definition of $\ell_0$-norm and is therefore difficult to solve -- it is an NP-hard problem. 
Instead, by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimisation problem can be approximated and hence becomes computationally feasible\todo{3.2: skal vi indfør z som en approximation til x. og så et nyt omega eller? eller kan vi beholde x} \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in  \boldsymbol{\Omega_{k}}} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{y} = \mathbf{Ax}.
\end{align} 
With this optimisation problem we find the best $k$-sparse solution $\mathbf{x}^\ast$. 
This method is referred to as Basis Pursuit. 
\\
The following theorem justifies that the $\ell_1$ optimisation problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}
A mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined with columns $\mathbf{A} = [\mathbf{a}_1, \dots, \mathbf{a}_N]$. 
By assuming uniqueness of a solution $\mathbf{x}^{\ast}$ to
\begin{align*}
\min_{\mathbf{x} \in \mathbb{R}^N} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y},
\end{align*}
the system $\lbrace \mathbf{a}_j, j \in \text{supp}( \mathbf{x}^\ast) \rbrace$ is linearly independent, and in particular
\begin{align*}
\Vert \mathbf{x}^\ast \Vert_0 = \text{card}(\text{supp} (\mathbf{x}^\ast)) \leq M.
\end{align*}
\end{theorem}
To prove this theorem one needs to realise that the set $\lbrace \mathbf{a}_j, j \in S \rbrace \leq M$, with $S = \text{supp}(\mathbf{x}^\ast)$, can not have more than $M$ linearly independence columns. 
So when $M \ll N$ a sparse signal is automatically achieved.
\begin{proof}
For the case of contradiction, assume that the set $\lbrace \mathbf{a}_j, j \in S \rbrace$ is linearly dependent. 
Thus there exists a non-zero vector $\mathbf{v} \in \mathbb{R}^N$ supported on $S$ such that $\mathbf{Av} = \textbf{0}$. Then, for any $t \neq 0$,
\begin{align*}
\Vert \mathbf{x}^\ast \Vert_1 < \Vert \mathbf{x}^\ast + t \mathbf{v} \Vert_1 = \sum_{j \in S} \vert x_j^\ast + t v_j \vert = \sum_{j \in S} \text{sgn}(x_j^\ast + t v_j )(x_j^\ast + t v_j ).
\end{align*}
If $|t|$ is small enough, namely, $|t| < \min_{j \in S} \frac{\vert x_j^\ast \vert}{\Vert \mathbf{v} \Vert_{\infty}}$, then
\begin{align*}
\text{sgn}(x_j^\ast + t v_j) = \text{sgn}(x_j^\ast), \quad \forall j \in S.
\end{align*}
It then follows that
\begin{align*}
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{j \in S} \text{sgn}(x_j^{\ast})(x_j^{\ast} + t v_j ) = \sum_{j \in S} \text{sgn}(x_j^{\ast})x_j^{\ast} + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j.
\end{align*}
This is a contradiction, because one can always choose a small $t \neq 0$ such that 
\begin{align*}
t \sum_{j \in S} \text{sgn}(x_j^\ast)v_j \leq 0,
\end{align*}
and therefore the set $\lbrace \mathbf{a}_j, j \in S \rbrace$ must be linearly independent.
\end{proof}
The Basis Pursuit algorithm makes the foundation of several algorithms solving alternative versions of \eqref{eq:SMV_p1} where noise is incorporated. 
An alternative solution method includes greedy algorithms such as the Orthogonal Matching Pursuit (OMP) \cite[P. 65]{FR}. 
At each iteration of the OMP algorithm an index set $S$ is updated by adding the index corresponding to a column in $\mathbf{A}$ that best describes the residual, hence greedy.
That is the part of $\mathbf{y}$ that is not yet explained by $\mathbf{Ax}$ is included. 
Then $\mathbf{x}$ is updated as the vector, supported by $S$, which minimize the residual, that is also the orthogonal projection of $\mathbf{y}$ onto the span$\lbrace \mathbf{a}_j \ \vert \ j \in S \rbrace$. The algorithm for OMP can be found in the appendix \ref{alg:OMP}.

\subsection{Conditions on the Mixing Matrix}\label{sec:dic_conditions}
In section \ref{sec:opti} the mixing matrix $\mathbf{A}$ was assumed known, in order to solve the optimisation problem \eqref{eq:SMV_p1}. 
However, in practise it is only the measurement vector $\mathbf{y}$ which is known. 
In this case the mixing matrix $\mathbf{A}$ is considered a estimate of the true mixing matrix. 
\\
To ensure exact or approximately reconstruction of the sparse signal $\mathbf{x}$, the mixing matrix must be constructed with certain conditions in mind. 

\subsubsection{Null Space Condition}
The null space property is a necessary and sufficient condition on $\mathbf{A}$ for exact reconstruction of every sparse signal $\mathbf{x}$ that solves the optimisation problem \eqref{eq:SMV_p1} \cite[p. 77]{FR}. 
The null space of the matrix $\mathbf{A}$ is defined as
\begin{align*}
\mathcal{N}(\mathbf{A}) = \{ \mathbf{z} \ : \ \mathbf{Az} = \textbf{0} \}.
\end{align*} 
The null space property is defined as
\begin{definition}[Null Space Property]
A matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is said to satisfy the null space property relative to a set $S \subset [N]$ if
\begin{align}
\Vert \mathbf{v}_S \Vert_1 < \Vert \mathbf{v}_{\overline{S}} \Vert_1 \quad \text{for all} \quad \mathbf{v} \in \text{null}(\mathbf{A}) \setminus \lbrace \mathbf{0} \rbrace,
\end{align}
where the vector $\mathbf{v}_S$ is the restriction of $\mathbf{v}$ to the indices in $S$, and $\bar{S}$ is the set $[N] \setminus S$. 
\end{definition}

\begin{theorem}
Given a matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$, any vector $\mathbf{x} \in \mathbb{R}^N$ with $\text{supp}(\mathbf{x}) \subset S$ is the unique solution of \eqref{eq:SMV_p1} with $\mathbf{y} = \mathbf{Ax}$ if and only if $\mathbf{A}$ satisfies the null space property relative to $S$.
\end{theorem}
\begin{proof}
$\Rightarrow:$  \\
Let $S \subseteq [N]$ be a fixed index set. Assume that any vector $\mathbf{x} \in \mathbb{R}^N$ with support $\text{supp}(\mathbf{x})\subset S$ is the unique minimizer of $\Vert \mathbf{z} \Vert_1$ with respect to $\mathbf{Az} = \mathbf{Ax}$. 
Thus, for any vector $\mathbf{v} \in \text{null}(\mathbf{A}) \setminus \{\mathbf{0} \}$, the vector $\mathbf{v}_S$ is the unique minimizer of $\Vert \mathbf{z} \Vert_1$ with respect to $\mathbf{Az} = \mathbf{Av}_S$. 
But 
\begin{align*}
\mathbf{0} = \mathbf{A}(\mathbf{v}_S + \mathbf{v}_{\overline{S}}) \implies \mathbf{Av}_S = \mathbf{A}(-\mathbf{v}_{\overline{S}}), \quad \text{with} \ -\mathbf{v}_{\overline{S}} \neq \mathbf{v}_S,
\end{align*}
or else $\mathbf{v} = \mathbf{0}$. 
It is concluded that $\Vert \mathbf{v}_S \Vert_1 < \Vert \mathbf{v}_{\overline{S}} \Vert_1$. 
This establishes the null space property relative to $S$.
\\ \\
$\Leftarrow:$ \\  
Conversely, assume that the null space property relative to $S$ holds. 
Given an index set $S \subseteq [N]$ with null space property and a vector $\mathbf{x} \in \mathbb{R}^N$ with $\text{supp}(\mathbf{x})\subset S$. Furthermore, given a vector $\mathbf{z} \in \mathbb{R}^N$ where $\mathbf{z} \neq \mathbf{x}$, such that $\mathbf{Az} = \mathbf{Ax}$. Consider then a vector $\mathbf{v}$ given by $\mathbf{v} := \mathbf{x} - \mathbf{z} \in \text{null}(\mathbf{A}) \setminus \{ \mathbf{0} \}$. 
From the null space property, the following is obtained:
\begin{align*}
\Vert \mathbf{x} \Vert_1 \leq \Vert \mathbf{x} - \mathbf{z}_S \Vert_1 &= \Vert \mathbf{v}_S \Vert_1 + \Vert \mathbf{z}_S \Vert_1 \\
&< \Vert \mathbf{v}_{\overline{S}} \Vert_1 + \Vert \mathbf{z}_S \Vert_1 \\
&= \Vert -\mathbf{z}_{\overline{S}} \Vert_1 + \Vert \mathbf{z}_S \Vert_1 = \Vert \mathbf{z} \Vert_1.
\end{align*}
This establishes the required sparseness of $\Vert \mathbf{x} \Vert_1$.
\end{proof}
Unfortunately, this is a condition which is hard to check in practice. Instead coherence can be used as a measure on $\mathbf{A}$ where a small coherence lead to a good choice of $\mathbf{A}$. Another condition which can be used in practice is the restricted isometry property (RIP) \cite{??}.

%\subsubsection{Coherence}
%The null space property provide a unique solution to the optimisation problem \eqref{eq:SMV_p1}, but it is unfortunately complicated to investigate. 
%Instead an alternative measure is presented.
%\\
%Coherence is a measure of quality, it determines whether a matrix $\mathbf{A}$ is a good choice for the optimisation problem \eqref{eq:SMV_p1}. 
%A small coherence describes the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
%\begin{definition}[Coherence]
%Coherence of the matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$, denoted as $\mu (\mathbf{A})$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
%\begin{align*}
%\mu (\mathbf{A}) = \max_{1 \leq i < j \leq n} \frac{\vert \langle \mathbf{a}_i, \mathbf{a}_j \rangle \vert}{\Vert \mathbf{a}_i \Vert_2 \Vert \mathbf{a}_j \Vert_2}.
%\end{align*}
%\end{definition}
%
%\subsubsection{Restricted Isometry Condition}
%Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix $\mathbf{A}$.
%\begin{definition}[Restricted Isometry Property (RIP)]
%A matrix $\mathbf{A}$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
%\begin{align*}
%(1 - \delta_k) \Vert \mathbf{x} \Vert_2^2 \leq \Vert \mathbf{Ax} \Vert_2^2 \leq (1 + \delta_k) \Vert \mathbf{x} \Vert_2^2,
%\end{align*}
%\end{definition}
%
%\begin{theorem}
%Suppose that the $2s$-th restricted isometry constant of the matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ satisfies
%\begin{align*}
%    \delta_{2s} < \frac{1}{3}.
%\end{align*}
%Then every $s$-sparse vector $\mathbf{x}^\ast \in \mathbb{R}^N$ is the unique solution of
%$$
%\min_{\mathbf{z} \in \mathbb{R}^N} \Vert \mathbf{z} \Vert_1 \quad \text{subject to} \quad \mathbf{Az} = \mathbf{Ax}.
%$$
%\end{theorem}
%\begin{proof}
%To proof the theorem one only need to show the null space condition:
%\begin{align*}
%\Vert \mathbf{v} \Vert_1 < \frac{1}{2} \Vert \mathbf{v} \Vert_1, \quad \forall \ \mathbf{v} \in \text{ker}(\mathbf{A})\setminus \{\mathbf{0}\}, \ S \subseteq [N], \ \text{card}(S) \leq s.
%\end{align*}
%Cf. Cauchy-Schwarz or $\Vert \mathbf{v}_S \Vert_1 \leq \Vert \mathbf{v}_S \Vert_2 \sqrt{s}$, one only need to show
%\begin{align*}
%\Vert \mathbf{v}_S \Vert_2 &\leq \frac{\rho}{2 \sqrt{s}} \Vert \mathbf{v} \Vert_1 \\
%\rho &= \dfrac{2 \delta_{2s}}{1 - \delta_{2s}} < 1,
%\end{align*}
%whenever $\delta_{2s} < 1/3$. Given $\mathbf{v} \in$ ker$(\mathbf{A})\setminus \{\mathbf{0}\}$, it is enough to consider an index set $S = S_0$ of $s$ largest absolute entries of the vector $\mathbf{v}$. The complement $\overline{S_0}$ of $S_0$ in $[N]$ is partition as $S_0 = S_1 \cup S_2 \cup \cdots$, where
%\begin{align*}
%    S_1 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0}, \\
%    S_2 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0 \cup S_1}.
%\end{align*}
%With $\mathbf{v} \in$ ker$(\mathbf{A})$:
%\begin{align*}
%\mathbf{A}(\mathbf{v}_{S_0}) = \mathbf{A}(-\mathbf{v}_{S_1} - \mathbf{v}_{S_2} - \cdots),
%\end{align*}
%so that
%\begin{align}\label{eq:L9_6}
%\Vert \textbf{v}_{S_0} \Vert_2^2 &\leq \frac{1}{1 - \delta_{2s}} \Vert \textbf{A}(\textbf{v}_{S_0}) \Vert_2^2 = \frac{1}{1 - \delta_{2s}} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_1}) + \textbf{A}(-\textbf{v}_{S_2}) + \cdots \rangle \nonumber \\
%    &= \frac{1}{1 - \delta_{2s}} \sum_{k \geq 1} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle.
%\end{align}
%According to Proposition 6.3 \cite[p. 135]{FR}, one also have
%\begin{align}\label{eq:L9_7}
%\langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle. \leq \delta_{2s} \Vert \textbf{v}_{S_0} \Vert_2 \Vert \textbf{v}_{S_k} \Vert_2.
%\end{align}
%Substituting \eqref{eq:L9_7} into \eqref{eq:L9_6} and dividing by $\Vert \mathbf{v}_{S_0} \Vert_2 > 0$
%\end{proof}
%
%The construction of the matrix $\mathbf{A}$ is of course essential for the solution of the optimisation problem. So far no one has manage to construct a matrix which is proved to be optimal for some compressive sensing set up. However some certain constructions have shown sufficient recovery guarantee.\\ \\
%To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated to the matrix $\mathbf{A}$ must be satisfied.\\
%This includes at first the null space condition, a property of the $A$ matrix which is hard to check in practise. 
%The Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix. Furthermore the coherence of a matrix is a measure of quality and is used to determine whether the matrix $A$ is a good choice for the optimisation problem.\\    
