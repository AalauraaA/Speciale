\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery or reconstruction of a signal from a minimal number of measurements. Assuming linear acquisition of the original information the relation between the measurements and the signal to be recovered is described by a linear model\cite{FR} 
\begin{align}\label{eq:model}
\mathbf{y} = \mathbf{Ax},
\end{align}
where $\mathbf{y} \in \mathbb{R}^M$ is the observed data, $\mathbf{x} \in \mathbb{R}^N$ is the original signal and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a matrix which models the linear measurement process or in other word it maps from $\mathbb{R}^{N}$ to $\mathbb{R}^{M}$.\\
In compressive sensing terminology, $\mathbf{x}$ is the signal of interest that is wished to be recovered by solving the linear system \eqref{eq:model}. In the typical compressive sensing case where $M<N$ the system become underdetermined and there are infinitely many solution, provided that a solution exist. Such system is also referred to as overcomplete(as the number of basis vectors is grather than the dimension of the input). However, by enforcing certian sparsity constraints it is possible to recover the wanted signal\cite{FR}.   
\\ \\
A signal is said to be $k$-sparse if the signal has at most $k$ non-zeros coefficient, for this purpose the $\ell_0$-norm is defined 
\begin{align*}
\Vert \mathbf{x} \Vert_0 := \text{card}(\text{supp}(\mathbf{x})) \leq k,
\end{align*}
The function $\text{card}(\cdot)$ is the cardinality and the support of $\mathbf{x}$ is giving as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set of integers ${1,2,\hdots,N}$ \cite[p. 41]{FR}. The set of all $k$-sparse signals is denoted as
\begin{align*}
\Sigma_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}

If a signal is assumed to be $k$-sparse with $k<M$  
 \cite[p. 8]{CS}. This lead to that the matrix $\mathbf{A}$ becomes rank-deficient and therefore have a non-empty null-space \cite[p. i	x]{CS}.
\\ \\
The linear model and finding the sparse signal $\mathbf{x}$ can be written as an optimisation problem
\begin{align*}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az},
\end{align*}
where $\mathbf{z}$ is all possible candidates to an $k$-sparse signal $\mathbf{x}$.
\\
Unfortunately, this optimisation problem is non-convex because of $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. Instead by replacing the $\ell_0$-norm with its convex approximation, the $\ell_1$-norm, the optimisation problem become computational feasible \cite[p. 27]{CS}
\begin{align}\label{eq:p1}
\min_{\mathbf{z}} \Vert \mathbf{z} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Az},
\end{align} 
and instead we find the best $k$-term approximation of the signal $\mathbf{x}$.

\subsection{Conditions on the Mixing Matrix}
To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated on the matrix $\mathbf{A}$ must be satisfied.

\subsubsection{Null Space Conditions}
The null space property (NSP) is some necessary and sufficient condition for exact recovery.
The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(A) = \{ z \ : \ Az = 0 \}.
\end{align*} 




%$\ell_p$ norm is given as
%\begin{align*}
%\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right)^{1/p}, \quad p \in [1, \infty)
%\end{align*}
%the norm is used as an measure of the strenght of a signal or as an error.

\subsubsection{Restricted Isometry Conditions}
NSP do not take account for noise and we must therefore look at some stronger conditions which incoperate noise, the following restricted isometry property (RIP) %\cite[p. 19]{•}.

\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}
holds for all $x \in \Sigma_k$
\end{definition}


If a matrix $A$ satisfy RIP then it will also satisfy the NSP as RIP is strictly stronger than NSP.
\begin{theorem}
If $A$ satisfies the RIP of order $2k$ with the constant $\delta_{2k} < \sqrt{2} -1$. Then
\begin{align*}
C = \frac{2}{1 - (1 + \sqrt{2}) \delta_{2k}}
\end{align*}
\end{theorem} 

\subsubsection{Coherence}
The NSP provide a unique solution to the optimisation problem, \eqref{eq:p1}, but is unfortunately complicated to investigate. Instead an alternative measure used for sparsity is presented.
\\%\cite[p. 24]{•}.
Coherence is a measure of quality and determine if the matrix $A$ is a good choice for the optimisation problem \eqref{eq:p1}. A small coherence describe the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
\begin{definition}[Coherence]
Coherence of the matrix $A \in \mathbb{R}^{M \times N}$, denoted as $\mu (A)$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
\begin{align*}
\mu (A) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}.
\end{align*}
\end{definition}


\subsection{Multiple Measurement Vector Model}
A multiple measurement vector (MMV) model consist of the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ which have $k < M$ rows that are non-zero (the activations of the sources), a observed mixed data matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$ and a dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$:
\begin{align*}
\mathbf{Y} = \mathbf{AX},
\end{align*}
where $L$ stand for the time samples. From the MMV model the non-zero rows of the source matrix $\mathbf{X}$ are the one of interest that are wanted recovered \cite[p. 11]{PHD}.


\paragraph{Notes:}
\begin{itemize}
\item DL recover more sources than sensors $N > M$ assumning the constraint is at any time we have $k < M$. This cause problems for the use on low density system where we have low $M$.
\item Recovery is not possible if $k \geq M$ since any random dictionary is sufficient to represent data points Y using only M basis vectors.
\item If the source signal is sparse it is enough just to find the non-zero rows of X denoted by the set S, because then the source signal can be obtained by the psudo-inverse solution $\hat{\mathbf{X}} = \mathbf{A}_S^{perp} \mathbf{Y}$ where $\mathbf{A}_S$ is derived from the dictionary matrix $\mathbf{A}$ by deleting the columns assoicated with the zero rows of X. $S$ is called the support. (We identify the locations of sources)
\end{itemize}