\section{Linear Algebra}
A relation between some observed measurements $\mathbf{y}$ can be described as a linear combinations between a coefficient matrix $\mathbf{A}$ and some vector $\mathbf{x}$ such that
\begin{align}\label{eq:SMV_model}
\mathbf{y} &= \mathbf{Ax},
\end{align}
where $\mathbf{y} \in \mathbb{R}^M$ is the observed measurement vector consisting of $M$ measurements, $\mathbf{x} \in \mathbb{R}^N$ is a source vector of $N$ elements, and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a coefficient matrix which models the linear measurement process column-wise. The linear model consist of $M$ equations and $N$ unknown.
\\ \\
For the case of $\mathbf{A}$ been a square matrix ($M = N$) a solution can be found to the linear model if the $\mathbf{A}$ has full rank -- $\mathbf{A}$ consist of linearly independent columns or rows. For $M > N$ the matrix said to have full rank when the columns are linearly independent. For $M < N$ the matrix has full rank when the rows are linearly independent. For linearly systems/model with $M = N$ is called determined, $M > N$ overdetermined and $M < N$ under-determined. 
\\
When full rank do not occur the matrix is then called rank-deficient.
\\ \\
By inverting $\mathbf{A}$ from \eqref{eq:SMV_model} the unknowing vector $\mathbf{x}$ can be achieved. Square matrix is invertible if an only if its has full rank or its determinant $\det(\mathbf{A}) \neq 0$. For rectangular matrices ($M > N$ and $M < N$) left-sided and right-sided inverse exists.
%\begin{itemize}
%\item Left-sided inverse ($M > N$): $(\mathbf{A} \mathbf{A})^{-1} \mathbf{A}^T \mathbf{A} = \mathbf{A}_{\text{left}}^{-1} = \mathbf{I}_N$ 
%\item Right-sided inverse ($M < N$): $\mathbf{A} \mathbf{A}^T (\mathbf{A} \mathbf{A}^T)^{-1} = \mathbf{A}_{\text{right}}^{-1} = \mathbf{I}_M$ 
%\end{itemize}
With the left-inverse the least norm solution of \eqref{eq:SMV_model} can be found.
\\ \\
For an determined system there will exist a unique solution. For an overdetermined system there do not exist a solution and for under-determined systems there exist infinitely many solutions.
\\
For rank-deficient matrices there do not exist an inverse and therefore \eqref{eq:SMV_model} can not be solve by inverting the model. But rank-deficient matrices  meaning have non-empty null space leading to infinitely solutions to \eqref{eq:SMV_model} \cite[p. ix]{CS}.
\\ \\
As described in chapter \ref{ch:motivation} the linear model of interest consist of $M$ sensors which is known and $N$ sources which is unknown. Furthermore, we also have that $M < N$ -- an under-determined system. It is therefore of interest to find a solution in the infinitely solution set.

\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery or reconstruction of a signal from a minimal number of observed measurements. Assuming linear acquisition of the original information the relation between the measurements and the signal to be recovered is described by the linear model giving in \eqref{eq:SMV_model} \cite{FR}.  
\\ \\
%\begin{align}\label{eq:model}
%\mathbf{y} = \mathbf{Ax}.
%\end{align}
%Here $\mathbf{y} \in \mathbb{R}^M$ is the measured data consisting of $M$ observations, $\mathbf{x} \in \mathbb{R}^N$ is the original signal consisting of $N$ possible sources. $\mathbf{A} \in \mathbb{R}^{M \times N}$ is a matrix which models the linear measurement process or in other words it projects each of the $N$ possible sources on to the $M$ observations. $\textbf{A}$ is  referred to as a dictionary or a mixing matrix .\\
In compressive sensing terminology, $\mathbf{x}$ is the signal of interest which is sought recovered by solving the linear system \eqref{eq:SMV_model}. In the typical compressive sensing case where $M < N$ the system becomes under-determined and there are infinitely many solutions, provided that a solution exist. Such system is also referred to as over-complete \textit{(as the number of column basis vectors is greater than the dimension of the input)}.
\\
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal \cite{FR}, hence the term sparse signal recovery.
%Yet another argument; If $M<<N$, it leads to the matrix $\mathbf{A}$ being rank-deficient\textit{(but not necessarily?)} which imply that $\mathbf{A}$ has a non-empty null space and this leads to infinitely many signals which yield the same solution $\mathbf{y} = \mathbf{Ax} = \mathbf{Ax'}$ \cite[p. ix]{CS}. Thus it is necessary to limit the solution space to a specific class of signals $\mathbf{x}$, for this a certain constraint on sparseness is introduced.   

\subsection{Sparseness} 
A signal is said to be $k$-sparse if the signal has at most $k$ non-zero coefficients. For the purpose of counting the non-zero entries of a vector representing a signal the $\ell_0$-norm is defined
\begin{align*}
\Vert \mathbf{x} \Vert_0 := \text{card}(\text{supp}(\mathbf{x})).
\end{align*}
The function $\text{card}(\cdot)$ gives the cardinality of the input and the support vector of $\mathbf{x}$ is given as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set of integers $\{1,2,\hdots,N\}$ \cite[p. 41]{FR}. The set of all $k$-sparse signals is denoted as
\begin{align*}
\Omega_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}

\subsection{Optimisation Problem}\label{sec:opti}
To find a solution to the linear model \eqref{eq:SMV_model} which is $k$-sparse with $k < M$ an optimisation problem can be used. An optimisation problem is defined as
\begin{align*}
\min f_0 (\mathbf{x}) \quad \text{s.t} \quad f_i (\mathbf{x}) \leq b_i, \quad i \in [n],
\end{align*}
where $f_0 \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ is an objective function and $f_i \ : \ \mathbb{R}^N \mapsto \mathbb{R}$ are the constraint functions. 
\\
To find the $k$-sparse solution our optimisation problem can be written as
\begin{align*}
\min_{\Omega_k} \Vert \mathbf{x} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax},
\end{align*}
where $\mathbf{x}$ is all possible candidates to a $k$-sparse signal $\mathbf{x}^\ast$. The objective function is given by an $\ell_0$ norm with the constraint function been the linear model described in \eqref{eq:SMV_model}. Unfortunately, this optimisation problem is non-convex due to the definition of $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. Instead by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimisation problem can approximated and therefore become computational feasible \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\min_{\mathbf{x}} \Vert \mathbf{x} \Vert_1 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax}.
\end{align} 
With this optimisation problem we find the best $k$-term approximation of the signal $\hat{\mathbf{x}}^\ast$. This method is referred to as Basis Pursuit. 
\\
The following theorem justifies that the $\ell_1$ optimisation problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}
A measurement matrix $\mathbf{A}$ is defined as $\textbf{A} \in \mathbb{R}^{M \times N}$ with columns $\textbf{A} = [\textbf{a}_1, \dots, \textbf{a}_N]$. By assuming uniqueness of a solution $\textbf{x}^{\ast}$ of
$$
\min_{\textbf{x} \in \mathbb{R}^N} \Vert \textbf{x} \Vert_1 \quad \text{s.t} \quad \textbf{Ax} = \textbf{y},
$$
the system $\{ \textbf{a}_j, j \in \text{supp} ( \textbf{x}^{\ast})\}$ is linearly independent, and in particular
$$
\Vert \textbf{x}^{\ast} \Vert_0 = \text{card}(\text{supp} (\textbf{x}^{\ast})) \leq M.
$$
\end{theorem}
To prove this theorem we need to realise that the size of the set $\{\textbf{a}_j, j \in S \} \leq M$, that we can not have more than $M$ linearly independence columns. So when $M \ll N$ then we automatically achieve a sparse signal.
\begin{proof}
By way of contradiction, lets assume that the set $\{\textbf{a}_j, j \in S \}$ is linearly dependent with $S =  \text{supp} (\textbf{x}^{\ast})$. This means that there exists a non-zero vector $\textbf{v} \in \mathbb{R}^N$ supported on $S$ such that $\textbf{Av} = \textbf{0}$. Then, for any $t \neq 0$,
$$
\Vert \textbf{x}^{\ast} \Vert_1 < \Vert \textbf{x}^{\ast} + t \textbf{v} \Vert_1 = \sum_{j \in S} \vert x_j^{\ast} + t v_j \vert = \sum_{j \in S} \text{sgn}(x_j^{\ast} + t v_j )(x_j^{\ast} + t v_j ).
$$
If $|t|$ is small enough, namely, $|t| < \min_{j \in S} \frac{|x_j^{\ast}|}{\Vert \textbf{v} \Vert_{\infty}}$, then
$$
\text{sgn}(x_j^{\ast} + t v_j) = \text{sgn}(x_j^{\ast}), \quad \forall j \in S.
$$
It the follows that
$$
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{j \in S} \text{sgn}(x_j^{\ast})(x_j^{\ast} + t v_j ) = \sum_{j \in S} \text{sgn}(x_j^{\ast})x_j^{\ast} + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j.
$$
This is a contradiction, because we can always choose a small $t \neq 0$ such that $t \sum_{j \in S} \text{sgn}(x_j^{\ast})v_j \leq 0$ and therefore the set $\{\textbf{a}_j, j \in S \}$ must be linearly independent.
\end{proof}
The Basis Pursuit makes the foundation of several algorithms solving alternative versions of \eqref{eq:SMV_p1} where noise is incorporated. A different type of solution method includes a greedy algorithm such as the Orthogonal Matching Pursuit \cite[P. 65]{FR}.    
\begin{algorithm}[H]
\caption{Orthogonal Matching Pursuit (OMP)}
\begin{itemize}
\item[1.] Give an measurement matrix $\mathbf{A}$ and a measurement vector $\mathbf{y}$
\item[2.] Initial $S^0 = \emptyset$ and $x_0 = \mathbf{0}$
\item[3.] Iterate until stopping criterion is met:
\begin{align*}
S^{n+1} &= S^n \cup \lbrace j_{n+1} \rbrace, \quad j_{n+1} = \arg \max_{j \in [N]} ( \vert (\mathbf{A}^\ast (\mathbf{y} - \mathbf{Ax}^n))_j\vert) \\
x^{n+1} &= \arg \min_{\mathbf{z} \in \mathbb{C}^N} (\Vert \mathbf{y} - \mathbf{Az} \Vert_2, \text{supp}(\mathbf{z}) \subset S^{n+1})
\end{align*}
\item[4.] $\mathbf{x}^\ast = \mathbf{x}^n$
\end{itemize}      
\end{algorithm}
      
      
\subsection{Conditions on the Dictionary}\label{sec:dic_conditions}
In section \ref{sec:opti} the measurement matrix $\mathbf{A}$ was known regarding to solve the optimisation problem given in \eqref{eq:SMV_p1}. But this is not always the case and instead a dictionary is used to find the measurement matrix $\mathbf{A}$. To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}^\ast$ from the optimisation problem \eqref{eq:SMV_p1} some conditions associated to the matrix $\mathbf{A}$ must be satisfied.

\subsubsection{Null Space Condition}
One condition of $\mathbf{A}$ is the null space property. The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(\mathbf{A}) = \{ \mathbf{z} \ : \ \mathbf{Az} = 0 \}.
\end{align*} 
The null space property is defined as
\begin{definition}[Null Space Property]
A matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is said to satisfy the null space property relative to a set $S \subset [N]$ if
\begin{align}
\Vert \textbf{v}_S \Vert_1 < \Vert \textbf{v}_{\overline{S}} \Vert_1 \quad \text{for all} \quad \textbf{v} \in \text{ker}(\mathbf{A} \setminus \lbrace \textbf{0} \rbrace),
\end{align}
where the vector $\mathbf{v}_S$ is the restriction of $\mathbf{v}$ to the indices in $S$. 
\end{definition}

\begin{theorem}
Given a matrix $\textbf{A} \in \mathbb{R}^{M \times N}$, every vector $\textbf{x} \in \mathbb{R}^N$ supported on a set $S$ is the unique solution of \eqref{eq:SMV_p1}) with $\textbf{y} = \textbf{Ax}$ if and only if $\textbf{A}$ satisfies the null space property relative to $S$.
\end{theorem}
\begin{proof}
$\Rightarrow:$ Given a fixed index set $S \subseteq [N]$, lets first assume that every vector $\textbf{x} \in \mathbb{R}^N$ supported on $S$ is the unique minimiser of $\Vert \textbf{z} \Vert_1$ subject to $\textbf{Az} = \textbf{Ax}$. Thus, for any $\textbf{v} \in \text{ker}(\mathbf{A}) \setminus \{\textbf{0} \}$, the vector $\textbf{v}_S$ is the unique minimizer $\Vert \textbf{z} \Vert_1$ subject to $\textbf{Az} = \textbf{Av}_S$. But we have that $\textbf{0} = \textbf{A}(\textbf{v}_S + \textbf{v}_{\overline{S}}) \implies \textbf{Av}_S = \textbf{A}(-\textbf{v}_{\overline{S}}$ and $-\textbf{v}_{\overline{S}} \neq \textbf{v}_S$ or else $\textbf{v} = \textbf{0}$. We conclude that $\Vert \textbf{v}_S \Vert_1 < \Vert \textbf{v}_{\overline{S}} \Vert_1$. This establishes the null space property relative to $S$.
\\ \\
$\Leftarrow:$ Conversely, lets assume that the null space property relative to $S$ holds. Then, given $S \subseteq [N]$ with null space property and a vector $\textbf{x} \in \mathbb{R}^N$ supported on $S$ and a vector $\textbf{z} \in \mathbb{R}^N$, $\textbf{z} \neq \textbf{x}$, satisfying $\textbf{Az} = \textbf{Ax}$, we consider the vector $\textbf{v} := \textbf{x} - \textbf{z} \in \text{ker}(\textbf{A}) \setminus \{ \textbf{0} \}$. In view of the null space property, we obtain
\begin{align*}
\Vert \textbf{x} \Vert_1 \leq \Vert \textbf{x} - \textbf{z}_S \Vert_1 &= \Vert \textbf{v}_S \Vert_1 + \Vert \textbf{z}_S \Vert_1 \\
&< \Vert \textbf{v}_{\overline{S}} \Vert_1 + \Vert \textbf{z}_S \Vert_1 \\
&= \Vert -\textbf{z}_{\overline{S}} \Vert_1 + \Vert \textbf{z}_S \Vert_1 = \Vert \textbf{z} \Vert_1
\end{align*}
This establishes the required minimality of $\Vert \textbf{x} \Vert_1$.
\end{proof}
Unfortunately, this is a property/condition which is hard to check in practice.

\subsubsection{Coherence}
The null space property provide a unique solution to the optimisation problem, \eqref{eq:SMV_p1}, but is unfortunately complicated to investigate. Instead an alternative measure used for sparsity is presented.
\\
Coherence is a measure of quality and determine if the matrix $\mathbf{A}$ is a good choice for the optimisation problem \eqref{eq:SMV_p1}. A small coherence describe the performance of a recovery algorithm as good with that choice of $\mathbf{A}$. 
\begin{definition}[Coherence]
Coherence of the matrix $A \in \mathbb{R}^{M \times N}$, denoted as $\mu (\mathbf{A})$, with columns $\mathbf{a}_1, \dots, \mathbf{a}_N$ for all $i \in [N]$ is given as
\begin{align*}
\mu (\mathbf{A}) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}.
\end{align*}
\end{definition}

\subsubsection{Restricted Isometry Condition}
Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix $\mathbf{A}$.
\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}%holds for all $x \in \Sigma_k$
\end{definition}

\begin{theorem}
Suppose that the $2s$-th restricted isometry constant of the matrix $\textbf{A} \in \mathbb{R}^{M \times N}$ satisfies
\begin{align*}
    \delta_{2s} < \frac{1}{3}.
\end{align*}
Then every $s$-sparse vector $\textbf{x}^\ast \in \mathbb{R}^N$ is the unique solution of
$$
\min_{\textbf{z} \in \mathbb{R}^N} \Vert \textbf{z} \Vert_1 \quad \text{subject to} \quad \textbf{Az} = \textbf{Ax}.
$$
\end{theorem}
\begin{proof}
To proof the theorem we only need to show the null space condition:
$$
\Vert \textbf{v} \Vert_1 < \frac{1}{2} \Vert \textbf{v} \Vert_1, \quad \forall \ \textbf{v} \in \text{ker}(\textbf{A})\setminus \{\textbf{0}\}, \ S \subseteq [N], \ \text{card}(S) \leq s.
$$
Cf. Cauchy-Schwarz or $\Vert \textbf{v}_S \Vert_1 \leq \Vert \textbf{v}_S \Vert_2 \sqrt{s}$, we only need to show
\begin{align*}
\Vert \textbf{v}_S \Vert_2 &\leq \frac{\rho}{2 \sqrt{s}} \Vert \textbf{v} \Vert_1 \\
\rho &= \dfrac{2 \delta_{2s}}{1 - \delta_{2s}} < 1,
\end{align*}
whenever $\delta_{2s} < 1/3$. Given $\textbf{v} \in$ ker$(\textbf{A})\setminus \{\textbf{0}\}$, it is enough to consider an index set $S = S_0$ of $s$ largest absolute entries of the vector $\textbf{v}$. The complement $\overline{S_0}$ of $S_0$ in $[N]$ is partition as $S_0 = S_1 \cup S_2 \cup \cdots$, where
\begin{align*}
    S_1 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0}, \\
    S_2 \ &: \ \text{index set of } s \text{ largest absolute entries of } \textbf{v} \text{ in } \overline{S_0 \cup S_1}.
\end{align*}
With $\textbf{v} \in$ ker$(\textbf{A})$:
$$
\textbf{A}(\textbf{v}_{S_0}) = \textbf{A}(-\textbf{v}_{S_1} - \textbf{v}_{S_2} - \cdots),
$$
so that
\begin{align}\label{eq:L9_6}
\Vert \textbf{v}_{S_0} \Vert_2^2 &\leq \frac{1}{1 - \delta_{2s}} \Vert \textbf{A}(\textbf{v}_{S_0}) \Vert_2^2 = \frac{1}{1 - \delta_{2s}} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_1}) + \textbf{A}(-\textbf{v}_{S_2}) + \cdots \rangle \nonumber \\
    &= \frac{1}{1 - \delta_{2s}} \sum_{k \geq 1} \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle.
\end{align}
According to Proposition \ref{prop:L9_1}, we also have
\begin{align}\label{eq:L9_7}
    \langle \textbf{A}(\textbf{v}_{S_0}), \textbf{A}(-\textbf{v}_{S_k}) \rangle. \leq \delta_{2s} \Vert \textbf{v}_{S_0} \Vert_2 \Vert \textbf{v}_{S_k} \Vert_2.
\end{align}
Substituting \eqref{eq:L9_7} into \eqref{eq:L9_6} and dividing by $\Vert \textbf{v}_{S_0} \Vert_2 > 0$
\end{proof}

%The construction of the matrix $\mathbf{A}$ is of course essential for the solution of the optimisation problem. So far no one has manage to construct a matrix which is proved to be optimal for some compressive sensing set up. However some certain constructions have shown sufficient recovery guarantee.\\ \\
%To ensure an exact or an approximate reconstruction of the sparse signal $\mathbf{x}$ some conditions associated to the matrix $\mathbf{A}$ must be satisfied.\\
%This includes at first the null space condition, a property of the $A$ matrix which is hard to check in practise. 
%The Restricted isometry condition is a stronger condition concerning the orthogonality of the matrix. Furthermore the coherence of a matrix is a measure of quality and is used to determine whether the matrix $A$ is a good choice for the optimisation problem.\\    

\subsection{Multiple Measurement Vector Model}
The linear model \eqref{eq:SMV_model} is also referred to as a single measurement vector (SMV) model. In order to adapt the model \eqref{eq:SMV_model} to a practical use the model is expanded to include multiple measurements and take noise into account.
\\ \\
A multiple measurement vector (MMV) model consist of the observed measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$ with $k \leq M$ non-zero rows and the dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$:
\begin{align}\label{eq:MMV_model}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align}
$L$ denote the number of observed samples each consisting of $M$ measurements and $\textbf{E} \in \mathbb{R}^{M \times L}$. For $L = 1$ the linear model will just be the SMV model \eqref{eq:SMV_model}. 
\\
The matrix $\mathbf{X}$ consist of $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ sparse vectors which has been stack column-wise leading to that $\mathbf{X}$ consist at most $k$ non-zero rows. As for the SMV model \eqref{eq:SMV_model} the MMV model \eqref{eq:MMV_model} is over-complete with $M \ll N$ and the number of samples is less than $M$, $L < M$ \cite[p. 42]{CS}.
\\ \\
The support of $\mathbf{X}$ denote the index set of non-zero rows of $\mathbf{X}$ and $\mathbf{X}$ is said to be row-sparse. As the columns in $\mathbf{X}$ are $k$-sparse and as mention before $\mathbf{X}$ has at most $k$ non-zero rows, the non-zero values occur in common location. By using this joint information it is possible to recover $\mathbf{X}$ from fewer measurements.
\\
By using the rank of $\mathbf{X}$ which give us information of the amount of linearly independent rows or columns and the spark of $\mathbf{A}$ the minimum set of linearly dependent columns it is possible to set some condition on the measurement to ensure recovery.
\\
When the support of $\mathbf{X}$ is equal to the amount of non-zero rows in $\mathbf{X}$, $\vert \text{supp}(\mathbf{X})\vert = k$ then the rank of $\mathbf{X}$ would be $\text{rank}(\mathbf{X}) \leq k$. If rank$(\mathbf{X}) = 1$ then are the $k$-sparse vectors $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ multiple of each other the joint information cannot be taken advantage of. But for large rank we can exploit the diversity of the columns in $\mathbf{X}$. This can be defined in a sufficient and necessary condition of the MMV model \eqref{eq:MMV_model} which must have
\begin{align*}
\vert \text{supp}(\mathbf{X}) \vert < \frac{\text{Spark} (\mathbf{A}) - 1 + \text{rank}(\mathbf{X})}{2}
\end{align*}
such that $\mathbf{X}$ can uniquely be determined.
\\
This result lead to that row-sparse matrix $\mathbf{X}$ with large rank can be recovered from fewer measurement \cite[p. 43]{CS}.


\subsection{Dictionary learning}\label{sec:dictionarylearning}
As clarified in section \ref{sec:dic_conditions} the choice of dictionary matrix $\textbf{A}$ is essential to achieve the best recovery of a sparse signal $\textbf{x}$ from the measurements $\textbf{y}$. Pre-constructed dictionaries do exist which in many cases results in simple and fast algorithms for reconstruction of $\textbf{x}$\cite{Elad_book}. Pre-constructed dictionaries are typically fitted to a specific kind of data, for instance the discrete Fourier transform or the discrete wavelet transform are used especially for sparse representation of images\cite{Elad_book}. Hence the results of using such dictionaries depend on how well they fit the data of interest, which is creating a certain limitation. An alternative is to consider an adaptive dictionary based on a set of training data that resembles the data of interest. For this purpose learning methods are considered to empirically construct a fixed dictionary which can take part in the application. Different dictionary learning algorithms exist, one is the K-SVD which is to be elaborated in this section. The K-SVD algorithm was presented in 2006 by Elad et al. and found to outperform pre-constructed dictionaries when computational cost is of secondary interest\cite{Elad2006}. \\
\\
Consider now $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$ as a training database, created by $\textbf{y}_i=\textbf{A}\textbf{x}_i$ for which we want to learn the best suitable dictionary $\textbf{A}$ and sparse representation $\textbf{X}=\left[ \textbf{x}_1, \dots ,\textbf{x}_L \right]$. For a known sparsity constraint $k$ this can be defined by an optimisation problem similar to the general compressive sensing problem of multiple measurements \cite{Elad_book}
\begin{align*}
\min_{\mathbf{A,X}} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_2^2 \quad st. \Vert \textbf{x}_i\Vert_0, \ 1\leq i \leq L.
\end{align*}  
The learning consist of jointly solving the optimization problem on $\textbf{X}$ and $\textbf{A}$. The uniqueness of $\textbf{A}$ depends on the recovery sparsity condition. As clarified earlier recovery is only possible if $k < M$\cite{phd2015}. Furthermore, consider $\textbf{A}_0$ such that every training signal can be represented by $k_0 < \text{spark}(\textbf{A}_0)/2$ columns of $\textbf{A}_0$, then $\textbf{A}_0$ is a unique dictionary, up to scaling and permutation of columns\cite{Elad_book}\todo{fungerer disse to uniqueness parameter sammen?}. Again the $\ell_0$-norm lead to an NP-hard problem an heuristic methods are need.     

\subsubsection{K-SVD}
K-SVD is a dictionary learning algorithm where the... 

______\\
In cases where the dictionary is unknown it is possible to learn the dictionary from the observed measurement provided that several observations are available $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$. 
In dictionary learning framework the inverse problem is defined as
\begin{align*}
\min_{\mathbf{A,X}} = \frac{1}{2} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_F^2 + \gamma \sum_{i=1}^{L} g(\mathbf{x}_i),
\end{align*}
where the function $g(\cdot)$ promotes sparsity of the source vectors at sample $i$. $\Vert \cdot \Vert_F$ is the Frobenius norm which is a vector norm defined as 
\begin{align*}
\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
\end{align*}
The true dictionary $\mathbf{A}$ is recovered if the sources $\mathbf{x}_s$ are sparse ($k_s < M$).
\\  \\
Include specific dictionary learning algorithm here...

