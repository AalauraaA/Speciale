Through this chapter an introduction to the concept compressive sensing is described with associated theory which later on will be used in the development of the algorithm with used methods known from compressive sensing to estimate the mixing matrix $A$ and the sparse source matrix $X$.

\section{Compressive Sensing}
Compressive sensing is the theory of efficient recover/reconstruct a signal from minimal measurements. This recovery is often described as a linear model/system 
\begin{align*}
\mathbf{y} = \mathbf{Ax},
\end{align*}
which consist of observed data $\mathbf{y} \in \mathbb{R}^M$, a matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ which models the linear measurements and signal $\mathbf{x} \in \mathbb{R}^N$. In compressive sensing terminology, $\mathbf{y}$ is the signal of interest that is wish recovered from minimal measurements meaning that the signal $\mathbf{x}$ must be sparse.
\\
A signal is said to be $k$-sparse if the signal has at most $k$ non-zeros coefficient: 
\begin{align*}
\Vert \mathbf{x} \Vert_0 = \text{card}(\text{supp}(\mathbf{x}) \leq k,
\end{align*}
where $\ell_0$ norm is used. The function $\text{card}$ is the cardinality of the support of $\mathbf{x}$. The support of $\mathbf{x}$ is giving as
\begin{align*}
\text{supp}(\mathbf{x}) = \{ j \in [N] \ : \ x_j \neq 0 \},
\end{align*} 
where $[N]$ a set \cite[p. 41]{FR}. The set of all $k$-sparse signal is denoted as
\begin{align*}
\Sigma_k = \{ \mathbf{x} \ : \ \Vert \mathbf{x} \Vert_0 \leq k \}.
\end{align*}
We want to recover the signal when there is $M << N$ and $k < M$ \cite[p. 8]{CS}.
\\ \\
The linear model and finding the sparse signal $\mathbf{x}$ can be written as an optimisation problem
\begin{align*}
\min_{\mathbf{x}} \Vert \mathbf{x} \Vert_0 \quad \text{s.t} \quad \mathbf{y} = \mathbf{Ax}.
\end{align*}



Compressive sensing is used to recover high-dimensional signal from incomplete measurements using efficient algorithm. In compressive sensing a linear model is used to describe the relationship:
$$
y = Ax,
$$
where $x$ is a $1 \times N$ vector and $A$ is a matrix of size $M \times N$.

As we want $M << N$ then $A$ becomes rank-deficieny and therefore have a nonempty nullspace.

We want x to be a sparse representation, meaning that we have a signal of lenght N we want to represent it with k << N nonzero coefficient


signal can be well-approximated from a linearly combination of few elements extracted from a known basis or dictionary. If the representation is exact then the signal is sparse.

$\ell_p$ norm is given as
\begin{align*}
\Vert x \Vert_p = \left( \sum_{i=1}^n \vert x_i \vert^p \right)^{1/p}, \quad p \in [1, \infty)
\end{align*}
the norm is used as an measure of the strenght of a signal or as an error.

Then $\ell_0$ norm is np hard to calculate and therefore we seek for an approximation within the $\ell_1$ norm. Therefore,  we instead find the best $k$-term approximation of the signal



some conditions must be satisfied to insure that we recover all sparse representation of a signal. Some of the most known conditions explore the null space: Null Space Conditions.
\\
The null space of the matrix $A$ is defined as
\begin{align*}
\mathcal{N}(A) = \{ z \ : \ Az = 0 \}.
\end{align*} 



NSP do not take account for noise and we must therefore look at some stronger conditions which incoperate noise, the following restricted isometry property (RIP) \cite[p. 19]{•}.

\begin{definition}[Restricted Isometry Property]
A matrix $A$ satisfies the RIP of order $k$ if there exists a $\delta_k \in (0,1)$ such that
\begin{align*}
(1 - \delta_k) \Vert x \Vert_2^2 \leq \Vert Ax \Vert_2^2 \leq (1 + \delta_k) \Vert x \Vert_2^2,
\end{align*}
holds for all $x \in \Sigma_k$
\end{definition}


If a matrix $A$ satisfy RIP then it will also satisfy the NSP as RIP is strictly stronger than NSP.
\begin{theorem}
If $A$ satisfies the RIP of order $2k$ with the constant $\delta_{2k} < \sqrt{2} -1$. Then
\begin{align*}
C = \frac{2}{1 - (1 + \sqrt{2}) \delta_{2k}}
\end{align*}
\end{theorem} 


Another measure used for sparsity is coherence \cite[p. 24]{•}.
\begin{definition}[Coherence]
Coherence of the matrix $A$, denoted as $\mu (A)$, is the largest absolute value between two columns $a_i$ and $a_j$ from $A$:
\begin{align*}
\mu (A) = \max_{1 \leq i < j \leq n} \frac{\vert \langle a_i, a_j \rangle \vert}{\Vert a_i \Vert_2 \Vert a_j \Vert_2}
\end{align*}
\end{definition}