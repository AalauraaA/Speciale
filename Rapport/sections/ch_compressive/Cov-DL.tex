\section{Covariance-Domain Dictionary Learning}\label{sec:Cov-DL}
Covariance-domain dictionary learning (Cov-DL) is an algorithm which can identify more sources $N$ than sensors $M$ for the linear model of observed EEG data
\begin{align*}
\mathbf{Y} = \mathbf{AX}.
\end{align*}
Cov-DL takes advantage of dictionary framework and transformation into another domain -- covariance domain -- to recover the mixing matrix $\mathbf{A}$ from the observed data $\mathbf{Y}$. Cov-DL work together with another algorithm to find the sparse source matrix $\mathbf{X}$, in this thesis M-SBL is used for the source recovery and is described in section \ref{sec:M-SBL}. 
\\
In the following section we assume that $\mathbf{X}$ is known but in practice a random sparse matrix will be used to represent the sources. 
\\
This section is inspired by chapter 3 in \cite{phd2015} and the article \cite{Balkan2015}.
\\ \\
(?)\\
In dictionary learning framework the inverse problem is defined as
\begin{align*}
\min_{A,X} = \frac{1}{2} \sum_{s=1}^{N_d} \Vert \mathbf{Y} - \mathbf{AX} \Vert_F^2 + \gamma \sum_{s=1}^{N_d} g(\mathbf{x}_s),
\end{align*}
where the function $g(\cdot)$ promotes sparsity of the source vector at time $t$ 
The true dictionary $\mathbf{A}$ is recovered if the sources $\mathbf{x}_s$ are sparse ($k_s < M$).


\paragraph{Introduction to Our Covariances:}
Let $s$ be the time segments that $\mathbf{Y}$ is divided into and let it be sampled with the frequency $S_f$ such that our observed data is known overlapping segments $\mathbf{Y}_s \in \mathbb{R}^{M \times t_s S_f}$ where $t_s$ is the length of the segments in seconds. With the segments the linear model still holds and is rewritten into
\begin{align*}
\mathbf{Y}_s = \mathbf{AX}_s, \quad \forall s.
\end{align*}
The sources are assumed uncorrelated in time segments of the whole time scheme of the observed data. 
\\
In the covariance-domain, the observed segmented data $\mathbf{Y}_s$ is described by its covariance:
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{Y}_s} &= \mathbf{A} \boldsymbol{\Lambda} \mathbf{A}^T + \mathbf{E}_s \\
\text{vech}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \sum_{i=1}^N \boldsymbol{\Lambda}_{s_{ii}} \text{vech}(\mathbf{a}_i \mathbf{a}_i^T) + \text{vech}(\mathbf{E}_s) \\
\text{vech}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \mathbf{D} \boldsymbol{\delta}_s + \text{vech}(\mathbf{E}_s), \quad \forall s.
\end{align*}
The vector $\boldsymbol{\delta}_s$ contains the diagonal entries of the source sample-covariance matrix
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{X}_s} = \frac{1}{L_s} \mathbf{X}_s \mathbf{X}_s^T,
\end{align*}
and the matrix $\mathbf{D} \in \mathbb{R}^{M(M+1)/2 \times N}$ consists of the columns $\mathbf{d}_i = \text{vech}(\mathbf{a}_i \mathbf{a}_i^T)$. D and $\delta_s$ are unknown.
\\
Our goal is to learn $\mathbf{D}$ and the find the associated matrix $\mathbf{A}$. When we have the dictionary matrix we can find  the mixing matrix by
\begin{align*}
\min_{\mathbf{a}_i} \Vert \mathbf{d}_i - \text{vech}(\mathbf{a}_i \mathbf{a}_i^T) \Vert_2^2.
\end{align*}




\paragraph{Introduction to Covariance Domain Transformation:}
By the assumption of uncorrelated sources, the sample covariance source matrix is given as
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{X}_s} &= \frac{1}{L_s} \mathbf{X}_s \mathbf{X}_s^T \\
&= \boldsymbol{\Lambda} + \mathbf{E},
\end{align*}
where $\boldsymbol{\Lambda}$ is the diagonal matrix of $\boldsymbol{\Sigma}_{\mathbf{X}_s}$. With this mindset, the linear model given in \eqref{} can then be modelled as
\begin{align*}
\mathbf{Y}_s \mathbf{Y}_s^T &= \mathbf{AX}_s \mathbf{X}_s^T \mathbf{A}^T \\
\boldsymbol{\Sigma}_{\mathbf{Y}_s} &= \mathbf{A} \boldsymbol{\Sigma}_{\mathbf{X}_s} \mathbf{A}^T \\
\boldsymbol{\Sigma}_{\mathbf{Y}_s} &= \mathbf{A} \boldsymbol{\Lambda} \mathbf{A}^T + \mathbf{E} \\
&= \sum_{i=1}^N \boldsymbol{\Lambda}_{ii} \mathbf{a}_i \mathbf{a}_i^T + \mathbf{E}.
\end{align*}
As the covariance matrices are symmetric the lower triangular part can be vectorised:
\begin{align*}
\text{vech}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \sum_{i=1}^N \boldsymbol{\Lambda}_{ii} \text{vector}(\mathbf{a}_i \mathbf{a}_i^T) + \text{vech}(\mathbf{E}) \\
\text{vech}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \sum_{i=1}^N \boldsymbol{\Lambda}_{ii} \mathbf{d}_i + \text{vech}(\mathbf{E}) \\
\text{vech}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \mathbf{D} \delta + \text{vech}(\mathbf{E}),
\end{align*}
where $\mathbf{d}_i = \text{vech}(\mathbf{a}_i \mathbf{a}_i^T)$. The size of the vectorised covariance matrices is $\frac{M(M+1)}{2}$.
\\
By use of the covariance domain it is possible to identify $\mathcal{O}(M^2)$ sources given the true dictionary matrix $\mathbf{A}$.


\paragraph{Notes}
Ting der skal uddybes:
\begin{itemize}
\item 
\item
\end{itemize}

Andet:
\begin{itemize}
\item In the case of EEG, this allows at most k = O(M) EEG sources to be simultaneously active which limits direct applicability of dictionary learning to low-density EEG systems.
\item We wish to handled cases where we have $\binom{N}{k}$ sources, where $1 \leq k \leq N$ can be jointly active.
\item section 3.3.1 in phd.
\item i phd så forstås source localisation som at finde support for x og source identification forstås som at finde værdierne i de non-zero indgange(?)

\end{itemize}

Trine indhold:
\begin{itemize}
\item til CS afsnit: kilde på at nok sparse giver den rigtige løsning, se [39] fra phd
\item Dictionary learning:
\begin{itemize}
\item jointly solveing optimisation problem where g is introduced
\item her siges at k<M= er nødvendigt for recovery( af x går jeg ud fra ) Fordi at enhver random dictionary kan bruges til at reprensentere y i dim M ved brug af M basis vektore i A.
\item altså når k > M så udgør de berørte søjler i A ikke længere en basis og x kan ikke bestemmes entydigt ? derfor er k<M et krav i standard algorithmer
\item der sættes så et nyt frame work hvor vi kan bruge standard algoritmer i vores tilfælde med k>M (forhåbenligt) 
\end{itemize}
\item EEG is non-stationary(afsnit 1.3.1 phd) source dynamics ændres med tiden afhængig af opave, fejlkilder kan være non stationære. så ICA dur ikke mixture model ICA var bedre til non stationær med statigvæk limited til k = M. 
\item the contribution er så at: support for X i MMV problem kan findes for k>M, altså altal non-zero entry k i x, med sufficient conditions ved sparse bayesian learning. herefter ved brug at uncorrelation af sources i et dictionary learning frame work så kan vi recover x når k>M. 
\end{itemize}  
  















