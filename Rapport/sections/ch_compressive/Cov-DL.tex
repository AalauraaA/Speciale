\section{Covariance-Domain Dictionary Learning}\label{sec:Cov-DL}
Covariance-domain dictionary learning (Cov-DL) is an algorithm proposed in \cite{Balkan2015} which claims to be able to identify more sources $N$ than available observations $M$ for the linear model.
\\
The section is inspired by chapter 3 in \cite{phd2015} and the article \cite{Balkan2015}.
\\ \\
%\textit{
%Notes, maybe prior to this section: 
%\begin{itemize}
%\item To avoid this Cov-DL was proposed by O. Balkan. Though the theory of getting from $M$ sources to $M^2$ was established in \cite{Pal2015}. saying that the design of the measurement/dictionary matrix is essential to overcome this issue, new conditions will be developed for this to be a success \cite{Pal2015},
%\item about the approach: "This will in turn lead to the development of new sampling
%schemes and justify the need for the use of nested and
%coprime sampling. Another noteworthy point is that we distinguish
%the recovery of the sparse vector from that of its support."\cite{Pal2015}. \\
%\item In \cite{Pal2015} it is in fact the support of $x$ which if found by use of the covariance domain, and this is what Balkan uses on the dictionary matrix i suppose.
%\item the assumed prior(which has not been exploited before \cite{Pal2015}): a prior on the correlation of the received signal is to assume a (pseudo)diagonal covariance matrix for the unknown signal, that is $\frac{1}{L_s}\textbf{X}_{s}\textbf{X}_s^{T}$\cite{Pal2015}, which is then under the assumption of uncorrelated sources in $x$\cite{phd2015}. This is what lead to the correlation constraint (3.3) in phd.  
%\item the conditions mentioned on the measurement/dictionary matrix in \cite{Pal2015} is: "Sparse sampling
%schemes such as nested and coprime sampling will be shown to
%satisfy these conditions". For $\textbf{A}$ we need "$O(M^2)$ Kruskal Rank for their Khatri-Rao products". This is not necessarily what Balkan uses.   
%\item The prior mentioned by Balkan is the uncorrelated sources. 
%\end{itemize}
%We know:
%\begin{itemize}
%\item Covariance matrix:(when we have several observations) $\Sigma_{\textbf{x}} = Cov(\textbf{x}) = Cov(\textbf{x},\textbf{x}) = \mathbb{E}[ \textbf{x}\textbf{x}^T ]-\mu_{x}\mu_{x^{T}}$ , symmetric and positive simidefinite.
%\item Sample covariance matrix: $'\Sigma_{\textbf{x}} = \frac{1}{L} \sum_{i=1}^{L}(\textbf{x}_i-\mathbb{E}[\textbf{x}])(\textbf{x}_i-\mathbb{E}[\textbf{x}])^T$ using unbiased estimate since we assume $\mathbb{E}[\textbf{x}]=0$
%\item this becomes $\frac{1}{L}\textbf{X}\textbf{X}^T$, where L is the length of the sample segment.  
%\item the covariance of two vectors $cor(\textbf{x},\textbf{y})$ is also referred to as the cross-correlation.
%\end{itemize}
%}
Consider the multiple measurement vector model as above
\begin{align*}
\mathbf{Y} = \mathbf{AX}+\textbf{E},
\end{align*}
which consist of multiple snapshots in time such that the model consist of several measurements. The observed data becomes a matrix of size $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source a matrix of size $\mathbf{X} \in \mathbb{R}^{N \times L}$ and the added noise is of size $\mathbf{E} \in \mathbb{R}^{M \times L}$ \todo{snakker vi ikke lidt dobbelt her? L er længden af vores snapshot men længere nede dele vi hvor data ind i samples rate gange tid og dropper L notationen}.
\\
Let $s$ be the index of time segments that the observed data $\mathbf{Y}$, is divided into and let $f_s$ be the sample frequency the observed data has been\todo{Her blev jeg i tvivl :)} sampled in. As such the observed data is divided into segments $\mathbf{Y}_s \in \mathbb{R}^{M \times t_s f_S}$, possibly overlapping, where $t_s$ is the length of the segments in seconds. For each segment the linear model still holds and is rewritten into
\begin{align*}
\mathbf{Y}_s = \mathbf{AX}_s + \textbf{E}_s, \quad \forall s.
\end{align*}
Cov-DL takes advantage of the dictionary framework and transformation into another domain -- covariance domain -- to recover the mixing matrix $\mathbf{A}$ from the observed data $\mathbf{Y}$. An important aspect of this method is the prior assumption that the sources are statistical independent between their columns within the defined time segments. This implies the entries in $\textbf{X}_s$ to be uncorrelated \todo{Lige lidt hjælp her. Kan ikke huske snakken om og det kun skulle være independnet}.\\  
\\
As the Cov-DL algorithm only focus on finding the mixing matrix $\mathbf{A}$ the algorithm must collaborate with another algorithm to determine the source matrix $\mathbf{X}$. In this thesis the Multiple Sparse Bayesian Learning (M-SBL) is used for the source recovery and is described in section \ref{sec:M-SBL} \todo{en ny section. Tjek lige om det er bedre formuleret}. 
%Cov-DL works together with another algorithm to find the source matrix $\mathbf{X}$, in this thesis Multiple Sparse Bayesian Learing (M-SBL) is used for the source recovery and is described in section \ref{sec:M-SBL}. 
\\
In this section we assume that $\mathbf{X}$ is known and a random sparse matrix is used to represent the sources. 


\subsection{Covariances domain representation}
The observed data $\mathbf{Y}_s$ can be described in the covariance domain by the sample covariance matrix. Considering the observations $\mathbf{Y}_s \in \mathbb{R}^{M\times L}$ the sample covariance is defined to find the covariance among the $M$ observations across the $L$ snapshots in time\todo{Tjek lige om det er rigtig. Vi nævner jo M som værende observationer og L som snapshot (før stod der at L var observationer).}, that is essentially the covariance matrix averaged over all observations, resulting in a $M \times M$ matrix $\boldsymbol{\Sigma}_{\mathbf{Y}_s}=[\sigma_{jk}]$. Each entry is defined by[wiki?] 
\begin{align*}
\sigma_{jk}= \frac{1}{L}\sum_{i=1}^{L}(y_{ji}- \bar{y}_j)(y_{ki}-\bar{y}_k)
\end{align*}
Let the observations(argument for at vi antager zero mean på vores observationer, pre-normalisering?) be normalised resulting in zeros mean $\mathbb{E}_s[\mathbf{Y}_s]=0$. \\
Using matrix notation the sample covariance of $\mathbf{Y}_s$ can be written as
\begin{align*}
\hat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = \frac{1}{L} \mathbf{Y}_s \mathbf{Y}_s^T
\end{align*}  
Similar the sources $\mathbf{X}_s$ can be described in the covariance domain by the sample covariance matrix
\begin{align*}
\hat{\boldsymbol{\Sigma}}_{\mathbf{X}_s} &= \frac{1}{L} \mathbf{X}_s \mathbf{X}_s^T \\
&= \boldsymbol{\Lambda}_s
\end{align*}
From the assumption of uncorrelated sources the sample covariance matrix is expected to be nearly diagonal, thus it can be expressed as $\boldsymbol{\Lambda}_s$ which is a diagonal matrix consisting of the diagonal entries of $\hat{\boldsymbol{\Sigma}}_{\mathbf{X}_s}$ \cite{Balkan2015}.
%\begin{align*}
%\boldsymbol{\Sigma}_{\mathbf{X}_s} &= \Lambda + \textbf{E}_s.
%\end{align*}
%where $\Lambda$ is a diagonal matrix consisting of the diagonal entries of $\Sigma_{\textbf{X}_s}$ and $\textbf{E}_s$ contains the remaining entries which is expected to be zeros or nearly zeros\cite{Balkan2015}.
\\ \\
Each segment of observations can be modelled as\todo{skal vi have $\mathbb{E}_s[]$ omkring her eller ej?, wiki covariance.\\
giver det mening at udelade $\frac{1}{L}$ på begge sider her?}
\begin{align}\label{eq:cov_model}
\hat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = \mathbf{Y}_s \mathbf{Y}_s^T &= \mathbf{A} \mathbf{X}_s \mathbf{X}_s^T \mathbf{A} + \mathbf{E}_s \mathbf{E}_s^T + 2 \mathbf{A} \mathbf{X}_s \mathbf{E}_s^T \nonumber \\
&= \mathbf{A} \boldsymbol{\Lambda}_s \mathbf{A}^T + \boldsymbol{\Sigma}_{\mathbf{E}_s} \nonumber \\
&= \mathbf{A} \boldsymbol{\Sigma}_{\mathbf{X}_s} \mathbf{A}^T + \mathbf{A} \boldsymbol{\Phi_s} \mathbf{A}^T  \nonumber \\
&= \mathbf{A} \boldsymbol{\Lambda}_s \mathbf{A}^T + \mathbf{A} \boldsymbol{\Phi_s} \mathbf{A}^T \nonumber \\
&= \sum_{i=1}^{N} \boldsymbol{\Lambda}_{s_{ii}} \textbf{a}_i\textbf{a}_i^{T} + \mathbf{A} \boldsymbol{\Phi_s} \mathbf{A}^T
\end{align}
Remember that $N$ is the dimension of $\mathbf{X}$ hence the number of possible sources and $k$ is the number of active sources.



It has been shown that by this model it is possible to identify $O(M^2)$ sources given the true dictionary\cite{Pal2015}(dog er vektoriseringen også includeret i resultatet - så måske flyttes udtalelsen?). The purpose of the Cov-DL algorithm is instead to find the dictionary $\textbf{A}$ from this expression and then still allow for $O(M^2)$ sources to be identified.

\subsection{Determination of the Dictionary}
In order to enable the possibility of identifying $O(M^2)$ sources and learning the corresponding dictionary $\mathbf{A}$ the model in \eqref{eq:cov_model} is rewritten\todo{er det muligt at vektoriceringen kun er til for at gøre dictionary learning problemet simplere? og ikke har noget med $O(M^2)$ at gøre, læs Pal2015}.  
At first both sides of the expression is vectorized. Because the covariance matrix is symmetric it is sufficient to vectorize only the lower triangular parts, including the diagonal. For this the function $\text{vec}(\cdot)$ is defined to map a symmetric $M \times M$ matrix into a vector of size $\frac{M(M+1)}{2}$ making a vectorization of its lower triangular part. Further let $\text{vec}^{-1}(\cdot)$ be the inverse function. Note that $\boldsymbol{\Lambda}_{s_{ii}}$ is a scalar hence not vectorized     
\begin{align}\label{eq:cov1}
\text{vec}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \sum_{i=1}^N \boldsymbol{\Lambda}_{s_{ii}} \text{vec}(\mathbf{a}_i \mathbf{a}_i^T) + \text{vec}(\mathbf{E}_s) \nonumber \\
&= \sum_{i=1}^N \mathbf{d}_i \boldsymbol{\Lambda}_{s_{ii}} + \text{vec}(\mathbf{E}_s) \nonumber \\
&= \mathbf{D} \boldsymbol{\delta}_s + \text{vec}(\mathbf{E}_s), \quad \forall s.
\end{align}
The vector $\boldsymbol{\delta}_s \in \mathbb{R}^{N}$ contains the diagonal entries of the source sample-covariance matrix $\boldsymbol{\Lambda}_s$
and the matrix $\mathbf{D} \in \mathbb{R}^{M(M+1)/2 \times N}$ consists of the columns $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$. Note that $\mathbf{D}$ and $\boldsymbol{\delta}_s$ are unknown while the left hand side is known from the observed data.
\\
With the a dictionary learning method such as K-SVD as mentioned in \ref{sec:K-SVD} it is possible to learn the unknown variables $\mathbf{D}$ and $\boldsymbol{\delta}_s$ for each of the segments $s$. With the learned variables \eqref{eq:cov1} can be used to find the mixing matrix $\mathbf{A}$. Comparing this expression to the original compressive sensing problem \eqref{eq:MMV_model} it is clear that higher dimensionality is achieved by representing the problem in the covariance domain. We now look at a problem which have $N$ unknowns in the sample covariance matrix $\boldsymbol{\Lambda}$ than the original $N \cdot L$ unknowns in $\mathbf{X}$
%Which allows for the number of active sources to exeed the number of observations(måske for tidligt at konkludere her).
\\ \\
The Cov-DL method consists of two different algorithms for  recovery of $\mathbf{D}$ and $\mathbf{A}$ depending on the number of total sources $N$ relative to the number of observations $M$. 

\subsubsection*{Cov-DL1 -- Over-complete \textbf{D}}
The case where $N > \frac{M(M+1)}{2}$ results in an over-complete system similar to the original system being over-complete when $N>M$. 
Though, it is again possible to solve the over-complete system if certain sparsity is withhold. Namely $\boldsymbol{\delta}_s$ being $\frac{M(M+1)}{2}$-sparse. Note that this sparsity constraint is  weaker than the original constraint $k < M$, which allows for identification of remarkably more sources than within the original domain as it is not necessarily violated when $k > M$.
Assuming a sufficient sparsity on $\boldsymbol{\delta}_s$ it is possible to learn the dictionary matrix of the covariance domain $\mathbf{D}$ by traditional dictionary learning methods, as introduced in section \ref{sec:dictionarylearning}, applied to the observations represented in the covariance domain $\text{vec}(\boldsymbol{\Sigma}_{\mathbf{Y}_s})\ \forall s$.
\\
When $\mathbf{D}$ is known it is possible to find the original mixing matrix $\mathbf{A}$ through the relation $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$.
\\
To do so each column is found by the optimisation problem 
\begin{align*}
\min_{\textbf{a}_i} \| \text{vec}^{-1}(\textbf{d}_i) -\textbf{a}_i\textbf{a}_i^T\|_2^2, 
\end{align*}
for which the global minimizer is $\mathbf{a}^{\ast}_i=\sqrt{\lambda_i} \textbf{b}_i$. Here $\lambda_i$ is the largest eigenvalue of $\text{vec}^{-1}(\textbf{d}_i)$,
\begin{align*}
\text{vec}^{-1}(\textbf{d}_i) = 
\begin{bmatrix}
d_{11} & d_{12} & \cdots & d_{1N} \\
d_{21} & d_{22} & \cdots & d_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
d_{N1} & d_{N2} & \cdots & d_{NN}
\end{bmatrix}, \quad i \in [N]
\end{align*}
and $\textbf{b}_i$ is the corresponding eigenvector.\todo{redegørelse for resultatet her skal laves}
  
  
\begin{algorithm}[H]
\caption{Cov-DL1 -- over-complete $\mathbf{D}$}
\begin{itemize}
\item[1.] Segmentation of measurements 
\item[2.] Transformation to covariance domain
\item[3.] Vectorising 
\item[4.] Learn D, by ?
\item[5.] Find A
\end{itemize}
\end{algorithm}


\subsubsection*{Cov-DL2 -- undercomplete \textbf{D}}




%\paragraph{Notes}
%
%\begin{itemize}
%\item In the case of EEG, this allows at most k = O(M) EEG sources to be simultaneously active which limits direct applicability of dictionary learning to low-density EEG systems.
%\item We wish to handled cases where we have $\binom{N}{k}$ sources, where $1 \leq k \leq N$ can be jointly active.
%\item section 3.3.1 in phd.
%\item i phd så forstås source localisation som at finde support for x og source identification forstås som at finde værdierne i de non-zero indgange(?)
%
%\end{itemize}
%
%Trine indhold:
%\begin{itemize}
%\item til CS afsnit: kilde på at nok sparse giver den rigtige løsning, se [39] fra phd
%\item Dictionary learning:
%\begin{itemize}
%\item jointly solveing optimisation problem where g is introduced
%\item her siges at k<M= er nødvendigt for recovery( af x går jeg ud fra ) Fordi at enhver random dictionary kan bruges til at reprensentere y i dim M ved brug af M basis vektore i A.
%\item altså når k > M så udgør de berørte søjler i A ikke længere en basis og x kan ikke bestemmes entydigt ? derfor er k<M et krav i standard algorithmer
%\item der sættes så et nyt frame work hvor vi kan bruge standard algoritmer i vores tilfælde med k>M (forhåbenligt) 
%\end{itemize}
%\item EEG is non-stationary(afsnit 1.3.1 phd) source dynamics ændres med tiden afhængig af opave, fejlkilder kan være non stationære. så ICA dur ikke mixture model ICA var bedre til non stationær med statigvæk limited til k = M. 
%\item the contribution er så at: support for X i MMV problem kan findes for k>M, altså altal non-zero entry k i x, med sufficient conditions ved sparse bayesian learning. herefter ved brug at uncorrelation af sources i et dictionary learning frame work så kan vi recover x når k>M. 
%\end{itemize}  
%  















