\section{Independent Component Analysis}\label{sec:ICA}
Independent Component Analysis (ICA) is a method which assume statistical independent between the components. By statistical independent the component value do not give information of another component value. Furthermore, ICA also assume that the data of interest is nongaussian as in most practical cases the data do not follow the gaussian distribution \cite[p. 3]{ICA}.


With this independence it is possible for ICA to separate the scalp measurements $\mathbf{Y}$ into the sources $\mathbf{X}$ and the mixing matrix $\mathbf{A}$.
\\
Through this section the mathematical concepts of Independent Component Analysis (ICA) will be explained and defined.
\\ \\
Lets set up an situations. We have some measurements that has been affect by some surrounding noise or "\textbf{sidel√∏bende}" measurements such as different conversations in a room. The measurements can be described by a vector $\mathbf{y}$ if we look at the one-dimensional case. $\mathbf{y}$ consist of the measurement from the original signal, a vector $\mathbf{x}$ and surrounding measurements, a matrix $\mathbf{A}$. This situation can be described as the linear model
\begin{align*}
\mathbf{y} = \mathbf{Ax} = \sum_{i=1}^n \mathbf{a}_i x_i
\end{align*}
We know the measurements $\mathbf{y}$ but if also knew the mixing parameter in $\mathbf{A}$ then by inverting the linear model we could solve the system and find the original signal. But this is not the case as the mixing matrix also is unknown.
\\
If we use the statistical properties of $\mathbf{x}$ then it would be possible to estimate both the mixing matrix and then the original signal. What ICA do is to assume statistical independence 
\\ \\
Lets define the ICA model which is a generative model meaning that the observed data is generated by a process of mixing components which are latent component. Let $n$ be the observed random variables such that $y_1, \dots, y_n$ are modelled as a linear combination of the random variables $x_1, \dots, x_n$:
\begin{align*}
y_i &= a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n, \quad i = 1, \dots, n \\
\mathbf{y} &= 
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \cdots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
\mathbf{x}
\end{align*}
where $\mathbf{y} = \{ y_i \}_{i \in [1,n]}$ and $\mathbf{x} = \{ x_t \}_{t \in [1,n]}$. Furthermore, $\mathbf{x}$ is statistically mutually independent.

\subsection{Estimation of Independent Components}
Notes:
Estimation with maximization of nongaussianity (see section 7.5 for nonguassianity)


\subsubsection{Kurtosis}
When estimation ICA with maximization of nongaussianity a measure of the nongaussianity is needed. Kurtosis is a quantitative measure used for nongaussianity of random variables. Kurtosis of a random variable $y$ is defined as
\begin{align*}
\text{kurt} (y) = \mathbb{E}[y^4] - 3 ( \mathbb{E}[y^2])^2,
\end{align*}
which is the fourth-order cumulant of the random variable $y$. By assuming that the random variable $y$ have been normalised such that its variance $\mathbb{E}[y^2] = 1$, the kurtosis is rewritten as
\begin{align*}
\text{kurt} (y) = \mathbb{E}[y^4] - 3.
\end{align*}
Because of this definition the kurtosis of nongaussian random variables the kurtosis will almost always be non-zero. For gaussian random variables the fourth moment equals $3(\mathbb{E}[y^2])^2$ thus the kurtosis will then be zero \cite[p. 171]{ICA}.
\\
By using the absolute value of the kurtosis gaussian random variables are still zero but the nongaussian random variables will be greater than zero. In this case the random variables are called supergaussian.
\\ \\
One complication with the use of kurtosis as measure is the used of measured samples as the kurtosis is sensitive to outliers in the measured data set \cite[p. 182]{ICA}. 
\subsubsection*{Gradient Algorithm}
For ICA the wish is to maximise the nongaussianity and therefore maximise the absolute value of kurtosis. One way to do this is to use a gradient algorithm.
\\ \\
With a gradient algorithm you start from an initial vector $\mathbf{w}$ and then compute the direction. The direction is computed from the absolute kurtosis of $y = \mathbf{w}^T \mathbf{z}$ giving some samples of the mixture vector $\mathbf{z}$. The direction which give us the highest kurtosis is the direction where $\mathbf{w}$ is moved.
\\
The gradient of the absolute value of kurtosis is computed as
\begin{align}\label{eq:kurt}
\frac{\partial \vert \text{kurt}(\mathbf{w}^T \mathbf{x})\vert}{\partial \mathbf{w}} &= 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{x})) (\mathbb{E}[\mathbf{z} (\mathbf{w}^T \mathbf{z})^3] - 3 \mathbf{w} \mathbb{E}[(\mathbf{w}^T\mathbf{z})^2] \nonumber \\
&= 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{x})) (\mathbb{E}[\mathbf{z} (\mathbf{w}^T \mathbf{z})^3] - 3 \mathbf{w} \Vert \mathbf{w} \Vert^2).
\end{align}
The absolute value of kurtosis is optimised onto the unit sphere, $\Vert \mathbf{w} \Vert^2 = 1$, the algorithm must project $\mathbb{w}$ onto the unit sphere in every step. This can easly be done by dividing $\mathbf{w}$ with its norm. 
\\
As it is the direction of $\mathbf{w}$ of interest the last part of 
\eqref{eq:kurt} can be omitted and instead the gradient of the absolute value of kurtosis is computed as
\begin{align*}
\frac{\partial \vert \text{kurt}(\mathbf{w}^T \mathbf{x})\vert}{\partial \mathbf{w}} = 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{x})) (\mathbb{E}[\mathbf{z} (\mathbf{w}^T \mathbf{z})^3]
\end{align*}
The expectation operator from the kurtosis definition can not be omitted and must therefore be estimated. This can be done by a time-average, denoted as $\gamma$:
\begin{align*}
\gamma = ((\mathbf{w}^T \mathbf{z})^4 - 3) - \gamma
\end{align*}
From the all above the following algorithm can be stated.
\begin{algorithm}[H]
\caption{Gradient Algorithm with Kurtosis}
\begin{itemize}
\item[1.] $\gamma = ((\mathbf{w}^T \mathbf{z})^4 - 3) - \gamma$
\item[2.] $\mathbf{w} = \gamma \mathbf{z})) \mathbb{E}[\mathbf{z} (\mathbf{w}^T \mathbf{z})^3]$
\item[3.] $\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}$
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
A measure of nongaussianity for the vector b which estimate 1 IC
Have some outliners so we introduce negentropy

\subsubsection{Negentropy}
Another measure of nongaussianity is the negentropy which based of on the differential entropy. The differential entropy $H$ of a random variable $\mathbf{y}$ with density $p_y (\boldsymbol{\theta})$ is defined as
\begin{align*}
H(\mathbf{y}) = - \int p_y (\boldsymbol{\theta}) \log (p_y (\boldsymbol{\theta}) \ d\boldsymbol{\theta}.
\end{align*}
The entropy describe the information of a random variable and for variables that becomes more random the entropy becomes larger, e.g. Gaussian random variable has a high entropy, in fact Gaussian random variable has the highest entropy among the random variables of the same variance. Furthermore, the entropy is small for clustered random variables \cite[p. 182]{ICA}.
\\ \\
To use the negentropy to define the nongaussianity within random variables, we normalised the differential entropy to obtain a entropy value equal to zero when the random variable is gaussian and non-negative otherwise. The negentropy $J$ is defined as 
\begin{align*}
J(\mathbf{y}) = H(\mathbf{y}_{\text{gaus}}) - H(\mathbf{y}),
\end{align*}
with $\mathbf{y}_{\text{gaus}}$ been a gaussian random variable of same covariance and correlation as $\mathbf{y}$ \cite[p. 182]{ICA}.
\\ \\
As the kurtosis is sensitive for outliers the negentropy is difficult to compute computationally as the negentropy require a estimate of the pdf. Instead it could be an idea to use an approximation of the negentropy.
 
\subsubsection{Approximation of Kurtosis and Negentropy}



\subsubsection*{Gradient Algorithm with Negentropy}
As described in section \ref{??} the gradient algorithm is used to maximising negentropy. The gradient of the approximated negentropy is given as
\begin{align*}
\mathbf{w} = \gamma \mathbb{E}[\mathbf{z} g(\mathbf{w}^T \mathbf{z})]
\end{align*}
with respect to $\mathbf{w}$ and where $\gamma = \mathbb{E}[G(\mathbf{w}^T \mathbf{z})] - \mathbb{E}[G(\nu)]$ with $\nu$ being the standardised gaussian random variable. To omitted the expectation $\gamma$ as we did with the sign of kurtosis, $\gamma$ is estimated as
\begin{align*}
\gamma = (G(\mathbf{w}^T \mathbf{z}) - \mathbb{E}[G(\nu)]) - \gamma.
\end{align*}



\begin{algorithm}[H]
\caption{Gradient Algorithm}
\begin{itemize}
\item[1.] Center the observed data to achieve zero mean
\item[2.] Whiten the centered data
\item[3.] Create the initial random vector $\mathbf{w}$ and the initial value for $\gamma$
\item[4.] Update
\begin{align*}
\mathbf{w} = \gamma \mathbf{z} g(\mathbf{w}^T \mathbf{z})
\end{align*}
\item[5.] Normalise $\mathbf{w}$
\begin{align*}
\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}
\end{align*}
\item[6.] Check sign of $\gamma$, if not a known prior, update
\begin{align*}
\gamma = (G(\mathbf{w}^T \mathbf{z}) - \mathbb{E}[G(\nu)]) - \gamma
\end{align*}
\item[7.] Repeat until convergence
\end{itemize}
\end{algorithm}

\paragraph{Notes:}
ICA can be used on Gaussian variables as little is done in addition to decorrelate for Gaussian variable

Whiting is useful to be done beore ICA

A drawback of ICA is the system must be $N \leq M$ meaning that there must more sensors than sources which is not the case in this project where we look at low density EEG system, $M \leq N$. Furthermore, ICA need that the sources are stationary which is not the nature of EEG that are very much nonstationary \cite[p. 7-8]{PHD}.
\\
Instead a mixture model of ICA model where we assume that the amount of activation $k$ in $N$ sources are equal to $M$ (sensor). We can used the short time frame of the sources to make them stationary
