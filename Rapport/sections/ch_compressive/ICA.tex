\chapter{Independent Component Analysis}\label{sec:ICA}
\textit{(it is not the intention to keep a whole chapter about ICA within the rapport, but rather a smaller section within the chapter of compressive sensing as a common solution method, and the rest as appendix.)}\\ \\ 
%Independent component analysis (ICA) is a common method within signal processing and especially for sparse signal recovery. The intention of ICA is to separate a multivariate signal into statistical independent and non-Gaussian signals and furthermore $\textbf{X}$ and identify the mixing matrix $\textbf{A}$ given the observed measurements $\textbf{Y}$. \\
%The main aspect of ICA is to assume statistical independent between the wanted components and nongaussiantity of the data. \cite[p. 3]{ICA}. 
%\\
%Through this section the mathematical concepts of ICA will be explained and defined in the noise-less case.
%\\ \\
%To understand what ICA is let us describe a situation where ICA could be used. Two people are having a conversation inside a room full of people which talk simultaneous with each other. The conversation between the two first mentioned people will the be affected by the surrounding conversations and noise. Such environment is often referred to as the cocktail party problem and is a difficult environment to be in as a hearing impaired.
%\\
%Let us describe the situation with some mathematical notations. The observed conversation which is affected by surrounding noise is denoted $\mathbf{y}$, the original individual conversations in the room is denoted by $\mathbf{x}$. All the original conversations are effected by each other and possibly noise, let us denoted this mixture by a matrix $\mathbf{A}$. We omit the time indexes and the time dependence to view the problem with random vectors. The problem can then be described as a linear model
%\begin{align}
%\mathbf{y} &= \mathbf{Ax} = \sum_{i=1}^n \mathbf{a}_i x_i,  \label{eq:ICA_1} \\
%y_i &= a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n, \quad i = 1, \dots, n. \label{eq:ICA_2}
%\end{align}
%The only known variables in this model are the observed conversation $\mathbf{y}$, the rest are unknown. 
%%The observation $y_i (t)$ would instead be a sample from the random vector if needed.

Independent component analysis (ICA) is a method that applies to  the general problem of decomposition of a measurement vector into a source vector and a mixing matrix. The intention of ICA is to separate a multivariate signal into statistical independent and non-Gaussian signals and furthermore identify the mixing matrix $\textbf{A}$, given only the observed measurements $\textbf{Y}$.
A well known application example of source separation is the cocktail party problem, where it is sought to listen to one specific person speaking in a room full of people having interfering conversations. 
Let $\textbf{y}\in \mathbb{R}^{M}$ be a single measurement from $M$ microphones containing a linear mixture of all the speak signal that are present in the room. When additional noise is not considered the problem can be described as the familiar linear model, 
\begin{align}
\textbf{y}=\textbf{Ax}\label{eq:ICA1}
\end{align}
where $\textbf{x}\in \mathbb{R}^{N}$ contain the $N$ underlying speak signals and $\textbf{A}$ is a mixing matrix where the coefficients depends (more og less?) on the distance from the source to the microphone. As such each $y_i$ is a weighted sum of all the present sources of speak.
\\ \\
By ICA both the mixing matrix $\textbf{A}$ and the sources signals $\textbf{x}$ are sought estimated from the observed measurements $\textbf{y}$. The main attribute of ICA is the assumption that the sources in $\textbf{x}$ are statistical independent and non-Gaussian distributed, hence the name independent components.\\     
\\
notes:\\
For simplicity assume $\textbf{A}$ is square i.e. $M=N$. When $\textbf{A}$ has been estimated the inverse is computed an $\textbf{x}$ can be found by solving \eqref{eq:ICA1} for $\textbf{x}$.\\
when we assume independese it is enough to solve system why?
\\
after whitening variance of all axis are equal and correlation of projection on every axix is 0. applying ICA onlu as to rotate whilie mini non-gausian. whiteining is a linear change of coordinates of the mixed data\url{http://arnauddelorme.com/ica_for_dummies/} "By rotating the axis and minimizing Gaussianity of the projection in the first scatter plot, ICA is able to recover the original sources which are statistically independent (this property comes from the central limit theorem which states that any linear mixture of 2 independent random variables is more Gaussian than the original variables)"\\
\\
\\
By independence, one means that change in one source signal do not affect the other sources signals. Theoretically that is the joint probability density function (pdf) of $\textbf{X}$ can be factorised into the product of the marginal pdfs of the components $x_i$
\begin{align*}
p(x_1, x_2, \dots, x_n) = p_1 (x_1) p_2(x_2) \cdots p_n(x_n),
\end{align*}.
\\
Furthermore it is assumed that the independent components $\mathbf{x}$ do not have Gaussian distributions as this will effect the recovery of $\mathbf{x}$ due to ICA using higher-order cumulant method. For Gaussian distribution the cumulant will be zero which is not wanted in the recovery process. Thus the distribution of the independent components are unknown. This will further be described in section \ref{sec:est_ica}.
\\
The possibility of separating a signal into independent and non-Gaussian components originates from the central limit theorem\cite[p. 34]{ICA}. The theorem state that the distribution any linear mixture of two or more independent random variables tents toward a Gaussian distribution, under certain conditions. Hence when a non-Gaussian distribution of the independent components is achieved through optimization it must be the original sources.\\

\subsection{Assumptions and Preprocessing}
For simplicity assume $\textbf{A}$ is square i.e. $M=N$ and invertible. As such when $\textbf{A}$ has been estimated the inverse is computed the components can simply be estimated as $\textbf{x}=\textbf{A}^{-1}\textbf{y}$\cite[p. 152-153]{ICA}.
\\ 
As both $\textbf{A}$ and $\textbf{x}$ are unknown the variances of the independent components can not be determined. However it is reasonable to assume that $\mathbf{x}$ has unit variance, as $\textbf{A}$ is adapted to this restriction. Any scalar multiplier within a source can be cancelled out by dividing the corresponding column in $\textbf{A}$ with the same scalar.\\  
This of course must also apply to the mixing matrix $\mathbf{A}$ which will be restricted in the recovery method, described in section \ref{sec:est_ica}.
For further simplification it is assumed without loss of generality that $\mathbb{E}[\mathbf{y}] = 0$ and $\mathbb{E}[\mathbf{x}] = 0$\cite[p. 154]{ICA}. If this assumption is not true, the measurements can be centred by subtracting the mean as preprocessing before doing ICA.\\
A preprocessing step central to ICA is to whiten the measurements $\mathbf{y}$. By the whitening process any correlation in the measurements are removed and unit variance is ensured. This ensures that the independent components $\mathbf{x}$ are uncorrelated and have unit variance(true?). Furthermore, this reduce the complexity of ICA and therefore simplifies the recovering process.
\\
Whitening is a linear transformation of the observed data. That is multiplying the measurement vector $\textbf{y}$ with a whitening matrix $\textbf{V}$,
\begin{align*}
\textbf{y}_{white} = \textbf{V}\textbf{y}
\end{align*} 
to obtain a new measurement vector $\textbf{y}_{white}$ that is white. To obtain a whitening matrix the eigenvalue decomposition (EVD) of the covariance matrix can be used,
\begin{align*}
\mathbb{E}[\mathbf{yy}^T] = \mathbf{EDE}^T
\end{align*}
here $\mathbf{D}$ is a diagonal matrix of eigenvalues and $\mathbf{E}$ is the associated eigenvectors. From $\mathbf{E}$ and $\mathbf{D}$ a whitening matrix is constructed \cite[p.159]{ICA}.
\begin{align*}
\mathbf{V} = \mathbf{ED}^{-1/2} \mathbf{E}^T.
\end{align*}
Where $\mathbf{D}^{-1/2}=diag{d_1^{-1/2},\hdots,d_n^{-1/2}}$ is a componentwise operation.\\  
By multiplying the measurement vector $\mathbf{y}$ with a whitening matrix $\mathbf{V}$ the data becomes white
\begin{align}
\mathbf{y}_{\text{white}} &= \mathbf{Vy} = \mathbf{VAx} = \mathbf{A}_{\text{white}} \mathbf{x} \nonumber
\end{align}
Furthermore the mixing matrix $\mathbf{A}_{\text{white}}$ becomes orthogonal 
\begin{align*}
 \mathbb{E}[\mathbf{y}_{\text{white}} \mathbf{y}_{\text{white}}^T] = \mathbf{A}_{\text{white}} \mathbb{E}[\mathbf{xx}^T] \mathbf{A}_{\text{white}}^T = \mathbf{A}_{\text{white}} \mathbf{A}_{\text{white}}^T = \mathbf{I}.
 \end{align*} 
Consequently ICA can restrict its search for the mixing matrix to the orthogonal matrix space -- That is instead of estimating $n^2$ parameters ICA now only has to estimate an orthogonal matrix which has $n(n-1)/2$ parameters/degrees of freedom \cite[p. 159]{ICA}.
\\
%%%%%
Furthermore, to support the assumption of nongaussiantity for an orthogonal mixing matrix $\mathbf{A}_{\text{white}}$ it is not possible to distinct the pdfs of the $\mathbf{y}_{\text{white}}$ and $\mathbf{x}$ as $\mathbf{A}_{\text{white}}$ is no longer included in the pdf of $\mathbf{y}_{\text{white}}$ and therefore are the two pdfs equal \cite[p. 161-163]{ICA}\todo{can we elaborate this last part}.

\subsection{Recovery of the Independent Components}\label{sec:est_ica}
A way to recover the independent components could be to take advantage of the assumption of nongaussiantity. By finding the estimate which maximise the nongaussiantity the real independent component can be found based on the fact that it must have a nongaussian distribution as mention in section \ref{sec:ICA}. But before the estimation a measure of nongaussiantity must be introduce -- this could be the kurtosis.

\subsubsection{Kurtosis}
Kurtosis is a quantitative measure used for nongaussianity of random variables. (Excess) Kurtosis of a random variable $y$ is defined as\todo{Tjek lige op denne definition}
\begin{align*}
\text{kurt} (y) &= \mathbb{E} \left[ \left(\frac{y - \mu}{\sigma}\right)^4 \right] - 3 \\
&= \mathbb{E}[y^4] - 3 ( \mathbb{E}[y^2])^2,
\end{align*}
which is the fourth-order cumulant of the random variable $y$. By assuming that the random variable $y$ has variance $\mathbb{E}[y^2] = 1$, the kurtosis is rewritten as
\begin{align*}
\text{kurt} (y) = \mathbb{E}[y^4] - 3.
\end{align*}
The kurtosis of nongaussian random variables will then almost always be different from zero. For gaussian random variables the fourth moment equals $3(\mathbb{E}[y^2])^2$ thus the kurtosis will then be zero \cite[p. 171]{ICA}.
\\
By using the absolute value of the kurtosis gaussian random variables are still zero but the nongaussian random variables will be greater than zero. In this case the random variables are called supergaussian.
\\ \\
One complication with kurtosis as a measure is that kurtosis is sensitive to outliers \cite[p. 182]{ICA}.
 
\paragraph{The Gradient Algorithm}
To recover the independent components $\mathbf{x}$ the nongaussianity is wish maximised. One way to do this is to use a gradient algorithm to maximise the kurtosis of $\mathbf{y}$.
\\ \\
The idea behind a gradient algorithm is to move in certain directions computed from the gradient of an objective function -- in this case it is the kurtosis -- until convergence is achieved.
\\
Let $\mathbf{w}$ be an initial vector used to compute the direction and let $y = \mathbf{w}^T \mathbf{y}_{\text{white}}$ given some samples of the observed and preprocessed vector. The direction $\mathbf{w}$ which provide the highest kurtosis is the new direction for the algorithm. This continues until convergence is reach defined by some tolerance.
\\
The gradient of $\vert \text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}}) \vert$ is computed as\todo{Regn lige efter}
\begin{align}\label{eq:kurt}
\frac{\partial \vert \text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}})\vert}{\partial \mathbf{w}} &= 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}})) (\mathbb{E}[\mathbf{y}_{\text{white}} (\mathbf{w}^T \mathbf{y}_{\text{white}})^3] - 3 \mathbf{w} \mathbb{E}[(\mathbf{w}^T \mathbf{y}_{\text{white}})^2] \nonumber \\
&= 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}})) (\mathbb{E}[\mathbf{y}_{\text{white}} (\mathbf{w}^T \mathbf{y}_{\text{white}})^3] - 3 \mathbf{w} \Vert \mathbf{w} \Vert^2).
\end{align}
The absolute value of kurtosis is optimised onto the unit sphere, $\Vert \mathbf{w} \Vert^2 = 1$, the algorithm must project $\mathbb{w}$ onto the unit sphere in every step. This can easily be done by dividing $\mathbf{w}$ with its norm. 
\\
Furthermore, the last part of \eqref{eq:kurt} can be omitted as it do not effect the direction. The expectation operator is omitted to achieve an adaptive algorithm:
\begin{align*}
\frac{\partial \vert \text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}})\vert}{\partial \mathbf{w}} = 4 \text{sign}(\text{kurt}(\mathbf{w}^T \mathbf{y}_{\text{white}})) \mathbf{y}_{\text{white}} (\mathbf{w}^T \mathbf{y}_{\text{white}})^3
\end{align*}
The expectation operator from the definition of kurtosis can not be omitted and must therefore be estimated. This can be done by a time-average estimate, denoted as $\gamma$:
\begin{align*}
\gamma = ((\mathbf{w}^T \mathbf{y}_{\text{white}})^4 - 3) - \gamma
\end{align*}
This results in the following gradient algorithm.
\begin{algorithm}[H]
\caption{Gradient Algorithm with Kurtosis}
\begin{itemize}
\item[1.] Center the observed data to achieve zero mean
\item[2.] Whiten the centered data
\item[3.] Create the initial random vector $\mathbf{w}$ and the initial value for $\gamma$
\item[4.] Compute $\mathbf{w} = \gamma \mathbf{y}_{\text{white}} (\mathbf{w}^T \mathbf{y}_{\text{white}})^3$
\item[5.] Normalise $\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}$
\item[6.] Update $\mathbf{w}$
\item[7.] Update $\gamma = ((\mathbf{w}^T \mathbf{z})^4 - 3) - \gamma$
\item[8.] Repeat until convergence
\item[9.] Independent components are found as $\mathbf{x} = \mathbf{w} \mathbf{y}_{\text{white}}$
\end{itemize}
\end{algorithm}

\paragraph{Fixed-Point Algorithm - FastICA}
A fixed-point algorithm to maximise the nongaussianity is more efficient than the gradient algorithm as the gradient algorithm converge slow depending on the choice of $\gamma$. The fixed-point algorithm is an alternative that could be used.
By using the gradient given in \eqref{eq:kurt} and then set the equation equal with $\mathbf{w}$:
\begin{align*}
\mathbf{w} \propto ( \mathbb{E}[\mathbf{y}_{\text{white}}(\mathbf{w}^T \mathbf{y}_{\text{white}})^3] - 3 \Vert \mathbf{w} \Vert^2 \mathbf{w})
\end{align*}
By this new equation, the algorithm find $\mathbf{w}$ by simply calculating the right-hand side:
\begin{align*}
\mathbf{w} = \mathbb{E}[\mathbf{y}_{\text{white}}(\mathbf{w}^T \mathbf{y}_{\text{white}})^3] - 3 \mathbf{w}
\end{align*}
As with the gradient algorithm the fixed-point algorithm do also divide the found $\mathbf{w}$ by its norm. Therefore is $\Vert \mathbf{w} \Vert$ omitted from the equation.
\\
Instead of $\gamma$ the fixed-point algorithm compute $\mathbf{w}$ directly from previous $\mathbf{w}$.
\\ \\
The fixed-point algorithm have been summed in the following algorithm.
\begin{algorithm}[H]
\caption{Fixed-Point Algorithm with Kurtosis}
\begin{itemize}
\item[1.] Center the observed data to achieve zero mean
\item[2.] Whiten the centered data
\item[3.] Create the initial random vector $\mathbf{w}$ 
\item[4.] Compute $\mathbf{w} = \mathbb{E}[\mathbf{y}_{\text{white}}(\mathbf{w}^T \mathbf{y}_{\text{white}})^3] - 3 \mathbf{w}$
\item[5.] Normalise $\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}$
\item[6.] Update $\mathbf{w}$
\item[7.] Repeat until convergence
\item[8.] Independent components are found as $\mathbf{x} = \mathbf{w} \mathbf{y}_{\text{white}}$
\end{itemize}
\end{algorithm}
The fixed-point algorithm is also called for FastICA as the algorithm has shown to converge fast and reliably, then the current and previous $\mathbf{w}$ laid in the same direction \cite[p. 179]{ICA}. 


\subsubsection{Negentropy}
Another measure of nongaussianity is the negentropy which is based  on the differential entropy. The differential entropy $H$ of a random variable/vector $\mathbf{y}$ with density $p_y (\boldsymbol{\theta})$ is defined as
\begin{align*}
H(\mathbf{y}) = - \int p_y (\boldsymbol{\theta}) \log (p_y (\boldsymbol{\theta})) \ d\boldsymbol{\theta}.
\end{align*}
The entropy describes the information of a random variable. For variables becoming more random the entropy becomes larger, e.g. gaussian random variables have a high entropy, in fact gaussian random variables have the highest entropy among the random variables of the same variance \cite[p. 182]{ICA}.
\\ \\
To use the negentropy to define the nongaussianity within random variables, the differential entropy is normalised to obtain a entropy value equal to zero when the random variable is gaussian and non-negative otherwise. The negentropy $J$ is defined as 
\begin{align*}
J(\mathbf{y}) = H(\mathbf{y}_{\text{gaus}}) - H(\mathbf{y}),
\end{align*}
with $\mathbf{y}_{\text{gaus}}$ being a gaussian random variable of the same covariance and correlation as $\mathbf{y}$ \cite[p. 182]{ICA}.
\\ \\
As the kurtosis is sensitive for outliers the negentropy is instead difficult to compute computationally as the negentropy require a estimate of the pdf. Instead it could be an idea to use an approximation of the negentropy.
 
\subsubsection{Approximation of Negentropy}
The way to approximate the negentropy is to look at the high-order cumulants using polynomial density expansions such that the approximation could be given as
\begin{align}\label{eq:approx_1}
J(y) \approx \frac{1}{12} \mathbb{E}[y^3]^2 + \frac{1}{48} \text{kurt}(y)^2.
\end{align}
The random variable $y$ has zero mean and unit variance and the kurtosis is introduced in the approximation. The approximation suffers from nonrobustness with the kurtosis and therefore a more generalised approximation is presented to avoid the nonrobustness.
\\ \\
For the generalised approximation the use of expectations of nonquadratic functions is introduced. The polynomial functions $y^3$ and $y^4$ from \eqref{eq:approx_1} are replaced by $G^i$ with $i$ being an index and $G$ being some function. The approximation in \eqref{eq:approx_1} then becomes
\begin{align*}
J(y) \approx (\mathbb{E}[G(y)] - \mathbb{E}[G(\nu)])^2.
\end{align*}
The choice of $G$ can lead to a better approximation than \eqref{eq:approx_1} and by choosing one which do not grow to fast more robust estimators can be obtained. The choice of $G$ could be the two following functions
\begin{align*}
G_1 (y) &= \frac{1}{a_1} \log (\cosh(a_1 y)), \quad 1 \leq a_1 \leq 2 \\
G_2 (y) &= -\exp \left( \frac{-y^2}{2} \right)
\end{align*}


\paragraph{Gradient Algorithm with Negentropy}
As described in section \ref{sec:est_ica} the gradient algorithm is used to maximising negentropy. The gradient of the approximated negentropy is given as
\begin{align*}
\mathbf{w} = \gamma \mathbf{y}_{\text{white}} g(\mathbf{w}^T \mathbf{y}_{\text{white}})
\end{align*}
with respect to $\mathbf{w}$ and where $\gamma = \mathbb{E}[G(\mathbf{w}^T \mathbf{y}_{\text{white}})] - \mathbb{E}[G(\nu)]$ with $\nu$ being the standardised gaussian random variable. $g$ is the derivative of the nonquadratic function $G$. To omitted the expectation $\gamma$ as we did with the sign of kurtosis, $\gamma$ is estimated as
\begin{align*}
\gamma = (G(\mathbf{w}^T \mathbf{y}_{\text{white}}) - \mathbb{E}[G(\nu)]) - \gamma.
\end{align*}
For the choice of $g$ the derivative of the functions presented in \eqref{eq:G} could be use to achieve a robust result. Alternative a derivative which correspond to the fourth-power as seen in the kurtosis could be used. The functions $g$ could be
\begin{align}\label{eq:G}
g_1 (y) &= \tanh (a_1 y), \quad 1 \leq a_1 \leq 2 \\
g_2 (y) &= y \exp\left( \frac{-y^2}{2} \right) \nonumber \\
g_3 (y) &= y^3 \nonumber
\end{align}

\begin{algorithm}[H]
\caption{Gradient Algorithm}
\begin{itemize}
\item[1.] Center the observed data to achieve zero mean
\item[2.] Whiten the centered data
\item[3.] Create the initial random vector $\mathbf{w}$ and the initial value for $\gamma$
\item[4.] Update
\begin{align*}
\mathbf{w} = \gamma \mathbf{y}_{\text{white}} g(\mathbf{w}^T \mathbf{y}_{\text{white}})
\end{align*}
\item[5.] Normalise $\mathbf{w}$
\begin{align*}
\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}
\end{align*}
\item[6.] Check sign of $\gamma$, if not a known prior, update
\begin{align*}
\gamma = (G(\mathbf{w}^T \mathbf{y}_{\text{white}}) - \mathbb{E}[G(\nu)]) - \gamma
\end{align*}
\item[7.] Repeat until convergence
\item[8.] Independent components are found as $\mathbf{x} = \mathbf{w} \mathbf{y}_{\text{white}}$
\end{itemize}
\end{algorithm}

\paragraph{Fixed-Point Algorithm with Negentropy}
As described in the section with kurtosis, the fixed-point algorithm removed the learning parameter and compute $\mathbf{w}$ directly:
\begin{align*}
\mathbf{w} = \mathbb{E}[\mathbf{z} g(\mathbf{w}^T \mathbf{z})]
\end{align*}
\todo{Write the expression from this equation to the on in the algorithm}.
\begin{algorithm}[H]
\caption{Fixed-Point Algorithm with Negentropy (FastICA)}
\begin{itemize}
\item[1.] Center the observed data to achieve zero mean
\item[2.] Whiten the centered data
\item[3.] Create the initial random vector $\mathbf{w}$
\item[4.] Update
\begin{align*}
\mathbf{w} = \mathbb{E}[ \mathbf{y}_{\text{white}} g(\mathbf{w}^T \mathbf{y}_{\text{white}})] - \mathbb{E}[g'(\mathbf{w}^T \mathbf{y}_{\text{white}})] \mathbf{w}
\end{align*}
\item[5.] Normalise $\mathbf{w}$
\begin{align*}
\mathbf{w} = \frac{\mathbf{w}}{\Vert \mathbf{w} \Vert}
\end{align*}
\item[6.] Repeat from 4. until convergence
\item[7.] Independent components are found as $\mathbf{x} = \mathbf{w} \mathbf{y}_{\text{white}}$
\end{itemize}
\end{algorithm}


%\paragraph{Notes:}
%A drawback of ICA is the system must be $N \leq M$ meaning that there must more sensors than sources which is not the case in this project where we look at low density EEG system, $M \leq N$. Furthermore, ICA need that the sources are stationary which is not the nature of EEG that are very much nonstationary \cite[p. 7-8]{PHD}.
%\\
%Instead a mixture model of ICA model where we assume that the amount of activation $k$ in $N$ sources are equal to $M$ (sensor). We can used the short time frame of the sources to make them stationary
