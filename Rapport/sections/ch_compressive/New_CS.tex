\section{System of Linear Equations}\label{sec:SMV}
Let $\mathbf{y} \in \mathbb{R}^M$ be some vector. By basic linear algebra $\mathbf{y}$ can always be described as a linear combination of a coefficient matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and some scalar vector $\mathbf{x} \in \mathbb{R}^N$ such that
\begin{align}\label{eq:SMV_model}
\mathbf{y} = \mathbf{Ax}.
\end{align}
Let $\mathbf{y}$ and $\mathbf{A}$ be known, then  
\ref{eq:SMV_model} makes a system of $M$ linear equations with $N$ unknowns, referred to as a linear system. 

To solve the linear system \ref{eq:SMV_model} with respect to $\textbf{x}$ one must look at the three different cases that can occur, depending on the relation between the number of linear equations $M$ and the number of unknowns $N$.
For $M = N$, the system has one unique solution, provided that a solution exist.  
If the square coefficient matrix $\mathbf{A}$ has full rank the solution can be found by inverting $\mathbf{A}$.
\begin{align*}
\mathbf{x} = \mathbf{A}^{-1} \mathbf{y}.
\end{align*}
For $M > N$ the system is over-determined, having more equations than unknown. In general there is no solution to an over-determined system.   
For $M < N$ the system is under-determined, having fewer equations than unknowns. There exists infinitely many solutions to an under-determined system, provided that one solution exist \cite[p. ix]{CS}.  

Consider now $\mathbf{y} \in \mathbb{R}^M$ as the observed measurements provided by $M$ EEG sensors at time $t$. 
The linear system \ref{eq:SMV_model} is then considered as a single measurement vector (SMV) model.  
Modelling the EEG measurements by the SMV model embody the following interpretations, based on chapter \ref{ch:motivation}.
Remember from chapter \ref{ch:motivation} that EEG measurements basically are mixture of original brain signals affected by volume conduction and noise.
The vector $\mathbf{x}$ is seen as the original brain signal sources, with each entry representing the signal of one source. 
Thus, $\mathbf{x} \in \mathbb{R}^N$ is referred to as the source vector. 
$N$ is considered the maximum number of sources, however zero-entries may occur. 
Let $k$ denote the number of non-zero entries in $\mathbf{x}$, referred to as the active sources at time $t$.   
The coefficient matrix $\mathbf{A}$, referred to as the mixing matrix, models the volume conduction by mapping the source vector from $\mathbb{R}^N$ to $\mathbb{R}^M$.
%where $M$ is the number of sensors hence the dimension of the measurement vector $\mathbf{y}$.             

\section{Multiple Measurement Vector Model of EEG}\label{sec:MMV}
In practice EEG measurements are sampled over time by a certain sample frequency. 
Thus multiple EEG measurement vectors are achieved.
Let $L$ represent the total number of samples. 
Now the SMV model is expanded to include $L$ measurement vectors and noise:
\begin{align}\label{eq:MMV_model}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align}
$\mathbf{Y} \in \mathbb{R}^{M \times L}$ is the observed measurement matrix, $\mathbf{X} \in \mathbb{R}^{N \times L}$ is the source matrix, and $\mathbf{A} \in \mathbb{R}^{M \times N}$ is the mixing matrix. 
Furthermore, $\mathbf{E} \in \mathbb{R}^{M \times L}$ is an additional noise matrix, to be expected from psychical measurements.  
The model is now referred to as the multiple measurement vector (MMV) model.
As for \eqref{eq:SMV_model} the solution set of the linear system \eqref{eq:MMV_model} depends on the relation between $N$ and $M$ \cite[p. 42]{CS}. 

In chapter \ref{ch:motivation} it is specified that the case of more sources than sensors, $N>M$, is the case of interest in this thesis.  
%flytte sidste del?

\subsection{Segmentation}\label{seg_segmentation}
In chapter \ref{ch:motivation} it is argued that EEG measurements are only stationary within small segments. 
Hence, the following segmentation is considered.

Let $f$ be the sample frequency of the observed EEG measurement matrix $\mathbf{Y}$ and let $t$ the length of a time interval in seconds determining the duration of one segment. 
Here $s$ is the segment index. 
As such the observed EEG measurement matrix $\mathbf{Y}$ can be divided into stationary segments $\mathbf{Y}_s \in \mathbb{R}^{M \times L_{s}}$, possibly overlapping, where $L_s = t \cdot f$ is the number of samples within one segment. 
For each segment the MMV model \eqref{eq:MMV_model} holds and is rewritten into \todo{A skal være As (hele vejen) med en passene beskrivelse}
\begin{align}\label{eq:MMV_seg}
\mathbf{Y}_s = \mathbf{AX}_s + \textbf{E}_s, \quad \forall s.
\end{align}
Due to a segment being stationary, it is assumed that each source remains either active or non-active throughout the segment.
Thus, $\mathbf{X}_s$, consists of $k$ non-zero rows -- the active sources.

In order to characterize the source matrix with respect to the number of non-zero rows, the term row sparseness is considered. 
Let the support $\text{supp}(\mathbf{X}_s)$ denote the index set of non-zero rows of $\mathbf{X}_s$.
To count the non-zero rows of a matrix the $\ell_0$-norm is defined 
\begin{align*}
\Vert \mathbf{X}_{s} \Vert_0 := \text{card}(\text{supp}(\mathbf{X}_{s})),
\end{align*}
where the function $\text{card}(\cdot)$ gives the cardinality of the input set. The segmented source matrix $\textbf{X}_s$ is said to be $k$-sparse if it contains at most $k$ non-zeros rows:
\begin{align*}
\Vert \mathbf{X}_s \Vert_0 \leq k.
\end{align*}
A model for the EEG measurements is now established.
From the model the aim is to recover the source matrix $\mathbf{X}_s$ for all segments, which gives us the separated original brain signals as intended by the problem statement. 
In the following section the solution method is presented and discussed -- outlining the remaining chapters of the thesis. 

\section{Solution Method}\label{sec:sol_met}
\todo[inline]{Denne section er ikke blevet opdateret i forhold til den nye problemformulering. Altså vi mangler at inddrage 'reproducerbarhed'.}
It is now justified that the EEG measurements can be modelled by the multiple measurement vector model defined by the system of linear equations \eqref{eq:MMV_seg}, including an additional noise.
By the problem statement cf. chapter \ref{ch:problemstatement}, the aim is to recover an approximation of the source vector $\mathbf{X}$, in the case where the number of sensors is less than the number of sources, $M < N$. 
That is recovering $\mathbf{X}$ from an under-determined linear system. 
Therefore, the solution must be found in the infinite solution space, provided that one solution exists, thus simple linear algebra can not be used. 
However, by considering numerical methods such as mathematical optimization it is possible to restrict the solution by some constraint and then find the unique optimal solution with respect to a defined cost relative to the solution which respects the constraint.
The theory of compressive sensing provides a framework for solving an under-determined system when $\mathbf{X}$ is known to have zero rows, thus row spareness. 
Specifically a unique solution $\mathbf{X}$ can be found when $\mathbf{X}$ is $M$-sparse, cf. theorem \ref{th:CS_A} in appendix \ref{app_sec:CS}. 
When the mixing matrix $\mathbf{A}$ is unknown, as it is in the current case, the concept of dictionary learning can be used to determine $\mathbf{A}$, again under the assumption that $\mathbf{X}$ is $M$-sparse.  

As discussed in chapter \ref{ch:motivation}, the aim of this thesis is to overcome the limitation of fewer sources than measurements, which is a limitation of compressive sensing. 
A method to overcome this limitation, with respect to learning $\mathbf{A}$, is the covariance domain dictionary learning (Cov-DL) method \cite{Balkan2015}, introduced in chapter \ref{ch:motivation}. 
The method manages to leverage the increased dimensionality of the covariance domain in order to allow the theory of compressive sensing to apply to an under-determined system. 
However, this method only apply to the process of learning $\mathbf{A}$, hence a different approach is necessary to recover $\mathbf{X}$.

For recovering $\mathbf{X}$, given both $\mathbf{Y}$ and $\mathbf{A}$, the method multiple sparse Bayesian learning (M-SBL), introduced in chapter \ref{ch:motivation}, is considered \todo{Rasmus synes alt nedstående er en gentagelse af kapitel 1 og forstår ikke hvorfor vi nævner det igen. Skal vi revidere den her sektion med solution methods? - L}.
O. Balkan \cite{Balkan2014} did also, in 2014, proposed a method which could identify the sources, in the time domain, by creating a likelihood which ensures the wanted sparsity of the source matrix $\mathbf{X}$ and controlled by some variance. This method is called multiple sparse Bayesian learning (M-SBL) and takes advantage of a Bayesian approach. 
In \cite{Balkan2014} a variance dependent log-likelihood which has been induced by a empirical prior that ensure sparsity of the likelihood has been constructed to be minimised with respect to the variance. 
From the log-likelihood an estimate for the source matrix $\mathbf{X}$ is obtained with respect to the support set $S$ created from the optimised variance \todo{De sidste to linje synes Rasmus er uklar og knudret -L}.
%which has been influenced by the variance used in the minimization 


%With these two methods the recovering of $\mathbf{A}$ and $\mathbf{X}$ can be done from the EEG measurements $\mathbf{Y}$ uniquely.



%fjernet indhold:
%The number of $k$ active source in the source vector $\mathbf{x} \in \mathbb{R}^N$ can be view as non-zeros -- the non-activations are described by zeros. 
%By using this joint information it is possible to recover $\mathbf{X}_s$ from fewer measurements \cite[p. 43]{CS}.

