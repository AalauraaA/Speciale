\chapter{Supplementary Theory for Chapter \ref{ch:Cov-DL}}\label{app:Cov-DL}
Throughout this chapter supplemental theory for understanding the covariance-domain dictionary learning (Cov-DL) is described. First an introduction to compressive sensing which is the framework behind Cov-DL. Then an dictionary learning algorithm used for finding the dictionary matrix $\mathbf{D}$ in section \ref{sec:cov1} will be described. And at last principal component analysis (PCA) is introduced as the method behind finding $\mathbf{D}$ in section \ref{sec:over_det}.

\section{Introduction to Compressive Sensing}\label{app_sec:CS}
Compressive sensing is the theory of efficient recovery of a signal from a minimal number of observed measurements. 
It is build upon empirical observations assuring that many signals can be approximated by remarkably sparser signals. 
Assume linear acquisition of the observed measurements. 
Then the relation between the measurements and the signal to be recovered can be modeled by the multiple measurement vector (MMV) model \eqref{eq:MMV_model} \cite{FR}. 

Through this section the introduction of the theory behind compressive sensing will be presented for one measurement vector of \eqref{eq:MMV_model}, $\mathbf{y}$, such that the theory is based on the linear system \eqref{eq:SMV_model}. This will be done for simplicity, but the theory will still apply for the extended linear system \eqref{eq:MMV_model}.

In compressive sensing terminology, $\mathbf{x} \in \mathbb{R}^N$ is the signal of interest sought recovered from the EEG measurement $\mathbf{y} \in \mathbb{R}^M$ by solving the linear system \eqref{eq:SMV_model}. 
In the typical compressive sensing case, the system is under-determined, $M < N$, and there will therefore exist infinitely many solutions, provided that one solution exist.
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal, hence the term sparse signal recovery \cite{FR}. 
The sparsity constraints are the ones presented in \ref{sec:SMV} where the $\ell_0$ is introduced to count the non-zeros of the signal of interest, the source vector $\mathbf{x}$. 
The number of non-zeros, active sources, $k$ describe how sparse the source vector is.
 
To find a $k$-sparse solution to the linear system \eqref{eq:SMV_model} it can be viewed as the following optimization problem. 
\begin{align*}\label{eq:SMV_p0}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_0 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align*}
Unfortunately, this optimization problem is non-convex due to the definition of the $\ell_0$-norm and is therefore difficult to solve -- it is a NP-hard problem. 
Instead, by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimization problem can be approximated and hence becomes computationally feasible \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align} 
With this optimization problem the best $k$-sparse solution $\mathbf{x}^\ast$ can be found. 
The optimization problem is referred to as $\ell_1$ optimization problem or Basis Pursuit. 
The following theorem justifies that the $\ell_1$ optimization problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}\label{th:CS_A}
A mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined with columns $\mathbf{A} = [\mathbf{a}_1, \dots, \mathbf{a}_N]$. 
By assuming uniqueness of a solution $\mathbf{x}^{\ast}$ to
\begin{align*}
\min_{\mathbf{x} \in \mathbb{R}^N} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y},
\end{align*}
the system $\lbrace \mathbf{a}_j, j \in \text{supp}( \mathbf{x}^\ast) \rbrace$ is linearly independent, and in particular
\begin{align*}
\Vert \mathbf{x}^\ast \Vert_0 = \text{card}(\text{supp} (\mathbf{x}^\ast)) \leq M.
\end{align*}
\end{theorem}
\begin{proof}
Assume that the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ of $l$ columns from matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is linearly dependent with the support $S = \text{supp}(\mathbf{x}^\ast)$.
Thus a non-zero vector $\mathbf{v} \in \mathbb{R}^N$ supported on $S$ exists such that $\mathbf{Av} = \textbf{0}$ -- the system is linear dependent. The unique solution $\mathbf{x}^\ast$ can then be written as, for any $t \neq 0$,
\begin{align}\label{eq:non_zero_t}
\Vert \mathbf{x}^\ast \Vert_1 < \Vert \mathbf{x}^\ast + t \mathbf{v} \Vert_1 = \sum_{l \in S} \vert x_l^\ast + t v_l \vert = \sum_{l \in S} \text{sgn}(x_l^\ast + t v_l )(x_l^\ast + t v_l ).
\end{align}
For a small $|t|$
\begin{align*}
|t| < \min_{l \in S} \frac{\vert x_l^\ast \vert}{\Vert \mathbf{v} \Vert_{\infty}},
\end{align*}
then the sign function becomes
\begin{align*}
\text{sgn}(x_l^\ast + t v_l) = \text{sgn}(x_l^\ast), \quad \forall l \in S.
\end{align*}
By including this result in \eqref{eq:non_zero_t} and remembering $t \neq 0$:
\begin{align*}
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{l \in S} \text{sgn}(x_l^{\ast})(x_l^{\ast} + t v_l ) = \sum_{l \in S} \text{sgn}(x_l^{\ast})x_l^{\ast} + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l.
\end{align*}
From this it can be seen that it is always possible to choose $t \neq 0$ small enough such that 
\begin{align*}
t \sum_{l \in S} \text{sgn}(x_l^\ast)v_l \leq 0,
\end{align*}
which contradicts that $\mathbf{v}$ make the columns of $\mathbf{A}$ linear dependent. 
Therefore, the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ must be linearly independent.
\end{proof}
From the theorem it must be concluded that the choice of the mixing matrix $\mathbf{A}$ has a significant impact on whenever a unique solution $\mathbf{x}^\ast$ exist for the $\ell_1$ optimization problem \eqref{eq:SMV_p1}. Therefore, when recovering $\mathbf{A}$, some considerations regarding the recovering process of $\mathbf{A}$ must be taken into account. A method for the recovering of $\mathbf{A}$ could be to use a dictionary. This will be explained in the following section \ref{sec:dictionarylearning}.

An alternative solution method to the $\ell_1$ optimization includes greedy algorithms like the Orthogonal Matching Pursuit (OMP) \cite[P. 65]{FR}. 
The OMP algorithm is an iteration process where an index set $S$ is updated -- at each iteration -- by adding indices corresponding to the columns of $\mathbf{A}$ which describe the residual best possible, hence greedy.
The vector $\mathbf{x}$ is then updated by a vector supported on $S$ which minimize the residual. That is the orthogonal projection of $\mathbf{y}$ onto the span$\lbrace \mathbf{a}_l \ \vert \ l \in S \rbrace$.

\section{K-SVD Algorithm}\label{app_sec:K-SVD_alg}
The dictionary learning algorithm K-SVD provides an updating rule which is applied to each column of $\mathbf{A}_0 = \left[ \mathbf{a}_1, \dots, \mathbf{a}_N \right] $ where $\mathbf{A}_0$ being a random initial dictionary matrix. Updating first the column $\mathbf{a}_{\cdot j}$ for $j=1, \dots, N$ and then the corresponding row of $\mathbf{X}$, $\mathbf{x}_{i\cdot}$ for $j=i$.
Let $\mathbf{a}_{\cdot j_{0}}$ be the column to be updated and let the remaining columns be fixed. By rewriting the objective function in \eqref{eq:SVD1} using matrix notation it is possible to isolate the contribution from $\mathbf{a}_{\cdot j_{0}}$:
\begin{align}\label{eq:SVD2} 
\Vert \mathbf{Y} - \mathbf{AX} \Vert_{F}^{2} 
&= \left\| \mathbf{Y} - \sum_{\substack{j=1 \\ j = i}}^{N} \mathbf{a}_{\cdot j} \mathbf{x}_{i \cdot} \right\|_{F}^{2} \nonumber \\
&= \left\| \left( \mathbf{Y}- \sum_{\substack{j \neq j_0 \\ j = i}}^{N} \mathbf{a}_{\cdot j} \mathbf{x}_{i \cdot} \right) - \mathbf{a}_{\cdot j_{0}} \mathbf{x}_{i_0 \cdot} \right\| _{F}^{2},
\end{align}
where $i = j$, $i_0 = j_0$ and $F$ is the Frobenius norm for matrices
\begin{align*}
\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{ij} \vert^2}.
\end{align*} 
In \eqref{eq:SVD2} the term in the parenthesis is denoted by $\mathbf{E}_{j_0}$, an error matrix. 
Hence by minimizing \eqref{eq:SVD2} with respect to $\mathbf{a}_{\cdot j_0}$ and $\mathbf{x}_{i_0 \cdot}$ an optimal contribution from $j_0$ can be obtained:
\begin{align}\label{eq:SVD3}
\min_{\mathbf{a}_{\cdot j_{0}}, \mathbf{x}_{i_0 \cdot}}\left\|\mathbf{E}_{j_{0}} - \mathbf{a}_{\cdot j_{0}} \mathbf{x}_{i_0 \cdot} \right\|_{F}^{2}.
\end{align} 
The optimal solution to \eqref{eq:SVD3} is known to be the rank-1 approximation of $\mathbf{E}_{j_{0}}$ \cite[p. 232]{Elad_book}. That is a partial single value decomposition (SVD) makes the best low-rank approximation of $\mathbf{E}_{j_0}$. 
The SVD of $\mathbf{E}_{j_0}$ is given as
\begin{align*}
\mathbf{E}_{j_0} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T \in \mathbb{R}^{M \times N},
\end{align*}
with $\mathbf{U} \in \mathbb{R}^{M \times M}$ and $\mathbf{V} \in \mathbb{R}^{N \times N}$ being unitary matrices\footnote{Unitary matrix: $\mathbf{U}^T \mathbf{U} = \mathbf{UU}^T = \mathbf{I}$} and $\boldsymbol{\Sigma} = \text{diag}\left(\sigma_1, \dots, \sigma_M \right) \in \mathbb{R}^{M \times N}$ a diagonal matrix. 
The $\sigma_j$ are the non-negative singular values of $\mathbf{E}_{j_0}$. 
The best $k$-rank approximation to $\mathbf{E}_{j_0}$, with $k < \text{rank}(\mathbf{E}_{j_0})$ is then given by \cite[p. 232]{Elad_book}: 
\begin{align*}
\mathbf{E}_{j_{0}}^{(k)} = \sum_{j=1}^{k} \sigma_j \mathbf{u}_{\cdot j} \mathbf{v}_{\cdot j}^T.
\end{align*} 
Since the outer product always has rank-1 then letting $\mathbf{a}_{\cdot j_0} = \mathbf{u}_{\cdot 1}$ and $\mathbf{x}_{i_0 \cdot} = \sigma_{j} \mathbf{v}_{\cdot 1}^T$ solves the optimization problem \eqref{eq:SVD3}.
However, in order to preserve the sparsity in $\mathbf{X}$ while optimizing, only the non-zero entries in $\mathbf{x}_{i_0 \cdot}$ are allowed to vary. 
For this purpose only a subset of columns in $\mathbf{E}_{j_0}$ is considered.
That is the columns which correspond to the non-zero entries of $\mathbf{x}_{i_0 \cdot}$. 
A matrix $\mathbf{P}_{i_0}$ is defined to restrict $\mathbf{x}_{i_0 \cdot}$ to only contain the non-zero-rows corresponding to $N_{j_0}$ non-zero rows
\begin{align*}
\mathbf{x}_{i_0 \cdot}^{(R)} = \mathbf{x}_{i_0 \cdot} \mathbf{P}_{i_0},
\end{align*}
with $R$ denoting the restriction. 
By applying the SVD to the error matrix $\mathbf{E}_{j_0}$ which has been restricted
\begin{align*}
\mathbf{E}_{j_0}^{(R)} = \mathbf{E}_{j_0} \mathbf{P}_{i_0},
\end{align*}
and updating $\mathbf{a}_{j_0}$ and $\mathbf{x}_{i_0 \cdot}^{(R)}$ the rank-1 approximation is found and the original representation vector is updated as 
\begin{align*}
\mathbf{x}_{i_0 \cdot} = \mathbf{x}_{i_0 \cdot}^{(R)} \mathbf{P}_{i_0}^{T}.
\end{align*}
The main steps of K-SVD is described in algorithm \ref{alg:K_SVD}. 
\begin{algorithm}[H]
\caption{K-SVD}
\begin{algorithmic}[1]
			\State$k = 0$			
			\State$\text{Initialize random} \quad  \textbf{A}_{(0)}$            
			\State$\text{Initialize} \quad \textbf{X}_{(0)}=\mathbf{0}$
			\State
            \Procedure{K-SVD}{$\textbf{A}_{(0)}$}    
            \State$\text{Normalize columns of} \ \textbf{A}_{(0)}$
            \While{$\text{error} \geq \text{limit}$} 
                \State $j = j+1$
                \For{$j \gets 1,2,\dots, L$} \Comment{updating each col. in $\textbf{X}_{(k)}$}
                	\State$\hat{\textbf{x}}_{j} = \min_{\textbf{x}} \|\textbf{y}_j -\textbf{A}_{(k-1)}\textbf{x}_{j}\| \quad \text{subject to} \quad \|\textbf{x}_{j}\| \leq k $ \Comment{use Basis Pursuit}
				\EndFor
				\State$\textbf{X}_{(k)} = \lbrace \hat{\textbf{x}}_{j} \rbrace_{j=1}^{L}$
				\For{$j_0 \gets 1, 2, \hdots, N$}
					\State$\Omega_{j_0} = \lbrace j \ \vert \ 1 \leq j \leq L, \textbf{X}_{(k)} [j_0, j]\neq 0\rbrace$
					\State$\text{From} \ \Omega_{j_0} \ \text{define} \ \textbf{P}_{i_0} $
					\State$\textbf{E}_{j_0} =  \textbf{Y} - \sum_{j \neq j_{0}}^{N} \textbf{a}_j \textbf{x}_{i \cdot}$
					\State$\textbf{E}_{j_0}^{(R)} =  \textbf{E}_{j_0} \textbf{P}_{i_0}$
					\State$\textbf{E}_{j_0}^{(R)} =\textbf{U} \boldsymbol{\Sigma} \textbf{V}^T$ \Comment{perform SVD}
					\State$\textbf{a}_{j_0} \gets \textbf{u}_{1}$ \Comment{update the $j_0$ col. in $\textbf{A}_{(k)}$}
					\State$\left( \textbf{x}_{i_0 \cdot} \right)^{(R)} \gets \sigma_{1} \textbf{v}_{1}$
					\State$\textbf{x}_{i_0 \cdot} \gets \left( \textbf{x}_{i_0 \cdot} \right)^{(R)} \textbf{P}_{i_0}^T $ \Comment{update the $i_0$ row in $\textbf{X}_{(k)}$}
				\EndFor
				\State$\text{error} = \Vert \textbf{Y} - \textbf{A}_{(k)} \textbf{X}_{(k)} \Vert_{F}^2 $
          		\EndWhile
            \EndProcedure
        \end{algorithmic} 
        \label{alg:K_SVD}
\end{algorithm}

\section{Principal Component Analysis}\label{app_sec:PCA}
In this section the method behind principal component analysis (PCA) used for the Cov-DL described in section \ref{sec:over_det}.

PCA is dimensionality reduction method used for reduction of dimensions of large data sets. 
In short, PCA used the statistical information of mean, variance and correlation between the data to transform the large data set into smaller datasets while maintaining most of the original information. 
These smaller data sets are known as the principal components and contain most of the information of the data set but with fewer dimensions. 
For some datasets, before PCA is applied, the data must undergo a standardization/scaling to remove any difference in the data. 
This is essential as large differences between the data would dominate. The standardization of a dataset $\mathbf{Z}$ is performed by
\begin{align*}
\tilde{\mathbf{z}}_i = \frac{\mathbf{z}_i - \bar{\mathbf{z}}_i}{s_{\mathbf{z}_i}}, \quad \forall i = 1, \dots, m
\end{align*}
where $\mathbf{z}_i$ is a row of a matrix $\mathbf{Z}$, $\bar{\mathbf{z}}_i$ is the sample mean of $\mathbf{z}_i$ and $s_{\mathbf{z}_i}$ is the standard deviation of $\mathbf{z}_i$. 
The standardized data set is now giving by $\tilde{\mathbf{Z}}$. 
The standardization step is unnecessary in the case of real EEG scalp measurements as no large difference between the data is present.

With $\mathbf{Z}$ or the scaled data $\tilde{\mathbf{Z}}$ a correlation matrix is computed as
\begin{align*}
\boldsymbol{\Sigma}_{\mathbf{Z}} = \text{Corr}(\mathbf{Z}) = \frac{1}{m} \mathbf{Z}^T \mathbf{Z}.
\end{align*}
From the correlation matrix a orthonormal basis of eigenvectors $\mathbf{p}_1, \dots, \mathbf{p}_m$ with corresponding eigenvalues $\lambda_1, \dots, \lambda_m$ exists, cf. theorem 6.15 in \cite[p. 375]{PCA}. 
Furthermore, one assumes that $\lambda_1 \geq \dots \geq \lambda_m$. 
%It exist beacuse Corr is symmetric
The eigenvectors and eigenvalues can be computed from the correlation matrix by e.g. using a singular value decomposition (SVD). 
With SVD an orthogonal matrix $\mathbf{P}$ with the eigenvectors $\mathbf{p}_1, \dots, \mathbf{p}_m$ as columns is obtained with the associated eigenvalues as a diagonal matrix denoted as $\mathbf{P}_{\text{diag}}$.
The principal components are then defined by
\begin{align*}
\mathbf{u}_i = \mathbf{Z} \mathbf{p}_i,
\end{align*}
where $\mathbf{p}_i$ is the $i$-th eigenvector of the correlation matrix $\boldsymbol{\Sigma}_{\mathbf{Z}}$. 
Thus, each principal component is a linear combination of the dataset $\mathbf{Z}$ \cite[p. 460] {PCA}. 
With the principal components the first $N$ components forms a set of basis vectors $\mathbf{U} = [\mathbf{u}_1, \dots, \mathbf{u}_N]$.

