The section is inspired by chapter 3 in \cite{phd2015} and the article \cite{Balkan2015}. INTRO.. 

\section{Introduction}
Covariance-domain dictionary learning (Cov-DL) is an algorithm proposed by O. Balkan \cite{Balkan2015}, claiming to successfully identify more active sources $k$ than available measurements $M$ from the multiple measurement vector model 

%\textit{
%Notes, maybe prior to this section: 
%\begin{itemize}
%\item To avoid this Cov-DL was proposed by O. Balkan. Though the theory of getting from $M$ sources to $M^2$ was established in \cite{Pal2015}. saying that the design of the measurement/dictionary matrix is essential to overcome this issue, new conditions will be developed for this to be a success \cite{Pal2015},
%\item about the approach: "This will in turn lead to the development of new sampling
%schemes and justify the need for the use of nested and
%coprime sampling. Another noteworthy point is that we distinguish
%the recovery of the sparse vector from that of its support."\cite{Pal2015}. \\
%\item In \cite{Pal2015} it is in fact the support of $x$ which if found by use of the covariance domain, and this is what Balkan uses on the dictionary matrix i suppose.
%\item the assumed prior(which has not been exploited before \cite{Pal2015}): a prior on the correlation of the received signal is to assume a (pseudo)diagonal covariance matrix for the unknown signal, that is $\frac{1}{L_s}\textbf{X}_{s}\textbf{X}_s^{T}$\cite{Pal2015}, which is then under the assumption of uncorrelated sources in $x$\cite{phd2015}. This is what lead to the correlation constraint (3.3) in phd.  
%\item the conditions mentioned on the measurement/dictionary matrix in \cite{Pal2015} is: "Sparse sampling
%schemes such as nested and coprime sampling will be shown to
%satisfy these conditions". For $\textbf{A}$ we need "$O(M^2)$ Kruskal Rank for their Khatri-Rao products". This is not necessarily what Balkan uses.   
%\item The prior mentioned by Balkan is the uncorrelated sources. 
%\end{itemize}
%We know:
%\begin{itemize}
%\item Covariance matrix:(when we have several observations) $\Sigma_{\textbf{x}} = Cov(\textbf{x}) = Cov(\textbf{x},\textbf{x}) = \mathbb{E}[ \textbf{x}\textbf{x}^T ]-\mu_{x}\mu_{x^{T}}$ , symmetric and positive simidefinite.
%\item Sample covariance matrix: $'\Sigma_{\textbf{x}} = \frac{1}{L} \sum_{i=1}^{L}(\textbf{x}_i-\mathbb{E}[\textbf{x}])(\textbf{x}_i-\mathbb{E}[\textbf{x}])^T$ using unbiased estimate since we assume $\mathbb{E}[\textbf{x}]=0$
%\item this becomes $\frac{1}{L}\textbf{X}\textbf{X}^T$, where L is the length of the sample segment.  
%\item the covariance of two vectors $cor(\textbf{x},\textbf{y})$ is also referred to as the cross-correlation.
%\end{itemize}
%}
\begin{align*}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align*}
where $\mathbf{Y} \in \mathbb{R}^{M \times L}$ is the observed  measurement matrix, $\mathbf{X} \in \mathbb{R}^{N \times L}$ the the source matrix and $\mathbf{E} \in \mathbb{R}^{M \times L}$ is the additional noise matrix \todo{f er samples pr sek., L er antal sampels i alt, Ls er antal samples pr segment og ts er længen pr segment i sekunder}.
\\
Let $f$ be the sample frequency of the observed data $\mathbf{Y}$ and let $s$ denoted a segment index. As such the observed data can be divided into segments $\mathbf{Y}_s \in \mathbb{R}^{M \times t_s f}$, possibly overlapping, where $t_s$ is the length of the segments in seconds. For each segment the linear model still holds and is rewritten into
\begin{align*}
\mathbf{Y}_s = \mathbf{AX}_s + \textbf{E}_s, \quad \forall s.
\end{align*}
Cov-DL takes advantage of the covariance domain where the dimensionality is increased allowing for an enlarged number of sources to be active while the dictionary remains recoverable.  
An important aspect of this method is the prior assumption that the sources within one segment are uncorrelated, that is the rows of $\textbf{X}_s$ being mutually uncorrelated. 
From the assumption of uncorrelated sources it can be assumed that the sample covariance of $\textbf{X}_s$ becomes nearly diagonal. This is of importance when the system is transformed to the covariance domain.    
\\
The Cov-DL do only recover the mixing matrix $\mathbf{A}$ given the measurements $\textbf{Y}$. Given $\textbf{A}$ the source matrix $\mathbf{X}$ is to be recovered by use of the Multiple Sparse Bayesian Learning algorithm, this is described in section \ref{ch:M-SBL} 


\section{Covariances domain representation}
Consider the covariance of a vector $\textbf{x}_i$  
\begin{align*}
\boldsymbol{\Sigma}=\mathbb{E}[(\textbf{x}_i-\mathbb{E}[\textbf{x}_i])(\textbf{x}_i-\mathbb{E}[\textbf{x}_i])].
\end{align*} 
Assume that all samples has zero mean and the same distribution within one segment. The observed measurements $\mathbf{Y}_s \in \mathbb{R}^{M\times L}$ can be described in the covariance domain by the sample covariance $\hat{\boldsymbol{\Sigma}}$ which is defined as the covariance among the $M$ measurements across the $L_s$ samples. That is a $M \times M$ matrix $\boldsymbol{\Sigma}_{\mathbf{Y}_s}=[\sigma_{jk}]$ with entries 
\begin{align*}
\sigma_{jk}= \frac{1}{L}\sum_{i=1}^{L}y_{ji}y_{ki}^T.
\end{align*}
Using matrix notation the sample covariance of $\mathbf{Y}_s$ can be written as
\begin{align*}
\hat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = \frac{1}{L} \mathbf{Y}_s \mathbf{Y}_s^T.
\end{align*}  
Similar the source matrix $\mathbf{X}_s$ can be described in the covariance domain by the sample covariance matrix
\begin{align*}
\hat{\boldsymbol{\Sigma}}_{\mathbf{X}_s} &= \frac{1}{L} \mathbf{X}_s \mathbf{X}_s^T = \boldsymbol{\Lambda}_s + \boldsymbol{\varepsilon} 
\end{align*}
From the assumption of uncorrelated sources within $\mathbf{X}_s$ the sample covariance matrix is expected to be nearly diagonal, thus it can be written as $\boldsymbol{\Delta}_s + \boldsymbol{\varepsilon}$ where $\boldsymbol{\Delta}_s$ is a diagonal matrix consisting of the diagonal entries of $\hat{\boldsymbol{\Sigma}}_{\mathbf{X}_s}$ and $ \boldsymbol{\varepsilon}$ is the estimation error\cite{Balkan2015}.
\\
Each segment is then modelled in the covariance domain as
\begin{align} 
\hat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = 
\frac{1}{L_s}\mathbf{Y}_s \mathbf{Y}_s^T &= 
\frac{1}{L_s} \left( \mathbf{A} \mathbf{X}_s + \mathbf{E}_s \right) \left( \mathbf{A} \mathbf{X}_s + \mathbf{E}_s\right)^T \nonumber \\ 
\mathbf{Y}_s \mathbf{Y}_s^T &= (\textbf{AX}_s)(\textbf{AX}_s)^T + \textbf{E}_s\textbf{E}_s^T+\textbf{E}_s(\textbf{AX}_s)^T+\textbf{AX}_s\textbf{E}_s^T \nonumber \\
&= \textbf{AX}_s\textbf{X}_s^T\textbf{A}^T +  \textbf{E}_s\textbf{E}_s^T + \textbf{E}_s\textbf{X}_s^T\textbf{A}^T + \textbf{AX}_s\textbf{E}_s^T \nonumber \\
&= \textbf{A}(\boldsymbol{\Delta}_s+\boldsymbol{\varepsilon})\textbf{A}^T + \textbf{E}_s\textbf{E}_s^T + \textbf{E}_s\textbf{X}_s^T\textbf{A}^T + \textbf{AX}_s\textbf{E}_s^T \nonumber \\
&= \textbf{A}\boldsymbol{\Delta}_s\textbf{A}^T + \textbf{A}\boldsymbol{\varepsilon}\textbf{A}^T + \textbf{E}_s\textbf{E}_s^T + \textbf{E}_s\textbf{X}_s^T\textbf{A}^T + \textbf{AX}_s\textbf{E}_s^T \label{eq:noise1}\\
&= \textbf{A}\boldsymbol{\Delta}_s\textbf{A}^T + \widetilde{\textbf{E}} \label{eq:noise2}\\
\end{align}
From \eqref{eq:noise1} to \eqref{eq:noise2} all terms where noise is included are defined as a united noise term $\widetilde{\textbf{E}}$. \todo{er yderligere argumentation nødvendig her?}. By vector notation \eqref{eq:noise2} is rewritten and then vectorised. Because the covariance matrix $\hat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s}$ is symmetric it is sufficient to vectorize only the lower triangular parts, including the diagonal. For this the function $\text{vec}(\cdot)$ is defined to map a symmetric $M \times M$ matrix into a vector of size $\frac{M(M+1)}{2}$ making a row-wise vectorization of its upper triangular part. Furthermore, let vec$^{-1}(\cdot)$ be the inverse function for de-vectorisation. This results in the following model        
\begin{align}
\boldsymbol{\Sigma}_{\mathbf{Y}_s} &= \sum_{i=1}^{N} \boldsymbol{\Delta}_{s_{ii}} \textbf{a}_i\textbf{a}_i^{T} + \widetilde{\textbf{E}} \nonumber \\
\text{vec}(\boldsymbol{\Sigma}_{\mathbf{Y}_s}) &= \sum_{i=1}^N \boldsymbol{\Lambda}_{s_{ii}} \text{vec}(\mathbf{a}_i \mathbf{a}_i^T) + \text{vec}( \widetilde{\textbf{E}}) \nonumber \\
&= \sum_{i=1}^N \mathbf{d}_i \boldsymbol{\Lambda}_{s_{ii}} + \text{vec}( \widetilde{\textbf{E}}) \nonumber \nonumber \\
&= \mathbf{D} \boldsymbol{\delta}_s + \text{vec}( \widetilde{\textbf{E}}), \quad \forall s. \label{eq:cov1}
\end{align}
Here $\boldsymbol{\delta}_s \in \mathbb{R}^{N}$ contains the diagonal entries of the source sample-covariance matrix $\boldsymbol{\Lambda}_s$
and the matrix $\mathbf{D} \in \mathbb{R}^{M(M+1)/2 \times N}$ consists of the columns $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$. Note that $\mathbf{D}$ and $\boldsymbol{\delta}_s$ are unknown while $\text{vec}(\boldsymbol{\Sigma}_{\mathbf{Y}_s})$ is known from the observed data.
By this transformation to the covariance domain one segments is now represented as the single measurement model with $M(M+1)/2$ ''measurements''. It has been shown that this model allow for identification of $k\leq M(M+1)/2$ active sources \cite{Pal2015}, which is a much weaker sparsity constraint than the original sparsity constraint $k\leq M$. 
The purpose of the Cov-DL algorithm is to leverage this model to find the dictionary $\textbf{A}$ from $\textbf{D}$ and then still allow for $k\leq M(M+1)/2$ active sources to be identified. That is the number of active sources are allowed to exceed the number of observations as intended.

\section{Determination of the Dictionary}
The goal is now to learn first $\textbf{D}$ and then the associated mixing matrix $\textbf{A}$. Two methods are considered relying on the relation of $M$ and $N$.   

\subsubsection*{Cov-DL1 -- under-determined \textbf{D}}
In the case of $N > \frac{M(M+1)}{2}$ $\textbf{D}$ becomes under-determined. This is  similar to the original system being under-determined when $N>M$. 
Thus, it is again possible to solve the under-determined system if certain sparsity is withhold. Namely $\boldsymbol{\delta}_s$ being $\frac{M(M+1)}{2}$-sparse.
Assuming the sufficient sparsity on $\boldsymbol{\delta}_s$ is withhold it is possible to learn the dictionary matrix of the covariance domain $\mathbf{D}$ by traditional dictionary learning methods applied to the observations represented in the covariance domain $\text{vec}(\boldsymbol{\Sigma}_{\mathbf{Y}_s})$ for all $s$.
For this K-SVD algorithm, described in section \ref{sec:dictionarylearning} is used. 
Note here that the number of samples that are used to learn the dictionary is remarkable reduces as one segment effectively corresponds to one sample in the covariance domain.  \\ 
When $\mathbf{D}$ is learned it is possible to find the original mixing matrix $\mathbf{A}$ that generated $\textbf{D}$ through the relation $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$. Here each column is found by the optimisation problem 
\begin{align*}
\min_{\textbf{a}_i} \| \text{vec}^{-1}(\textbf{d}_i) -\textbf{a}_i\textbf{a}_i^T\|_2^2, 
\end{align*}
for which the global minimizer is $\mathbf{a}^{\ast}_i=\sqrt{\lambda_i} \textbf{b}_i$. Here $\lambda_i$ is the largest eigenvalue of $\text{vec}^{-1}(\textbf{d}_i)$,
\begin{align*}
\text{vec}^{-1}(\textbf{d}_i) = 
\begin{bmatrix}
d_{11} & d_{12} & \cdots & d_{1N} \\
d_{21} & d_{22} & \cdots & d_{2N} \\
\vdots & \vdots & \ddots & \vdots \\
d_{N1} & d_{N2} & \cdots & d_{NN}
\end{bmatrix}, \quad i \in [N]
\end{align*}
and $\textbf{b}_i$ is the corresponding eigenvector.\todo{redegørelse for resultatet her skal laves}
  
%\begin{algorithm}[H]
%\caption{Cov-DL1}
%\begin{algorithmic}[1]
%			\Procedure{Segmentation}{$\textbf{Y}$}
%			\State $\text{define lengt of segments}\ L_s$
%			\State $n_{seg} = L/L_S$
%			\State $\text{split Y into } \ n_{seg} \ \text{segments of length} \ L_s$
%			\EndProcedure  
%            \Procedure{Cov-DL1}{$\textbf{Y}_s$}    
%			\For{ $s \gets 1,\hdots, \text{n_seg}$ }			
%				\State$\text{compute sample covariance matrix}\ \widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s} $
%				\State$\textbf{y}_s = \text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$	
%			\EndFor			
%			\State$\textbf{Y}_cov = \{\textbf{y}_s\}_{s=1}^{\text{n\_seg}}$		
%			\State$k=0$
%			\Procedure{K-SVD}()
%			\EndProcedure            
%           \EndProcedure
%        \end{algorithmic} 
%        \label{alg:basicICA}
%\end{algorithm}

\subsubsection*{Cov-DL2 -- undercomplete \textbf{D}}
\todo{kan ve udelade denne med det argument at vi altid vil finde så mange sources som muligt. fordi vi ikke ved hvor mange der der }



%\paragraph{Notes}
%
%\begin{itemize}
%\item In the case of EEG, this allows at most k = O(M) EEG sources to be simultaneously active which limits direct applicability of dictionary learning to low-density EEG systems.
%\item We wish to handled cases where we have $\binom{N}{k}$ sources, where $1 \leq k \leq N$ can be jointly active.
%\item section 3.3.1 in phd.
%\item i phd så forstås source localisation som at finde support for x og source identification forstås som at finde værdierne i de non-zero indgange(?)
%
%\end{itemize}
%
%Trine indhold:
%\begin{itemize}
%\item til CS afsnit: kilde på at nok sparse giver den rigtige løsning, se [39] fra phd
%\item Dictionary learning:
%\begin{itemize}
%\item jointly solveing optimisation problem where g is introduced
%\item her siges at k<M= er nødvendigt for recovery( af x går jeg ud fra ) Fordi at enhver random dictionary kan bruges til at reprensentere y i dim M ved brug af M basis vektore i A.
%\item altså når k > M så udgør de berørte søjler i A ikke længere en basis og x kan ikke bestemmes entydigt ? derfor er k<M et krav i standard algorithmer
%\item der sættes så et nyt frame work hvor vi kan bruge standard algoritmer i vores tilfælde med k>M (forhåbenligt) 
%\end{itemize}
%\item EEG is non-stationary(afsnit 1.3.1 phd) source dynamics ændres med tiden afhængig af opave, fejlkilder kan være non stationære. så ICA dur ikke mixture model ICA var bedre til non stationær med statigvæk limited til k = M. 
%\item the contribution er så at: support for X i MMV problem kan findes for k>M, altså altal non-zero entry k i x, med sufficient conditions ved sparse bayesian learning. herefter ved brug at uncorrelation af sources i et dictionary learning frame work så kan vi recover x når k>M. 
%\end{itemize}  
%  















