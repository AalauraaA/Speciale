\section{Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery of a signal from a minimal number of observed measurements. 
It is build upon empirical observations assuring that many signals can be approximated by remarkably sparser signals.   
Assume linear acquisition of the original measurements, then the relation between the measurements and the signal to be recovered can be modelled by the linear system \eqref{eq:SMV_model} \cite{FR}.  
\\ 
In compressive sensing terminology, $\mathbf{x} \in \mathbb{R}^N$ is the signal of interest which is sought recovered from the measurements $\mathbf{y} \in \mathbb{R}^M$ by solving the linear system \eqref{eq:SMV_model}. 
The coefficient matrix $\mathbf{A}$ is in the context of compressive sensing referred to as the mixing matrix or the dictionary matrix.  
In the typical compressive sensing case the system is under-determined, $M < N$,  and there exist infinitely many solutions, provided that a solution exist.
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal, hence the term sparse signal recovery \cite{FR}.
 
\subsection{Optimisation Problem}\label{sec:opti}
To find a $k$-sparse solution to the linear system \eqref{eq:SMV_model} it can be viewed as the following optimisation problem. 
\begin{align*}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_0 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align*}
Unfortunately, this optimisation problem is non-convex due to the definition of the $\ell_0$-norm and is therefore difficult to solve -- it is an NP-hard problem. 
Instead, by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimisation problem can be approximated and hence becomes computationally feasible\todo{3.2: skal vi indfør z som en approximation til x. og så et nyt omega eller? eller kan vi lade x* være løslingen til både P0 og P1} \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align} 
With this optimisation problem we find the best $k$-sparse solution $\mathbf{x}^\ast$. 
This method is referred to as Basis Pursuit. 
\\
The following theorem justifies that the $\ell_1$ optimisation problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}
A mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined with columns $\mathbf{A} = [\mathbf{a}_1, \dots, \mathbf{a}_N]$. 
By assuming uniqueness of a solution $\mathbf{x}^{\ast}$ to
\begin{align*}
\min_{\mathbf{x} \in \mathbb{R}^N} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y},
\end{align*}
the system $\lbrace \mathbf{a}_j, j \in \text{supp}( \mathbf{x}^\ast) \rbrace$ is linearly independent, and in particular
\begin{align*}
\Vert \mathbf{x}^\ast \Vert_0 = \text{card}(\text{supp} (\mathbf{x}^\ast)) \leq M.
\end{align*}
\end{theorem}
To prove this theorem one needs to realise that the set $\lbrace \mathbf{a}_j, j \in S \rbrace \leq M$, with $S = \text{supp}(\mathbf{x}^\ast)$, can not have more than $M$ linearly independence columns. This will be done by a contradiction.
So when $M \ll N$ a sparse signal is automatically achieved.
\begin{proof}
Assume that the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ is linearly dependent with the support $S = \text{supp}(\mathbf{x}^\ast)$.
Thus a non-zero vector $\mathbf{v} \in \mathbb{R}^N$ supported on $S$ exists such that $\mathbf{Av} = \textbf{0}$ -- the system is linear dependent. The unique solution $\mathbf{x}^\ast$ can then be written as, for any $t \neq 0$,
\begin{align}\label{eq:non_zero_t}
\Vert \mathbf{x}^\ast \Vert_1 < \Vert \mathbf{x}^\ast + t \mathbf{v} \Vert_1 = \sum_{l \in S} \vert x_l^\ast + t v_l \vert = \sum_{l \in S} \text{sgn}(x_l^\ast + t v_l )(x_l^\ast + t v_l ).
\end{align}
For a small $|t|$
\begin{align*}
|t| < \min_{l \in S} \frac{\vert x_l^\ast \vert}{\Vert \mathbf{v} \Vert_{\infty}},
\end{align*}
then the sign function become
\begin{align*}
\text{sgn}(x_l^\ast + t v_l) = \text{sgn}(x_l^\ast), \quad \forall l \in S.
\end{align*}
By including this result in \eqref{eq:non_zero_t} and remembering $t \neq 0$:
\begin{align*}
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{l \in S} \text{sgn}(x_l^{\ast})(x_l^{\ast} + t v_l ) = \sum_{l \in S} \text{sgn}(x_l^{\ast})x_l^{\ast} + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l.
\end{align*}
From this it can be seen that it is always possible to choose $t \neq 0$ small enough such that 
\begin{align*}
t \sum_{l \in S} \text{sgn}(x_l^\ast)v_l \leq 0,
\end{align*}
which contradicts that $\mathbf{v}$ make the columns of $\mathbf{A}$ linear dependent. 
Therefore, the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ must be linearly independent.
\end{proof}

The Basis Pursuit algorithm makes the foundation of several algorithms solving alternative versions of \eqref{eq:SMV_p1} where noise is incorporated. 
An alternative solution method includes greedy algorithms such as the Orthogonal Matching Pursuit (OMP) \cite[P. 65]{FR}. 
At each iteration of the OMP algorithm an index set $S$ is updated by adding the index corresponding to a column in $\mathbf{A}$ that best describes the residual, hence greedy.
That is the part of $\mathbf{y}$ that is not yet explained by $\mathbf{Ax}$ is included. 
Then $\mathbf{x}$ is updated as the vector, supported by $S$, which minimize the residual, that is also the orthogonal projection of $\mathbf{y}$ onto the span$\lbrace \mathbf{a}_l \ \vert \ l \in S \rbrace$. The algorithm for OMP can be found in the appendix \ref{alg:OMP}.

