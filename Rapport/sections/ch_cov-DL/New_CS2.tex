\chapter{Introduction to Compressive Sensing}\label{sec:CS}
Compressive sensing is the theory of efficient recovery of a signal from a minimal number of observed measurements. 
It is build upon empirical observations assuring that many signals can be approximated by remarkably sparser signals.   
Assume linear acquisition of the observed measurements, then the relation between the measurements and the signal to be recovered can be modelled by the multiple measurement vector (MMV) model \eqref{eq:MMV_model} \cite{FR}. 

Through this section the introduction of the theory behind compressive sensing will be presented for one measurement vector of \eqref{eq:MMV_model}, $\mathbf{y}$, such that the theory is based on the linear system \eqref{eq:SMV_model}. This will be done for simplicity but the theory will still apply for the extend linear system \eqref{eq:MMV_model}.

In compressive sensing terminology, $\mathbf{x} \in \mathbb{R}^N$ is the signal of interest which is sought recovered from the EEG measurement $\mathbf{y} \in \mathbb{R}^M$ by solving the linear system \eqref{eq:SMV_model}. 
In the typical compressive sensing case the system is under-determined, $M < N$, and there will therefore exist infinitely many solutions, provided that one solution exist.
However, by enforcing certain sparsity constraints it is possible to recover the wanted signal, hence the term sparse signal recovery \cite{FR}. The sparsity constraints are the ones presented in \ref{sec:SMV} where the $\ell_0$ is introduced to count the non-zeros of the signal of interest, the source vector $\mathbf{x}$. The number of non-zeros (active sources) $k$ describe how sparse the source vector is.
 
%\subsection{Optimisation Problem}\label{sec:opti}
To find a $k$-sparse solution to the linear system \eqref{eq:SMV_model} it can be viewed as the following optimisation problem. 
\begin{align*}\label{eq:SMV_p0}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_0 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align*}
Unfortunately, this optimisation problem is non-convex due to the definition of the $\ell_0$-norm and is therefore difficult to solve -- it is an NP-hard problem. 
Instead, by replacing the $\ell_0$-norm with the $\ell_1$-norm, the optimisation problem can be approximated and hence becomes computationally feasible \todo{4.1: skal vi indfør z som en approximation til x. og så et nyt omega eller? eller kan vi lade x* være løslingen til både P0 og P1} \cite[p. 27]{CS}
\begin{align}\label{eq:SMV_p1}
\mathbf{x}^\ast = \arg \min_{\mathbf{x} \in \mathbb{C}} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y}.
\end{align} 
With this optimisation problem the best $k$-sparse solution $\mathbf{x}^\ast$ can be found. 
The optimisation problem is referred to as $\ell_1$ optimisation problem or Basis Pursuit. 
The following theorem justifies that the $\ell_1$ optimisation problem finds a sparse solution \cite[p. 62-63]{FR}.
\begin{theorem}\label{th:CS_A}
A mixing matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined with columns $\mathbf{A} = [\mathbf{a}_1, \dots, \mathbf{a}_N]$. 
By assuming uniqueness of a solution $\mathbf{x}^{\ast}$ to
\begin{align*}
\min_{\mathbf{x} \in \mathbb{R}^N} \Vert \mathbf{x} \Vert_1 \quad \text{subject to} \quad \mathbf{Ax} = \mathbf{y},
\end{align*}
the system $\lbrace \mathbf{a}_j, j \in \text{supp}( \mathbf{x}^\ast) \rbrace$ is linearly independent, and in particular
\begin{align*}
\Vert \mathbf{x}^\ast \Vert_0 = \text{card}(\text{supp} (\mathbf{x}^\ast)) \leq M.
\end{align*}
\end{theorem}
%To prove this theorem one needs to realise that the set $\lbrace \mathbf{a}_j, j \in S \rbrace$, with $S = \text{supp}(\mathbf{x}^\ast)$, can not have more than $M$ linearly independence columns. This will be done by a contradiction.
%So when $M \ll N$ a sparse signal is automatically achieved.
\begin{proof}
Assume that the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ of $l$ columns from matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is linearly dependent with the support $S = \text{supp}(\mathbf{x}^\ast)$.
Thus a non-zero vector $\mathbf{v} \in \mathbb{R}^N$ supported on $S$ exists such that $\mathbf{Av} = \textbf{0}$ -- the system is linear dependent. The unique solution $\mathbf{x}^\ast$ can then be written as, for any $t \neq 0$,
\begin{align}\label{eq:non_zero_t}
\Vert \mathbf{x}^\ast \Vert_1 < \Vert \mathbf{x}^\ast + t \mathbf{v} \Vert_1 = \sum_{l \in S} \vert x_l^\ast + t v_l \vert = \sum_{l \in S} \text{sgn}(x_l^\ast + t v_l )(x_l^\ast + t v_l ).
\end{align}
For a small $|t|$
\begin{align*}
|t| < \min_{l \in S} \frac{\vert x_l^\ast \vert}{\Vert \mathbf{v} \Vert_{\infty}},
\end{align*}
then the sign function become
\begin{align*}
\text{sgn}(x_l^\ast + t v_l) = \text{sgn}(x_l^\ast), \quad \forall l \in S.
\end{align*}
By including this result in \eqref{eq:non_zero_t} and remembering $t \neq 0$:
\begin{align*}
\Vert \textbf{x}^{\ast} \Vert_1 < \sum_{l \in S} \text{sgn}(x_l^{\ast})(x_l^{\ast} + t v_l ) = \sum_{l \in S} \text{sgn}(x_l^{\ast})x_l^{\ast} + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l = \Vert \textbf{x}^{\ast} \Vert_1 + t \sum_{l \in S} \text{sgn}(x_l^{\ast})v_l.
\end{align*}
From this it can be seen that it is always possible to choose $t \neq 0$ small enough such that 
\begin{align*}
t \sum_{l \in S} \text{sgn}(x_l^\ast)v_l \leq 0,
\end{align*}
which contradicts that $\mathbf{v}$ make the columns of $\mathbf{A}$ linear dependent. 
Therefore, the set $\lbrace \mathbf{a}_l, l \in S \rbrace$ must be linearly independent.
\end{proof}
From the theorem is must be conclude that the choice of the mixing matrix $\mathbf{A}$ has a significant impact on whenever a unique solution $\mathbf{x}^\ast$ exist for the $\ell_1$ optimisation problem \eqref{eq:SMV_p1}. Therefore, when recovering $\mathbf{A}$, some considerations regarding the recovering process of $\mathbf{A}$ must be taken into account. A method for the recovering of $\mathbf{A}$ could be to use a dictionary. This will be explain in the following section \ref{sec:dictionarylearning}.
%The $\ell_1$ optimisation makes the foundation of several algorithms solving alternative versions of \eqref{eq:SMV_p1} where noise is incorporated. 

An alternative solution method to the $\ell_1$ optimisation includes greedy algorithms such as the Orthogonal Matching Pursuit (OMP) \cite[P. 65]{FR}. 
The OMP algorithm is an iteration process where an index set $S$ is updated -- at each iteration -- by adding indices corresponding to the columns of $\mathbf{A}$ which describe the residual best possible, hence greedy.
The vector $\mathbf{x}$ is then updated by a vector supported on $S$ which minimise the residual, that is the orthogonal projection of $\mathbf{y}$ onto the span$\lbrace \mathbf{a}_l \ \vert \ l \in S \rbrace$.
