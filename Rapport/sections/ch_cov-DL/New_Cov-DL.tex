\section{Covariance Domain Representation}\label{sec:cov}
Consider a single sample vector $\textbf{y}_i\in \mathbb{R}^{M}$, containing EEG measurements. 
The covariance of $\textbf{y}_i$ is defined as
\begin{align*}
\boldsymbol{\Sigma}_{\textbf{y}_i}=\mathbb{E}[(\textbf{y}_i-\mathbb{E}[\textbf{y}_i])(\textbf{y}_i-\mathbb{E}[\textbf{y}_i])^T],
\end{align*}
where $\mathbb{E}[\cdot]$ is the expected value operator. 
Let $\textbf{Y}_{s}=\left[\textbf{y}_1, \hdots ,\textbf{y}_{L_s}\right]$ be the observed measurements matrix containing all samples of segment $s$.
Furthermore, assume that all sample vectors $\textbf{y}_i$ within one segment has zero mean and the same distribution.  
Then $\mathbf{Y}_s \in \mathbb{R}^{M \times L_s}$ is to be described in the covariance domain by the sample covariance $\widehat{\boldsymbol{\Sigma}}$. The sample covariance $\widehat{\boldsymbol{\Sigma}}$ is defined as the empirical covariance among the $M$ measurements across the $L_s$ samples. That is a $M \times M$ matrix $\widehat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = [\sigma_{jk}]$ with entries 
\begin{align*}
\sigma_{jk}= \frac{1}{L_s}\sum_{i=1}^{L_s} y_{ji} y_{ki}.
\end{align*}
Using matrix notation the sample covariance of $\mathbf{Y}_s$ can be written as
\begin{align*}
\widehat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = \frac{1}{L_s} \mathbf{Y}_s \mathbf{Y}_s^T.
\end{align*} 
Similar, the source matrix $\mathbf{X}_s$ can be described in the covariance domain by the sample covariance matrix:
\begin{align*}
\widehat{\boldsymbol{\Sigma}}_{\mathbf{X}_s} &= \frac{1}{L_s} \mathbf{X}_s \mathbf{X}_s^T = \boldsymbol{\Lambda}_s + \boldsymbol{\varepsilon}. 
\end{align*}
The second equality comes from the assumption of the sources within $\mathbf{X}_s$ being uncorrelated within one segment. By uncorrelated sources $\mathbf{X}_s$ the sample covariance matrix is asumed to be nearly diagonal. Thus it can be written as $\boldsymbol{\Lambda}_s + \boldsymbol{\varepsilon}$ where $\boldsymbol{\Lambda}_s$ is a diagonal matrix consisting of the diagonal entries of $\widehat{\boldsymbol{\Sigma}}_{\mathbf{X}_s}$ and $ \boldsymbol{\varepsilon}$ is a non-diagonal matrix representing the estimation error \cite{Balkan2015}.

Each segment is now modelled in the covariance domain, note that the $\frac{1}{L_s}$ factor is left out in order to comply to the instruction of the source \cite{Balkan2015}\todo{convenience alert! we should remove $\Sigma$ here i think}.
\begin{align} 
\widehat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s} = \mathbf{Y}_s \mathbf{Y}_s^T &=  \left( \mathbf{A}_s \mathbf{X}_s + \mathbf{E}_s \right) \left( \mathbf{A}_s \mathbf{X}_s + \mathbf{E}_s\right)^T \nonumber \\ 
 &=  (\textbf{A}_s\textbf{X}_s)(\textbf{A}_s\textbf{X}_s)^T + \textbf{E}_s \textbf{E}_s^T + \textbf{E}_s (\textbf{A}_s\textbf{X}_s)^T + \textbf{A}_s\textbf{X}_s \textbf{E}_s^T  \nonumber \\
&= \textbf{A}_s\textbf{X}_s \textbf{X}_s^T \textbf{A}_s^T + \textbf{E}_s \textbf{E}_s^T + \textbf{E}_s \textbf{X}_s^T \textbf{A}_s^T + \textbf{A}_s\textbf{X}_s \textbf{E}_s^T  \nonumber \\
&= \textbf{A}_s(\boldsymbol{\Lambda}_s +\boldsymbol{\varepsilon}) \textbf{A}_s^T + \textbf{E}_s \textbf{E}_s^T + \textbf{E}_s \textbf{X}_s^T \textbf{A}_s^T + \textbf{A}_s\textbf{X}_s \textbf{E}_s^T \nonumber \\
&= \textbf{A}_s \boldsymbol{\Lambda}_s \textbf{A}_s^T + \textbf{A}_s \boldsymbol{\varepsilon} \textbf{A}_s^T + \textbf{E}_s \textbf{E}_s^T + \textbf{E}_s \textbf{X}_s^T \textbf{A}_s^T + \textbf{A}_s\textbf{X}_s \textbf{E}_s^T \label{eq:noise1} \\
&= \textbf{A}_s \boldsymbol{\Lambda}_s \textbf{A}_s^T + \widetilde{\textbf{E}}_s \label{eq:noise2}
\end{align}
From \eqref{eq:noise1} to \eqref{eq:noise2} all terms where noise, $\boldsymbol{\varepsilon}$ and $\mathbf{E}_S$, is included, are aggregated in a joint noise term $\widetilde{\textbf{E}}$. 
Next, the expression \eqref{eq:noise2} is rewritten through a vectorization. 
Because the covariance matrix $\widehat{\boldsymbol{\Sigma}}_{\mathbf{Y}_s}$ is symmetric it is sufficient to vectorize only the lower triangular part, including the diagonal. 
For this purpose the function $\text{vec}(\cdot)$ is defined to map a symmetric $M \times M$ matrix into a vector of size $\widetilde{M}$ by row-wise vectorization of the lower triangular part. The increased dimension $\widetilde{M}$ becomes 
\begin{align}
\widetilde{M} := \frac{M(M+1)}{2}.
\end{align}
Furthermore, let $\text{vec}^{-1}: \mathbb{R}^{\widetilde{M}} \rightarrow \mathbb{R}^{M\times M}$ be the inverse function for devectorisation. 

Applying $\text{vec}(\cdot)$ results in the following expression, which concludes the transformation of model \eqref{eq:MMV_seg} into the covariance domain. 
Let $\textbf{a}_i$ be the $i$-th column of $\textbf{A}_s$, as such the matrix product can be written in sum form where $\boldsymbol{\Lambda}_{s_{ii}}$ is the $ii$-th entry of $\boldsymbol{\Lambda}_s$.  
\begin{align}
\mathbf{Y}_s \mathbf{Y}_s^T &= \sum_{i=1}^{N} \textbf{a}_i \boldsymbol{\Lambda}_{s_{ii}} \textbf{a}_i^{T} + \widetilde{\textbf{E}}_s, \quad \boldsymbol{\Lambda}_{s_{ii}} \nonumber \\
\text{vec}\left( \mathbf{Y}_s \mathbf{Y}_s^T \right) &= \sum_{i=1}^N \text{vec}(\mathbf{a}_i \mathbf{a}_i^T) \boldsymbol{\Lambda}_{s_{ii}} + \text{vec}( \widetilde{\textbf{E}}_s) \nonumber \\
&= \sum_{i=1}^N \mathbf{d}_i \boldsymbol{\Lambda}_{s_{ii}} + \text{vec}( \widetilde{\textbf{E}}_s) \nonumber \\
&= \mathbf{D} \boldsymbol{\delta}_s + \text{vec}( \widetilde{\textbf{E}}_s), \quad \forall s. \label{eq:cov1}
\end{align}
Here $\boldsymbol{\delta}_s \in \mathbb{R}^{N}$ contains the diagonal entries of the source sample-covariance matrix $\boldsymbol{\Lambda}_s$
and the matrix $\mathbf{D} \in \mathbb{R}^{\widetilde{M} \times N}$ consists of the columns $\mathbf{d}_j = \text{vec}(\mathbf{a}_j \mathbf{a}_j^T)$. Note that $\mathbf{D}$ and $\boldsymbol{\delta}_s$ are unknown while $\text{vec}\left( \mathbf{Y}_s \mathbf{Y}_s^T \right)$ is known from the observed data.
By this transformation to the covariance domain, one segment is now represented by the single measurement model with $\widetilde{M}$ ''measurements''. 

It has been shown that this transformed model allows for identification of $k \leq \widetilde{M}$ active sources \cite{Pal2015}, which is a much weaker sparsity constraint than the original sparsity constraint $k \leq M$. 
The purpose of the Cov-DL algorithm is to leverage this transformed model to find the dictionary $\mathbf{A}$ from $\mathbf{D}$ and then still allow for $k \leq \widetilde{M}$ active sources to be recovered. 
That is the number of active sources are allowed to exceed the number of sensors as intended.

\section{Recovery of the Mixing Matrix}
The goal is now to learn first $\textbf{D}$ and then the associated mixing matrix $\textbf{A}$. 
Two methods are considered relying on the relation between $M$ and $N$. 
For now the noise vector is ignored.


\input{sections/ch_cov-DL/Cov-DL1.tex}
\input{sections/ch_cov-DL/Cov-DL2.tex}

\section{Pseudo Code of the Cov-DL Algorithm}\label{seg:alg_cov}
\begin{algorithm}[H]
\caption{Cov-DL}
\begin{algorithmic}[1]
           \Procedure{Cov-DL}{$\textbf{Y}_s$}    
			\For{$s \gets 1,\hdots, \text{n\_seg}$}			
				\State$\text{compute sample covariance matrix}\ \widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s} $
				\State$\textbf{y}_{\text{cov}_s} = \text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$	
			\EndFor			
			\State$\textbf{Y}_{\text{cov}} = \{\textbf{y}_{\text{cov}_s}\}_{s=1}^{\text{n\_seg}}$
			\State
			\If{$N \geq \widetilde{M}$}		
			\Procedure{K-SVD}{$\textbf{Y}_{\text{cov}}$}
			\State$\text{returns} \ \textbf{D} \in \mathbb{R}^{\widetilde{M}}\times N$
			\EndProcedure
			\For{$j \gets 1, \hdots, N$}
			\State$\textbf{T} = \text{vec}^{-1}(d_j)$            
			\State$\lambda_j\gets \max\{\text{eigenvalue}(\textbf{T})\}$
			\State$\textbf{b}_j \gets \ \text{eigenvector}(\lambda_j)$
			\State$\textbf{a}_j \gets \sqrt{\lambda_j}\textbf{b}_j$
			\EndFor
			\State$\textbf{A} = \{\textbf{a}_j\}_{j=1}^N$
			\EndIf
			\State
			\If{$N < \widetilde{M}$}
				\Procedure{PCA}{$\text{vec}(\boldsymbol{\Sigma}_{\textbf{Y}_s})$}
				\State$\text{returns} \ \textbf{U}\in \mathbb{R}^{\widetilde{M}\times N}$
				\EndProcedure
				\Procedure{Min. $\textbf{A}$ in }{$\Vert  \textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T - \textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T \Vert_{F}^{2}$}
				\State$\text{returns}\ \textbf{A}= \{\textbf{a}_j\}_{j=1}^{N}$
				\EndProcedure
			\EndIf
           \EndProcedure
        \end{algorithmic} 
        \label{alg:Cov1}
\end{algorithm}

\section{Considerations and Remarks}
Through this chapter different theory aspects haven been investigated to create a foundation to present one method to be use in the localization of the sources from EEG measurements -- the recovering of the mixing matrix $\mathbf{A}$ -- yet one method is still to be presented.
Before introducing a method to recover the source matrix $\mathbf{X}$ from the found mixing matrix $\mathbf{A}$ and EEG measurements $\mathbf{Y}$ some considerations and remarks regarding the Cov-DL algorithm must be taken. 
This will be used in the implementation of the Cov-DL which will be described in chapter \ref{ch:implementation}.

The length of each segment determined whenever the covariance of the source matrix $\mathbf{X}$ can be described as a diagonal matrix $\boldsymbol{\Lambda}$. 
That is a segment of $L_s$ samples becomes stationary and therefore the sources within that segment becomes uncorrelated -- the covariance of the source can be described by a diagonal matrix. 
The number of samples $L_s$ used in one segment affect whenever the segment is stationary or not. 
This must be taken into account in the preprocessing part for the implementation of Cov-DL when the EEG measurements are divided into segments.

For the Cov-DL algorithm when $\mathbf{D}$ is under-determined a dictionary learning algorithm K-SVD is used to learn the matrix $\mathbf{D}$ and by that an estimate, $\hat{\mathbf{A}}$, for the mixing matrix $\mathbf{A}$. 
Because of the segmentation the number of samples used in the dictionary learning are reduced remarkably and will affect the learning process. 
This is another point which must be taken into account in the preprocessing part of the implementation of Cov-DL. 
To improved the dictionary learning the overlapping of the segments can be look into as each segment will have some similarity and therefore learn towards one direction.

For the Cov-DL algorithm when $\mathbf{D}$ is over-determined the solution tends to be unique when $M < N < \widetilde{M}$ from testing the solution. 
That is the cost function tends toward a local minima and therefore an unique solution occur in first run of one trial. 
For the implementation of Cov-DL it would therefore be necessary to include several random initial points when finding the mixing matrix $\mathbf{A}$ for $\mathbf{D}$ being over-determined \todo{Dette afsnit skal lige revideres. Vi snakker om test og trial and error men vi har jo ikke beskrevet dette endnu - evt. mindre implementerings orienteret, i forhold til det vi har beskrevet endnu}.

For a general perspective the sources within the source matrix $\mathbf{X}$ must not be constant over time when using the MMV model \eqref{eq:MMV_model} \todo{Find lige kilde pÃ¥ dette argument}...