\subsection{Over-determined System}\label{sec:over_det}
Consider again the measurements represented in the covariance-domain \eqref{eq:cov1}.
In the case of $\widetilde{M} > N$ an over-determined system is achieved where $\mathbf{D}$ is high and thin. 
In general such a system is inconsistent. 
Thus, it is not possible to find $\mathbf{D}$ by traditional dictionary learning methods and different methods must be considered.
Let the set for transformed measurements be denoted by 
\begin{align*}
\mathbf{Y}_{\text{cov}} := \left\{\text{vec}\left( \widehat{\boldsymbol{\Sigma}}_{\mathbf{Y}_{s'}} \right) \right\}_{s' = 1}^{L_{s'}}.
\end{align*}
When $\widetilde{M} > N $ it is expected from model \eqref{eq:cov1} that the transformed measurements $\mathbf{Y}_{\text{cov}}$ live on or near a subspace of dimension $N$. 
This subspace is spanned by the columns of $\mathbf{D} \in \mathbb{R}^{\widetilde{M} \times N}$, and is denoted as $\mathcal{R}(\mathbf{D})$. 
To learn $\mathcal{R}(\mathbf{D})$ without having to impose any sparsity constraint on $\boldsymbol{\delta}$ it is possible to use principal component analysis (PCA). The basic theory of PCA in found in appendix \ref{app_sec:PCA}. 

PCA is applied to the set of transformed measurements $\mathbf{Y}_{\text{cov}}$ and the $N$ first principal components are determined. 
The principal components form a set of basis vectors $\mathbf{U} = [\mathbf{u}_1, \dots, \mathbf{u}_N]$. 
That is a new basis which spans the subspace on which $\mathbf{Y}_{\text{cov}}$ lives. 
Thus the equality $\mathcal{R}(\mathbf{U}) = \mathcal{R}(\mathbf{D})$ can be justified \cite{Balkan2015}. 
However, this equality does not imply that $\mathbf{D} = \mathbf{U}$. 
In the case of two bases spanning the same vector space, namely $\mathcal{R}(\mathbf{U}) = \mathcal{R}(\mathbf{D})$, the projection operator of the given subsets must be the same. 
Consider the projection operator $\text{P}$ projecting onto the space $\mathcal{R}(\mathbf{D})$ spanned by the columns of $\mathbf{D}$, $\text{P}:\mathbb{R}^{\widetilde{M}}\rightarrow \mathcal{R}(\mathbf{D})$. 
Due to $\mathbf{D}$ having full rank it is a well-known result that $\text{P} = \mathbf{D}(\mathbf{D}^T \mathbf{D})^{-1} \mathbf{D}^T$. 
Thus $\mathcal{R}(\mathbf{U})$ and $\mathcal{R}(\mathbf{D})$ having the same projection operator is true if and only if 
\begin{align*}
\mathbf{D} (\mathbf{D}^T\mathbf{D})^{-1} \mathbf{D}^T = \mathbf{U}(\mathbf{U}^T \mathbf{U})^{-1} \mathbf{U}^T.
\end{align*}
Now, remember from the relation between $\mathbf{A}$ and $\mathbf{D}$ that $\mathbf{d}_j = \text{vec}(\mathbf{a}_j \mathbf{a}_j^T)$. 
From this it is possible to obtain $\mathbf{D}$ and then $\mathbf{A}$, such that $\mathbf{D}$ spans $\mathcal{R}(\mathbf{U})$ and $\mathbf{d}_j = \text{vec}(\mathbf{a}_j \mathbf{a}_j^T)$. 
This is specified by the following optimization problem \cite{Balkan2015}
\begin{align}
\min_{\left\{\mathbf{a}_j\right\}_{j = 1}^{N}}\Vert  \mathbf{D}(\mathbf{D}^T \mathbf{D})^{-1} \mathbf{D}^T &- \mathbf{U}(\mathbf{U}^T \mathbf{U})^{-1}\mathbf{U}^T \Vert_{F}^{2} \nonumber \\
\text{s.t.} \ \mathbf{d}_j &= \text{vec}(\mathbf{a}_j \mathbf{a}_j^T),\label{eq:Cov_DL2}
\end{align}      
where $\mathbf{U}$ results from PCA performed on $\mathbf{Y}_{\text{cov}}$.
From the source proposing the method \cite{Balkan2015}, it is only notified that the optimization problem \eqref{eq:Cov_DL2} is minimized by quasi-Newton optimization methods. Hence, the exact minimization approach can not be recreated. 
In the following section the optimization problem is analyzed and processed in order to determine a suitable solution method. 
 
\subsection{Solution to Optimization Problem}
The optimization problem \eqref{eq:Cov_DL2} consists of an objective function forming a least-squares problem with respect to the Frobenius norm. It is given that the squared norm, both the Euclidean and the Frobenius norm, are strictly convex \cite[p. 173]{norm_optimization}.
Thus the objective function of \eqref{eq:Cov_DL2} is assumed to be convex.
The constraint in \eqref{eq:Cov_DL2} is a set of quadratic equality constraints. 
This categorizes the optimization problem as a quadratically constraint quadratic program. 
However, the constraints are not necessarily convex. 
By the constraints not being considered convex the optimization problem does not meet the requirements of a convex optimization problem. Hence, the numerical solution methods for convex optimization problems, for which convergence is ensured, does not apply directly. In fact a non-convex quadratically constraint quadratic program ins know to be a NP-hard problem \cite{qcqp}. Thus, some sort of relaxation is preferred.  

Due to the nature of the constraints, it should be possible to reformulate the objective function to include the constraints into the objective function. That is constructing an unconstrained least-squares problem, which is a special subclass of convex optimization \cite{cvxbook}.

Let $\mathbf{D} = f\left(\mathbf{a}_1, \dots, \mathbf{a}_N\right)$ where $f\left(\mathbf{a}_1, \dots, \mathbf{a}_N\right) = \left\{\mathbf{d}_j = \text{vec}\left(\mathbf{a}_j \mathbf{a}_j^T\right) \right\}_{j=1}^{N}$. 
Then an optimization problem without constraints is achieved, and it can be solved by use of basic gradient methods, for instance the Newton method. In order to avoid an explicit expression of the inverse Hessian, used in the Newton method, quasi-Newton methods can be considered \cite{Optimization2007}. 
The general idea of quasi-Newton methods is to let the direction of search be based on a positive definite matrix generated from available data as an alternative to the Hessian. 
%Chosen for this thesis is the Broyden-Fletcher-Goldfarb-Shanno method.

Rendering of general optimization theory and the theory of quasi-Newton methods is omitted in this thesis and the reader is referred to source \cite{Optimization2007}.
For the implementation of Cov-DL in chapter \ref{ch:implementation} a predefined optimization module, using a quasi-Newton method, will be applied.   
   

%To make the constraint convex and linear the constraints are rewritten with respect to the assumption that $\textbf{a}_i\textbf{a}_i^{T} = \textbf{A}_i$. This results in the following constraints 
%\begin{align}
%\textbf{d}_i &= \text{vec}(\textbf{A}_i) \\
%\textbf{A}_i &\geq  0 \\
%\text{rank}(\textbf{A}_i) &= 1 \ \text{altid rank 1 n√•r ydre produkt}
%\end{align}          
%by this a set of (hopefully)convex and linear constraints are achieved, both equality and inequality constraints.
%Now a classic quadratic programming problem is achieved      for which effective solution methods exist.   