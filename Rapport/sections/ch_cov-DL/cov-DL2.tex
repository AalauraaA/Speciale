\subsection{Over-determined System}\label{sec:over_det}
Consider again the measurements represented in the covariance domain \eqref{eq:cov1}.
In the case of $\widetilde{M} > N$ an over-determined system is achieved where $\mathbf{D}$ is high and thin. 
In general such a system is inconsistent. 
Thus, it is not possible to find $\mathbf{D}$ by traditional dictionary learning methods and different methods most be considered.
Let the set for transformed measurements be denoted by 
\begin{align*}
\mathbf{Y}_{\text{Cov}} := \left\{\text{vec}\left( \mathbf{Y}_{s'} \mathbf{Y}_{s'}^T \right) \right\}_{s' = 1}^{L_{s'}}.
\end{align*}
When $ \widetilde{M} > N $ it is expected from model \eqref{eq:cov1} that the transformed measurements $\mathbf{Y}_{\text{Cov}}$ live on or near a subspace of dimension $N$. 
This subspace is spanned by the columns of $\mathbf{D} \in \mathbb{R}^{\widetilde{M} \times N}$, and is denoted as $\mathcal{R}(\mathbf{D})$. 
To learn $\mathcal{R}(\mathbf{D})$ without having to impose any sparsity constraint on $\boldsymbol{\delta}$ it is possible to use Principal Component Analysis (PCA). The basic theory of PCA in found in appendix \ref{app_sec:PCA}. 

PCA is applied to the set of transformed measurements $\mathbf{Z}$ and the $N$ first principal components are determined. 
The principal components form a set of basis vectors $\mathbf{U} = [\mathbf{u}_1, \dots, \mathbf{u}_N]$. 
That is a new basis which spans the subspace of which $\mathbf{Z}$ lives. 
Thus the equality, as claimed in \cite{Balkan2015}, $\mathcal{R}(\mathbf{U}) = \mathcal{R}(\mathbf{D})$ can be justified\todo{LÆS: bør vi bare sige at det er givet? eller helgardere os?}.  
However, this equality does not imply that $\mathbf{D} = \mathbf{U}$. 
In the case of two bases spanning the same vector space, namely $\mathcal{R}(\mathbf{U}) = \mathcal{R}(\mathbf{D})$, the projection operator of the given subsets must be the same. 
Consider the projection matrix the projection operator $\text{P}$ projecting onto the space $\mathcal{R}(\mathbf{D})$ spanned by the columns of $\mathbf{D}$, $\text{P}:\mathbb{R}^{\widetilde{M}}\rightarrow \mathcal{R}(\mathbf{D})$. 
Due to $\mathbf{D}$ having full rank it is a well known result that $\text{P} = \mathbf{D}(\mathbf{D}^T \mathbf{D})^{-1} \mathbf{D}^T$. 
Thus $\mathcal{R}(\mathbf{U})$ and $\mathcal{R}(\mathbf{D})$ having the same projection matrix is true if and only if $\mathbf{D} (\mathbf{D}^T\mathbf{D})^{-1} \mathbf{D}^T = \mathbf{U}(\mathbf{U}^T \mathbf{U})^{-1} \mathbf{U}^T$. 
Now, remember from the relation between $\mathbf{A}$ and $\mathbf{D}$ that $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$. 

From this it is possible to obtain $\mathbf{D}$ and then $\mathbf{A}$, such that $\mathbf{D}$ span $\mathcal{R}(\mathbf{U})$ and $\mathbf{d}_i = \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)$. 
This is specified by the following optimisation problem \cite{Balkan2015}
\begin{align}
\min_{\left\{\mathbf{a}_i\right\}_{i = 1}^{N}}\Vert  \mathbf{D}(\mathbf{D}^T \mathbf{D})^{-1} \mathbf{D}^T &- \mathbf{U}(\mathbf{U}^T \mathbf{U})^{-1}\mathbf{U}^T \Vert_{F}^{2} \nonumber \\
\text{s.t.} \ \mathbf{d}_i &= \text{vec}(\mathbf{a}_i \mathbf{a}_i^T)\label{eq:Cov_DL2}
\end{align}      
where $\mathbf{U}$ results from PCA performed on $\textbf{Y}_{\text{Cov}}$.
From the source \cite{Balkan2015} proposing the method it is only notified that that optimization problem \eqref{eq:Cov_DL2} is minimised by quasi-Newton optimization methods. Hence the exact minimization approach can not be depicted.    
In the following section the optimization problem is analysed and processed in order to determine a suitable solution method. 
 
\subsection{Solution to Optimization Problem}
The optimization problem \eqref{eq:Cov_DL2} consists of an objective function forming a least-square problem with respect to the Frobenius norm. It is given that the squared norm, both the Euclidean and the Frobenius norm, are strictly convex \cite[p.173]{norm_optimization}.
Thus the objective function of \eqref{eq:Cov_DL2} is assumed to be convex.
The constraints in \eqref{eq:Cov_DL2} is a set of quadratic equality constraints. 
This categorize the optimization problem as a quadratically constraint quadratic program. 
However, the constraint is not necessarily convex.  
By the constraints not being considered convex the optimization problem does not meet the requirements of a convex optimization problem. Hence the numerical solution methods for convex optimization problems, for which convergence is ensured, does not apply directly. In fact a non-convex quadratically constraint quadratic program in know to be a NP-hard problem\cite{qcqp}. Thus some sort of relaxation is preferred.  

Due to the nature of the constraints it should be possible to reformulate the objective function to include the constraints into the objective function. That is constructing an unconstrained least-squares problem, which is a special subclass of convex optimization \cite{cvxbook}.

Let $\textbf{D} = f(\textbf{a}_1, \hdots, \textbf{a}_N)$, where $f(\textbf{a}_1, \hdots, \textbf{a}_N) = \left\{\textbf{d}_j = \text{vec}(\textbf{a}_j\textbf{a}_j^T) \right\}_{j=1}^{N} $. Then an optimization problem without constraints is achieved and it can be solved by use of basic gradient methods, for instance the Newton method. In order to avoid an explicit expression of the inverse Hessian, which is used in the Newton method, quasi-Newton methods can be considered\cite{Optimization2007}.  
The general idea of quasi-Newton methods is to let the direction of search be based on a positive definite matrix which is generated from available data, with the purpose of estimating the Hessian. 
%Chosen for this thesis is the Broyden-Fletcher-Goldfarb-Shanno method.

Rendering of general optimization theory plus the theory of quasi-Newton methods is omitted in this thesis and the reader is referred to source \cite{Optimization2007}.
For the implementation of Cov-DL in chapter \ref{ch:implementation} a predefined optimization module, using a quasi-Newton method, will be applied.   
   

%To make the constraint convex and linear the constraints are rewritten with respect to the assumption that $\textbf{a}_i\textbf{a}_i^{T} = \textbf{A}_i$. This results in the following constraints 
%\begin{align}
%\textbf{d}_i &= \text{vec}(\textbf{A}_i) \\
%\textbf{A}_i &\geq  0 \\
%\text{rank}(\textbf{A}_i) &= 1 \ \text{altid rank 1 når ydre produkt}
%\end{align}          
%by this a set of (hopefully)convex and linear constraints are achieved, both equality and inequality constraints.
%Now a classic quadratic programming problem is achieved      for which effective solution methods exist.   