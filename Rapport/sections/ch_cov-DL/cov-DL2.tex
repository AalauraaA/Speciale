\subsection{Over-determined System}\label{sec:over_det}
Consider again the measurements represented in the covariance domain \eqref{eq:cov1}.
In the case of $\widetilde{M} > N$ an over-determined system is achieved where $\textbf{D}$ is high and thin. In general such a system is inconsistent. Thus it is not possible to find $\textbf{D}$ by traditional dictionary learning method	s and different methods most be considered.

When $ \widetilde{M} > N $ it is expected from model \eqref{eq:cov1} that the transformed measurements $\text{vec}\left( \mathbf{Y}_{s'} \mathbf{Y}_{s'}^T \right)$ will live on or near a subspace of dimension $N$. 
This subspace is spanned by the columns of $\textbf{D}$, and is denoted as $\mathcal{R}(\textbf{D})$. 
To learn $\mathcal{R}(\textbf{D})$ without having to impose any sparsity constraint on $\boldsymbol{\delta}$ it is possible to use Principal Component Analysis (PCA). The basic theory of PCA in found in appendix \ref{app_sec:PCA}. 

PCA is applied to the set of training samples $\textbf{Z} = \left\{\text{vec}\left( \mathbf{Y}_{s'} \mathbf{Y}_{s'}^T \right)) : s' = 1, \hdots, L_{s'} \right\}$ and the $N$ first principal components are determined. The principal components form a set of basis vectors $\textbf{U}=[u_1,\hdots, u_N$. 


Each principal component is defined by $\textbf{u}_i = \textbf{Z}\textbf{p}_i$ where $\textbf{p}_i$ is the $i'th$ eigenvector of the correlation matrix $\boldsymbol{\Sigma}_{\textbf{Z}}$. 

  
 such that $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$. 
However, this does not imply that $\textbf{D}=\textbf{U}$. 
In the case of two sets of basis vectors spanning the same space, namely $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$, the projection operator of the given subset must be unique \todo{need prove here? or is there just one $P:V\rightarrow V $ where $V = \mathcal{R}(\textbf{U})= \mathcal{R}(\textbf{D})$} 
The projection matrix $\text{P}:\mathcal{R}(\textbf{U})\rightarrow \mathcal{R}(\textbf{D})$($\textbf{U}$?) can be find by considering the projection of an vector $\textbf{c}\in \mathbb{R}^{\widetilde{M}}$ onto $\mathcal{R}(\textbf{U})$, that is solving the least squares problem $\| \textbf{Ax}-\textbf{c}\|^{2}$ where $\textbf{Ax}\in \mathcal{R}(\textbf{U})$ and $\text{P} = \textbf{Ax}$. When $\textbf{A}$ has full rank the solution is given by the normal equation
\begin{align*}
\textbf{A}^T\textbf{A}\textbf{x} &= \textbf{A}^T\textbf{c}\\
\textbf{x} &= (\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T\textbf{c}
\end{align*}	
resulting in
\begin{align*}
\text{P}\textbf{c} = \textbf{Ax} &= \textbf{A}(\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T\textbf{c}\\
\text{P} &= \textbf{A}(\textbf{A}^T\textbf{A})^{-1}\textbf{A}^T 
\end{align*} 
Thus $\mathcal{R}(\textbf{U})$ and $\mathcal{R}(\textbf{D})$ having the same projection matrix is true if and only if $\textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T=\textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T$. 
Now, remember from the relation between $\textbf{A}$ and $\textbf{D}$ that $\textbf{d}_i = \text{vec}(\textbf{a}_i\textbf{a}_i^T)$. 
From this it is possible to obtain $\textbf{A}$ through the following optimisation problem 
\begin{align}
\min_{\left\{\textbf{a}_i\right\}_{i = 1}^{N}}\Vert  \textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T &- \textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T \Vert_{F}^{2} \nonumber \\
\text{s.t.} \ \textbf{d}_i&=\text{vec}(\textbf{a}_i\textbf{a}_i^T)\label{eq:Cov_DL2}
\end{align}      
where $\textbf{U}$ results from PCA performed on $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$.
In the following section the optimization problem is analysed and processed in order to determine a suitable solution method \todo{Vi skal have en kilde på dette sidste afsnit - L}. 
%Additional optimization theory to support the analysis is found in appendix \ref{app:optimazion}  

\subsection{Solution to Optimization Problem}
The optimization problem \eqref{eq:Cov_DL2} consists of an objective function forming a least-square problem with respect to the Frobenius norm.
That is a convex quadratic objective function \todo{Jan spørger om hvordan vi ser dett? - L}.
The constraints is a set of quadratic equality constraints. In general it is a rule of thumb that non-linear equality constraint are not convex \todo{Kilde på dette udsagn -L}.
Due to the constraints not being considered convex the optimization problem does not meet the requirements of a convex optimization problem. Hence the numerical solution methods for convex optimization problems, for which convergence is ensured, does not imply directly. 

Due to the nature of the constraints it should be possible to reformulate the objective function to include the constraints into the objective function. That is construction an unconstrained least-squares problem, which is a special subclass of convex optimization \cite{cvxbook}.

Let $\textbf{D} = f(\textbf{a}_1, \hdots, \textbf{a}_N)$ then an optimization problem without constraints is achieved and it can be solved by use of for instance the gradient descent method.
\todo[inline]{description of the exact solution method is missing, vi skal have slået den nye solver op, det skulle være en quasi newton methode, her skal bare en kort beskrivelse}

   

%To make the constraint convex and linear the constraints are rewritten with respect to the assumption that $\textbf{a}_i\textbf{a}_i^{T} = \textbf{A}_i$. This results in the following constraints 
%\begin{align}
%\textbf{d}_i &= \text{vec}(\textbf{A}_i) \\
%\textbf{A}_i &\geq  0 \\
%\text{rank}(\textbf{A}_i) &= 1 \ \text{altid rank 1 når ydre produkt}
%\end{align}          
%by this a set of (hopefully)convex and linear constraints are achieved, both equality and inequality constraints.
%Now a classic quadratic programming problem is achieved      for which effective solution methods exist.   