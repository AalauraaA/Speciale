\subsection{Over-determined system}
Consider again the measurements represented in the covariance domain \eqref{eq:cov1}.
In the case of $N < \widetilde{M}$ an over-determined system is achieved where $\textbf{D}$ is high and thin. In general such system is inconsistent\todo{tjek, har vi tidligerer nævnt at der teoretisk godt kan være en løsning?}. Thus it is not possible to find $\textbf{D}$ by traditionally dictionary learning methods and different methods most be considered.

When $N < \widetilde{M}$ it is certain that the tranformed measurements $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$ will live on or near a subspace of dimension $N$. 
This subspace is spanned by the columns of $\textbf{D}$, and is denoted as $\mathcal{R}(\textbf{D})$. 
To learn $\mathcal{R}(\textbf{D})$ without having to impose any sparsity constraint on $\boldsymbol{\delta}_s$ it is possible to use Principal Component Analysis(PCA)\todo{evt. teoretisk beskrivelse af PCA i appendix?}. 
By use of PCA a set of basis vectors $\textbf{U}$ is achieved such that $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$. 
This however do not imply that $\textbf{D}=\textbf{U}$. 
In the case of two sets of basis vectors span the same space, namely $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$, the projection operator of the given subset must be unique. 
Which is true if and only if $\textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T=\textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T$\todo{kilde foruden phd p. 51?}. 
Remember from the above derivation the condition that $\textbf{d}_i = \text{vec}(\textbf{a}_i\textbf{a}_i^T)$. 
From this it is possible to obtain $\textbf{A}$ through the optimisation problem 
\begin{align}
\min_{\textbf{a}_i}\Vert  \textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T &- \textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T \Vert_{F}^{2} \nonumber \\
\text{s.t.} \ \textbf{d}_i&=\text{vec}(\textbf{a}_i\textbf{a}_i^T)\label{eq:Cov_DL2}
\end{align}      
where $\textbf{U}$ is learned by use of PCA performed on $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$.
In the following section the optimization problem is analysed and processed in order to determine a suitable method to solve the problem. Additional optimization theory to support the analysis is found in appendix \ref{app:optimazion}  

\subsection{Solution to optimization problem}
The optimization problem \eqref{eq:Cov_DL2} consist of an objective function forming a least-square problem with respect to the frobenius norm.
That is a convex quadratic objective function.
The constraints is a set of quadratic equality constraints. In general it is a thumb rule that non-linear equality constraint are not convex...
To make the constraint convex and linear the constraints are rewritten with respect to the assumption that $\textbf{a}_i\textbf{a}_i^{T} = \textbf{A}_i$. This results in the following constraints 
\begin{align}
\textbf{d}_i &= \text{vec}(\textbf{A}_i) \\
\textbf{A}_i &\geq  0 \\
\text{rank}(\textbf{A}_i) &= 1 \ \text{altid rank 1 når ydre produkt}
\end{align}          
by this a set of (hopefully)convex and linear constraints are achieved, both equality and inequality constraints.
Now a classic quadratic programming problem is achieved      for which effective solution methods exist.   