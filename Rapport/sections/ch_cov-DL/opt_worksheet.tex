\chapter*{Optimization Worksheet}
\section{Content to the chapter}
\subsection{Over-determined \textbf{D}}


\section{Theory for appendix}
\begin{itemize}
\item notes for driving optimization problem\\
We have the matrix of measurements $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})\in \mathbb{R}^{\frac{m(m+1)}{2}} \forall s$. from the model we know that the matrix $\textbf{D}$ makes a basis of the measurements vectors, denoted $\mathcal{R}(\textbf{D})$, that is a basis of dimension N, provided that $N < \frac{M(M+1)}{2}$ which is the case where the system is over-determined(more equation than variables, which has no solution. hence no ordinary dictionary learning). 
we want to learn D, but first we can estimate $\mathcal{R}(\textbf{D})$. 
we know we can do that by PCA of the set of all $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$. 
But why:\\
PCA is a method for dimensionality reduction. \\
we represent the measurements by its first $N$ principal components. 
each found component is linear a combination of the measurements, and has the highest possible variance constraint by being ortogonal to the previously found components. the found components makes an uncorrelated orthogonal basis set $\textbf{U}$. 
thus  $\mathcal{R}(\textbf{D}) = \mathcal{R}(\textbf{U})$ must be true. however this does not imply that $\textbf{D}=\textbf{U}$. 
$j$ principal components is the first $j$ eigenvectors(can be shown) of the covariance matrix, from the $L$ samples it is possible to determined the sample-mean(to be subtracted from the samples) and the covariance matrix(from which to determine the eigenvalues).[p. 125, ICA book]
note that it is important to perform the PCA on the found segments where the covariance matrix is stationary. on-line/real time PCA is discussed on page 132[ICA book].	       


\item convex quadratic objective function
\item convex solution set/ feasible set 

\end{itemize}
\section{Notes}
noter fra møde:
\begin{itemize}
\item augmented Lagrangian
\item semidefinite relaxzation 
\end{itemize}
noter:
\begin{itemize}
\item any matrix $\textbf{A}\in \mathbb{R}^{M\times N}$ is called \textbf{over complete} if one or more columns is a linear combination of the others -- this must be the case when $N>M$ which is also an \textbf{under-determined} system (så det der står i rapporten er rigtig dog skal vi nok ikke kalde D-underdetermined, men systemet med D.)
\end{itemize}
noter til optimeringsproblemet: 
\begin{itemize}
\item we are dealing with an constraint optimization problem
\item when constraints are included it means that we not necessarily can search for the solution along the direction of the negative gradient, but must use methods to determine the feasible search directions given the constraints.
\item an important concept within constrained optimization is lagrange multipliers. 
\item furthermore, we have the 1. order necessary condition (KKT) for $x*$ to be a solution to a constraint problem. and the 2. order conditions.[PO]
\item we have a set of equality constraints. for a vector to be a regular point(possible solution?)to the vector has to be a solution to the constraint and the jacobian of the constraints. Jacobian matrisen er $p \times n $ hvor $p<n$ skal gælde.[PO] 
\item our equality constrints are quadratic and non-convex - according to Jan. 
\item why non-convex? \\
a solution set is convex when you can draw a line between any solution and remain inside the solution set. 
\item why is our quadratic objective convex?\\
is the objective has a positive semi definite Hessian matrix then is can be viewed as a convex programming problem. \\
 
\end{itemize}

\section{Principal Component Analysis}
