\chapter*{Optimization Worksheet}
\section{Content to the chapter}
\subsection{Over-determined \textbf{D}}
In the case of $N < \frac{M(M+1)}{2}$ an over-determined system is achieved and it is not possible to find $\textbf{D}$ by dictionary learning methods.
By assuming that $\frac{M(M+1)}{2}$ will be close to $N$, because $N > M$ is given,\todo{how else can they live on the same space??} then the measurements in the covariance domain $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$ will live on or near a subspace of dimension $N$. 
This subspace is spanned by the columns of $\textbf{D}$, and is denoted as $\mathcal{R}(\textbf{D})$. 
To learn $\mathcal{R}(\textbf{D})$ without having to impose any sparsity constraint on $\boldsymbol{\delta}_s$ it is possible to use Principal Component Analysis(PCA)\todo{evt. teoretisk beskrivelse af PCA i appendix?}. 
By use of PCA a set of basis vectors $\textbf{U}$ is achieved such that $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$. 
This however do not imply that $\textbf{D}=\textbf{U}$. 
In the case of two sets of basis vectors span the same space, namely $\mathcal{R}(\textbf{U})=\mathcal{R}(\textbf{D})$, the projection operator of the given subset must be unique. 
Which is true if and only if $\textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T=\textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T$\todo{kilde foruden phd p. 51?}. 
Remember from the above derivation the condition that $\textbf{d}_i = \text{vec}(\textbf{a}_i\textbf{a}_i^T)$. 
From this it is possible to obtain $\textbf{A}$ through the optimisation problem 
\begin{align}
\min_{\textbf{a}_i}\Vert  \textbf{D}(\textbf{D}^T\textbf{D})^{-1}\textbf{D}^T &- \textbf{U}(\textbf{U}^T\textbf{U})^{-1}\textbf{U}^T \Vert_{F}^{2} \nonumber \\
\text{s.t.} \ \textbf{d}_i&=\text{vec}(\textbf{a}_i\textbf{a}_i^T)\label{eq:Cov_DL2}
\end{align}      
where $\textbf{U}$ is learned by use of PCA performed on $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$.
In the following section the optimization problem is analysed and processed in order to determine a suitable method to solve the problem. Additional optimization theory to support the analysis is found in appendix \ref{app:optimazion}  

\subsection{Solution to optimization problem}
The optimization problem \eqref{eq:Cov_DL2} consist of an objective function forming a least-square problem with respect to the frobenius norm.
That is a convex quadratic objective function.
The constraints is a set of quadratic equality constraints. In general it is a thumb rule that non-linear equality constraint are not convex...
To make the constraint convex and linear the constraints are rewritten with respect to the assumption that $\textbf{a}_i\textbf{a}_i^{T} = \textbf{A}_i$. This results in the following constraints 
\begin{align}
\textbf{d}_i &= \text{vec}(\textbf{A}_i) \\
\textbf{A}_i &\geq  0 \\
\text{rank}(\textbf{A}_i) &= 1 \ \text{altid rank 1 når ydre produkt}
\end{align}          
by this a set of (hopefully)convex and linear constraints are achieved, both equality and inequality constraints.
Now a classic quadratic programming problem is achieved      for which effective solution methods exist.   

\section{Theory for appendix}
\begin{itemize}
\item notes for driving optimization problem\\
We have the matrix of measurements $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})\in \mathbb{R}^{\frac{m(m+1)}{2}} \forall s$. from the model we know that the matrix $\textbf{D}$ makes a basis of the measurements vectors, denoted $\mathcal{R}(\textbf{D})$, that is a basis of dimension N, provided that $N < \frac{M(M+1)}{2}$ which is the case where the system is over-determined(more equation than variables, which has no solution. hence no ordinary dictionary learning). 
we want to learn D, but first we can estimate $\mathcal{R}(\textbf{D})$. 
we know we can do that by PCA of the set of all $\text{vec}(\widehat{\boldsymbol{\Sigma}}_{\textbf{Y}_s})$. 
But why:\\
PCA is a method for dimensionality reduction. \\
we represent the measurements by its first $N$ principal components. 
each found component is linear a combination of the measurements, and has the highest possible variance constraint by being ortogonal to the previously found components. the found components makes an uncorrelated orthogonal basis set $\textbf{U}$. 
thus  $\mathcal{R}(\textbf{D}) = \mathcal{R}(\textbf{U})$ must be true. however this does not imply that $\textbf{D}=\textbf{U}$. 
$j$ principal components is the first $j$ eigenvectors(can be shown) of the covariance matrix, from the $L$ samples it is possible to determined the sample-mean(to be subtracted from the samples) and the covariance matrix(from which to determine the eigenvalues).[p. 125, ICA book]
note that it is important to perform the PCA on the found segments where the covariance matrix is stationary. on-line/real time PCA is discussed on page 132[ICA book].	 
 

      



\item convex quadratic objective function
\item convex solution set/ feasible set 

\end{itemize}
\section{Notes}
noter fra møde:
\begin{itemize}
\item augmented Lagrangian
\item semidefinite relaxzation 
\end{itemize}
noter:
\begin{itemize}
\item any matrix $\textbf{A}\in \mathbb{R}^{M\times N}$ is called \textbf{over complete} if one or more columns is a linear combination of the others -- this must be the case when $N>M$ which is also an \textbf{under-determined} system (så det der står i rapporten er rigtig dog skal vi nok ikke kalde D-underdetermined, men systemet med D.)
\end{itemize}
noter til optimeringsproblemet: 
\begin{itemize}
\item we are dealing with an constraint optimization problem
\item when constraints are included it means that we not necessarily can search for the solution along the direction of the negative gradient, but must use methods to determine the feasible search directions given the constraints.
\item an important concept within constrained optimization is lagrange multipliers. 
\item furthermore, we have the 1. order necessary condition (KKT) for $x*$ to be a solution to a constraint problem. and the 2. order conditions.[PO]
\item we have a set of equality constraints. for a vector to be a regular point(possible solution?)to the vector has to be a solution to the constraint and the jacobian of the constraints. Jacobian matrisen er $p \times n $ hvor $p<n$ skal gælde.[PO] 
\item our equality constrints are quadratic and non-convex - according to Jan. 
\item why non-convex? \\
a solution set is convex when you can draw a line between any solution and remain inside the solution set. 
\item why is our quadratic objective convex?\\
is the objective has a positive semi definite Hessian matrix then is can be viewed as a convex programming problem. \\
 
\end{itemize}