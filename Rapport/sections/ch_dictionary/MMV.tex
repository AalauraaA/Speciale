As clarified in section \ref{sec:dic_conditions} the estimation of the dictionary matrix $\mathbf{A}$ is essential to achieve the best recovery of the sparse signal $\mathbf{x}$ from the measurements $\mathbf{y}$. Pre-constructed dictionaries do exist which in many cases results in simple and fast algorithms for reconstruction of $\mathbf{x}$ \cite{Elad_book}. Pre-constructed dictionaries are typically fitted to a specific kind of data. For instance the discrete Fourier transform or the discrete wavelet transform are used especially for sparse representation of images \cite{Elad_book}. Hence the results of using such dictionaries depend on how well they fit the data of interest, which is creating a certain limitation. An alternative is to consider an adaptive dictionary based on a set of training data that resembles the data of interest. For this purpose learning methods are considered to empirically construct a fixed dictionary which can take part in the application. Different dictionary learning algorithms exist. One is the K-SVD which is to be elaborated in this chapter. The K-SVD algorithm was presented in 2006 by Elad et al. and found to outperform pre-constructed dictionaries when computational cost is of secondary interest \cite{Elad2006}. Before the K-SVD algorithm can be investigated the linear model \eqref{eq:SMV_model} must be expanded to be considering sets of training data.

\section{Multiple Measurement Vector Model}\label{sec:MMV}
The linear model \eqref{eq:SMV_model} is also referred to as a single measurement vector (SMV) model. In order to adapt the model \eqref{eq:SMV_model} to a practical use the model is expanded to include multiple measurement vectors and take noise into account. A multiple measurement vector (MMV) model consists of the observed measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$, the dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and the noise vector $\textbf{E} \in \mathbb{R}^{M \times L}$:
\begin{align}\label{eq:MMV_model}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align}
$L$ denotes the number of observed measurement vectors each consisting of $M$ measurements, that is $L$ samples are given. For $L = 1$ the linear model will just be the SMV model \eqref{eq:SMV_model}. The matrix $\mathbf{X}$ consists of $k$-sparse vectors $\mathbf{x}_1, \dots, \mathbf{x}_L$ which have been stacked column-wise such that $\mathbf{X}$ consist of at most $k$ non-zero rows. As for the SMV model \eqref{eq:SMV_model} the MMV model \eqref{eq:MMV_model} is under-determined with $M \ll N$ and $k < M$ \cite[p. 42]{CS}.
\\ \\
The support of $\mathbf{X}$ denotes the index set of non-zero rows of $\mathbf{X}$ and $\mathbf{X}$ is said to be row-sparse. As the columns in $\mathbf{X}$ are $k$-sparse and as mentioned before, $\mathbf{X}$ has at most $k$ non-zero rows, the non-zero values occur in common location for all columns. By using this joint information it is possible to recover $\mathbf{X}$ from fewer measurements \cite[p. 43]{CS}.

%\\
%By using the rank of $\mathbf{X}$, which give us information of the amount of linearly independent rows or columns, and the spark of $\mathbf{A}$ which is the minimum set of linearly dependent columns, it is possible to set some conditions on the system to ensure recovery.
%\\
%When $\vert \text{supp}(\mathbf{X})\vert = k$ then $\text{rank}(\mathbf{X}) \leq k$. If rank$(\mathbf{X}) = 1$ then are the $k$-sparse vectors $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ multiples of each other and the joint information can not be taken advantage of. But for large rank it is possible to exploit the diversity of the columns in $\mathbf{X}$. This can be defined as a sufficient and necessary condition of the MMV model \eqref{eq:MMV_model}. MMV system $\textbf{Y}=\textbf{AX}$ must have
%\begin{align*}
%\vert \text{supp}(\mathbf{X}) \vert < \frac{\text{Spark} (\mathbf{A}) - 1 + \text{rank}(\mathbf{X})}{2}
%\end{align*}
%such that $\mathbf{X}$ can uniquely be determined.
%\\
%This result says that a row-sparse matrix $\mathbf{X}$ with large rank can be recovered from fewer\todo{Skal "fewer" udspecificeres yderligere?} measurement \cite[p. 43]{CS}.

%\subsection{Dictionary learning}\label{sec:dictionarylearning}
%In cases where the dictionary is unknown it is possible to learn the dictionary from the observed measurement provided that several observations are available $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$. 
%In dictionary learning framework the inverse problem is defined as
%\begin{align*}
%\min_{\mathbf{A,X}} = \frac{1}{2} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_F^2 + \gamma \sum_{i=1}^{L} g(\mathbf{x}_i),
%\end{align*}
%where the function $g(\cdot)$ promotes sparsity of the source vectors at sample $i$ \cite[p. 4]{phd}. $\Vert \cdot \Vert_F$ is the Frobenius norm which is a vector norm defined as 
%\begin{align*}
%\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
%\end{align*}
%With the MMV model defined as a optimisation problem different dictionary learning algorithm can be used to learn the dictionary $\mathbf{A}$. One of them is the K-SVD algorithm.

\section{K-SVD}\label{sec:dictionarylearning}
Consider $\mathbf{Y} = \left[ \mathbf{y}_1, \dots, \mathbf{y}_L \right]$, $\mathbf{y}_j \in \mathbb{R}^M$ as a training database, created by $\mathbf{y}_j = \mathbf{A}\mathbf{x}_j$ for which one want to learn the best suitable dictionary $\mathbf{A}$ and sparse representation $\mathbf{X} = \left[ \mathbf{x}_1, \dots, \mathbf{x}_L  \right]$, $\mathbf{x}_j \in \mathbb{R}^N$. For a known sparsity constraint $k$ this can be defined by an optimisation problem similar to the general compressive sensing problem of multiple measurements \cite{Elad_book}
\begin{align}\label{eq:SVD1}
\min_{\mathbf{A,X}} \sum_{j=1}^{L} \Vert \mathbf{y}_j - \mathbf{Ax}_j \Vert_2^2 \quad \text{subject to} \quad \Vert \textbf{x}_j \Vert_0 \leq k, \ 1 \leq j \leq L.
\end{align}  
The learning consists of jointly solving the optimization problem on $\mathbf{X}$ and $\mathbf{A}$. The uniqueness of $\mathbf{A}$ depends on the recovery sparsity condition. As clarified earlier recovery is only possible if $k < M$ \cite{phd2015}. 
%Furthermore, consider $\textbf{A}_0$ as a initial dictionary matrix such that every training signal can be represented by $k_0 < \text{spark}(\textbf{A}_0)/2$ columns of $\textbf{A}_0$, then $\textbf{A}_0$ is a unique dictionary, up to scaling and permutation of columns\cite{Elad_book}\todo{fungerer disse to uniqueness parameter sammen?}. Again the $\ell_0$-norm lead to an NP-hard problem an heuristic methods are need.     

\subsection{K-SVD Algorithm}
The dictionary learning algorithm K-SVD provides an updating rule which is applied to each column of $\mathbf{A}_0 = \left[ \mathbf{a}_0, \dots, \mathbf{a}_N \right] $ where $\mathbf{A}_0$ being a random initial dictionary matrix. Updating first $\mathbf{a}_j$ and then the corresponding coefficients in $\mathbf{X}$ which it is multiplied with the $i$-th row in $\mathbf{X}$ denoted by $\mathbf{x}_{i \bullet}$.
\\
Let $\mathbf{a}_{j_{0}}$ be the column to be updated and let the remaining columns be fixed. By rewriting the objective function in \eqref{eq:SVD1} using matrix notation it is possible to isolate the contribution from $\mathbf{a}_{j_{0}}$ \todo{Tjek nedenstående udledning. a og x er ikke lige lange da $a_j$ er M lang mens $x_{i \bullet}$ er L lang}.
\begin{align}\label{eq:SVD2} 
\Vert \textbf{Y} - \textbf{AX} \Vert_{F}^{2} 
&= \left\| \textbf{Y} - \sum_{j=1}^{N} \textbf{a}_j \textbf{x}_{i \bullet} \right\|_{F}^{2} \nonumber \\
&= \left\| \left( \textbf{Y}- \sum_{j \neq j_0}^{N} \textbf{a}_j \textbf{x}_{i \bullet} \right) - \textbf{a}_{j_{0}} \textbf{x}_{i_0 \bullet} \right\| _{F}^{2},
\end{align}
where $i = j$, $i_0 = j_0$ and where $F$ is the Frobenius norm that works on matrices
\begin{align*}
\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{ij} \vert^2}.
\end{align*} 
In \eqref{eq:SVD2} the term in the parenthesis is denoted by $\textbf{E}_{j_0}$, an error matrix, and hence by minimising \eqref{eq:SVD2} with respect to $\mathbf{a}_{j_0}$ and $\mathbf{x}_{i_0 \bullet}$ leads to the optimal contribution from $j_0$
%In \eqref{eq:SVD2} the term in the parenthesis makes the an error matrix $\textbf{E}_{i_0}$ without the contribution from $i_{0}$, hence minimising \eqref{eq:SVD2} with respect to $\textbf{a}_{i_{0}}$ and $\textbf{x}_{i_{0}}^{T}$ leads to the optimal contribution from $i_{0}$ (can I say it this way..?). 
\begin{align}\label{eq:SVD3}
\min_{\textbf{a}_{j_{0}}, \textbf{x}_{i_0 \bullet}}\left\|\textbf{ E}_{j_{0}} - \textbf{a}_{j_{0}} \textbf{x}_{i_0 \bullet} \right\|_{F}^{2}.
\end{align} 
The optimal solution to \eqref{eq:SVD3} is known to be the rank-1 approximation of $\textbf{E}_{j_{0}}$. This comes from the Eckart–Young–Mirsky theorem \cite{?} saying that a partial single value decomposition (SVD) makes the best low-rank approximation of a matrix such as $\textbf{E}_{j_0}$. The SVD is given as
\begin{align*}
\mathbf{E}_{j_0} = \textbf{U} \boldsymbol{\Sigma} \textbf{V}^T \in \mathbb{R}^{M\times N},
\end{align*}
with $\textbf{U} \in \mathbb{R}^{M\times M}$ and $\textbf{V} \in \mathbb{R}^{N\times N}$ being unitary matrices\footnote{Unitary matrix: $\textbf{U}^T\textbf{U}=\textbf{UU}^T=\textbf{I}$} and $\boldsymbol{\Sigma} = \text{diag}\left[\boldsymbol{\sigma}_1, \dots, \boldsymbol{\sigma}_M \right] \in \mathbb{R}^{M \times N}$ a diagonal matrix. $\boldsymbol{\sigma}_j$ are the non-negative singular values of $\textbf{E}_{j_0}$. 
%such that $\sigma_1 \geq \sigma_2 \geq \hdots \geq 0$. 
The best $k$-rank approximation to $\textbf{E}_{j_0}$, with $k < \text{rank}(\textbf{E}_{j_0})$ is then given by \todo{kilde}: 
\begin{align*}
\textbf{E}_{j_{0}}^{(k)} = \sum_{j=1}^{k} \boldsymbol{\sigma}_j \textbf{u}_{j} \textbf{v}_{j}^T.
\end{align*} 
Since the outer product always have rank-1 letting $\textbf{a}_{j_0} = \textbf{u}_1$ and $\textbf{x}_{i_0 \bullet} = \boldsymbol{\sigma}_{j} \textbf{v}_{1}^T$ solves the optimisation problem \eqref{eq:SVD3}.
However in order to preserve the sparsity in $\textbf{X}$ while optimising, only the non-zero entries in $\textbf{x}_{i_0 \bullet}$ are allowed to vary. For this purpose only a subset of columns in $\textbf{E}_{j_0}$ is considered, those which correspond to the non-zero entries of $\textbf{x}_{i_0 \bullet}$. A matrix $\textbf{P}_{i_0}$ is defined to restrict $\textbf{x}_{i_0 \bullet}$ to only contain the non-zero-rows corresponding to $N_{j_0}$ non-zero rows:
\begin{align*}
\textbf{x}_{i_0 \bullet}^{(R)} = \textbf{x}_{i_0 \bullet} \textbf{P}_{i_0}
\end{align*}
where $R$ denoted the restriction. By applying the SVD to the error matrix which has been restricted $\textbf{E}_{j_0}^{(R)} = \textbf{E}_{j_0} \textbf{P}_{i_0}$ and updating $\textbf{a}_{j_0}$ and $\textbf{x}_{i_0 \bullet}^{(R)}$ the rank-1 approximation is found and the original representation vector is updated as $\textbf{x}_{i_0 \bullet} = \textbf{x}_{i_0 \bullet}^{(R)} \textbf{P}_{i_0}^{T}$.  
\\ \\
The main steps of K-SVD is described in algorithm \ref{alg:K_SVD}. 
\begin{algorithm}[H]
\caption{K-SVD}
\begin{algorithmic}[1]
			\State$k = 0$			
			\State$\text{Initialize random} \quad  \textbf{A}_{(0)}$            
			\State$\text{Initialize} \quad \textbf{X}_{(0)}=\mathbf{0}$
			\State
            \Procedure{K-SVD}{$\textbf{A}_{(0)}$}    
            \State$\text{Normalize columns of} \ \textbf{A}_{(0)}$
            \While{$\text{error} \geq \text{limit}$} 
                \State $j = j+1$
                \For{$j \gets 1,2,\dots, L$} \Comment{updating each col. in $\textbf{X}_{(k)}$}
                	\State$\hat{\textbf{x}}_{j} = \min_{\textbf{x}} \|\textbf{y}_j -\textbf{A}_{(k-1)}\textbf{x}_{j}\| \quad \text{subject to} \quad \|\textbf{x}_{j}\| \leq k $ \Comment{use Basis Pursuit}
				\EndFor
				\State$\textbf{X}_{(k)} = \lbrace \hat{\textbf{x}}_{j} \rbrace_{j=1}^{L}$
				\For{$j_0 \gets 1, 2, \hdots, N$}
					\State$\Omega_{j_0} = \lbrace j \ \vert \ 1 \leq j \leq L, \textbf{X}_{(k)} [j_0, j]\neq 0\rbrace$
					\State$\text{From} \ \Omega_{j_0} \ \text{define} \ \textbf{P}_{i_0} $
					\State$\textbf{E}_{j_0} =  \textbf{Y} - \sum_{j \neq j_{0}}^{N} \textbf{a}_j \textbf{x}_{i \bullet}$
					\State$\textbf{E}_{j_0}^{(R)} =  \textbf{E}_{j_0} \textbf{P}_{i_0}$
					\State$\textbf{E}_{j_0}^{(R)} =\textbf{U} \boldsymbol{\Sigma} \textbf{V}^T$ \Comment{perform SVD}
					\State$\textbf{a}_{j_0} \gets \textbf{u}_{1}$ \Comment{update the $j_0$ col. in $\textbf{A}_{(k)}$}
					\State$\left( \textbf{x}_{i_0 \bullet} \right)^{(R)} \gets \boldsymbol{\sigma}_{1} \textbf{v}_{1}$
					\State$\textbf{x}_{i_0 \bullet} \gets \left( \textbf{x}_{i_0 \bullet} \right)^{(R)} \textbf{P}_{i_0}^T $ \Comment{update the $i_0$ row in $\textbf{X}_{(k)}$}
				\EndFor
				\State$\text{error} = \Vert \textbf{Y} - \textbf{A}_{(k)} \textbf{X}_{(k)} \Vert_{F}^2 $
          		\EndWhile
            \EndProcedure
        \end{algorithmic} 
        \label{alg:K_SVD}
\end{algorithm}
The dictionary learning algorithm K-SVD is a generalisation of the well known K-means clustering also referred to as vector quantization. In K-means clustering a set of $K$ vectors is learned referred to as mean vectors. Each signal sample is then represented by its nearest mean vector. That corresponds to the case with sparsity constraint $k = 1$ and the representation reduced to a binary scalar $x = \lbrace 1, 0 \rbrace$. Further instead of computing the mean of $K$ subsets the K-SVD algorithm computes the SVD factorisation of the $K$ different sub-matrices that correspond to the $K$ columns of $\textbf{A}$.
