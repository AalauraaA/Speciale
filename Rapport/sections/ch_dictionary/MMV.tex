
\subsection{Multiple Measurement Vector Model}\label{sec:MMV}
The linear model \eqref{eq:SMV_model} is also referred to as a single measurement vector (SMV) model. In order to adapt the model \eqref{eq:SMV_model} to a practical use the model is expanded to include multiple measurement vectors and take noise into account.
\\ \\
A multiple measurement vector (MMV) model consist of the observed measurement matrix $\mathbf{Y} \in \mathbb{R}^{M \times L}$, the source matrix $\mathbf{X} \in \mathbb{R}^{N \times L}$, the dictionary matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ and the noise vector $\textbf{E} \in \mathbb{R}^{M \times L}$:
\begin{align}\label{eq:MMV_model}
\mathbf{Y} = \mathbf{AX}+\textbf{E}.
\end{align}
$L$ denote the number of observed measurement vectors each consisting of $M$ measurements, that is $L$ samples is given. For $L = 1$ the linear model will just be the SMV model \eqref{eq:SMV_model}. 
\\
The matrix $\mathbf{X}$ consist of $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ $k$-sparse vectors which has been stacked column-wise such that $\mathbf{X}$ consist of at most $k$ non-zero rows. As for the SMV model \eqref{eq:SMV_model} the MMV model \eqref{eq:MMV_model} is under-determined with $M \ll N$ and $k < M$\cite[p. 42]{CS}.
\\ \\
The support of $\mathbf{X}$ denote the index set of non-zero rows of $\mathbf{X}$ and $\mathbf{X}$ is said to be row-sparse. As the columns in $\mathbf{X}$ are $k$-sparse and as mention before $\mathbf{X}$ has at most $k$ non-zero rows, the non-zero values occur in common location for all columns. By using this joint information it is possible to recover $\mathbf{X}$ from fewer measurements.
\\
By using the rank of $\mathbf{X}$, which give us information of the amount of linearly independent rows or columns, and the spark of $\mathbf{A}$ which is the minimum set of linearly dependent columns, it is possible to set some conditions on the system to ensure recovery.
\\
When $\vert \text{supp}(\mathbf{X})\vert = k$ then $\text{rank}(\mathbf{X}) \leq k$. If rank$(\mathbf{X}) = 1$ then are the $k$-sparse vectors $\lbrace \mathbf{x}_i \rbrace_{i=1}^L$ multiples of each other and the joint information can not be taken advantage of. But for large rank it is possible to exploit the diversity of the columns in $\mathbf{X}$. This can be defined as a sufficient and necessary condition of the MMV model \eqref{eq:MMV_model}. MMV system $\textbf{Y}=\textbf{AX}$ must have
\begin{align*}
\vert \text{supp}(\mathbf{X}) \vert < \frac{\text{Spark} (\mathbf{A}) - 1 + \text{rank}(\mathbf{X})}{2}
\end{align*}
such that $\mathbf{X}$ can uniquely be determined.
\\
This result says that a row-sparse matrix $\mathbf{X}$ with large rank can be recovered from fewer\todo{Skal "fewer" udspecificeres yderligere?} measurement \cite[p. 43]{CS}.

%\subsection{Dictionary learning}\label{sec:dictionarylearning}
%In cases where the dictionary is unknown it is possible to learn the dictionary from the observed measurement provided that several observations are available $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$. 
%In dictionary learning framework the inverse problem is defined as
%\begin{align*}
%\min_{\mathbf{A,X}} = \frac{1}{2} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_F^2 + \gamma \sum_{i=1}^{L} g(\mathbf{x}_i),
%\end{align*}
%where the function $g(\cdot)$ promotes sparsity of the source vectors at sample $i$ \cite[p. 4]{phd}. $\Vert \cdot \Vert_F$ is the Frobenius norm which is a vector norm defined as 
%\begin{align*}
%\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
%\end{align*}
%With the MMV model defined as a optimisation problem different dictionary learning algorithm can be used to learn the dictionary $\mathbf{A}$. One of them is the K-SVD algorithm.

\section{Dictionary learning}\label{sec:dictionarylearning}
As clarified in section \ref{sec:dic_conditions} the estimation of dictionary matrix $\textbf{A}$ is essential to achieve the best recovery of the sparse signal $\textbf{x}$ from the measurements $\textbf{y}$. Pre-constructed dictionaries do exist which in many cases results in simple and fast algorithms for reconstruction of $\textbf{x}$\cite{Elad_book}. Pre-constructed dictionaries are typically fitted to a specific kind of data, for instance the discrete Fourier transform or the discrete wavelet transform are used especially for sparse representation of images\cite{Elad_book}. Hence the results of using such dictionaries depend on how well they fit the data of interest, which is creating a certain limitation. An alternative is to consider an adaptive dictionary based on a set of training data that resembles the data of interest. For this purpose learning methods are considered to empirically construct a fixed dictionary which can take part in the application. Different dictionary learning algorithms exist, one is the K-SVD which is to be elaborated in this section. The K-SVD algorithm was presented in 2006 by Elad et al. and found to outperform pre-constructed dictionaries when computational cost is of secondary interest\cite{Elad2006}. \\
\\
Consider now $\textbf{Y}=\left[ \textbf{y}_1, \dots ,\textbf{y}_L \right]$, $\textbf{y}_i\in \mathbb{R}^{M}$ as a training database, created by $\textbf{y}_i=\textbf{A}\textbf{x}_i$ for which we want to learn the best suitable dictionary $\textbf{A}$ and sparse representation $\textbf{X}=\left[ \textbf{x}_1, \dots ,\textbf{x}_L \right]$, $\textbf{x}_i\in \mathbb{R}^{N}$. For a known sparsity constraint $k$ this can be defined by an optimisation problem similar to the general compressive sensing problem of multiple measurements \cite{Elad_book}
\begin{align}
\min_{\mathbf{A,X}} \sum_{i=1}^{L} \Vert \mathbf{y} _i - \mathbf{Ax}_i \Vert_2^2 \quad st. \ \Vert \textbf{x}_i\Vert_0\leq k, \ 1\leq i \leq L.\label{eq:SVD1}
\end{align}  
The learning consist of jointly solving the optimization problem on $\textbf{X}$ and $\textbf{A}$. The uniqueness of $\textbf{A}$ depends on the recovery sparsity condition. As clarified earlier recovery is only possible if $k < M$\cite{phd2015}. Furthermore, consider $\textbf{A}_0$ such that every training signal can be represented by $k_0 < \text{spark}(\textbf{A}_0)/2$ columns of $\textbf{A}_0$, then $\textbf{A}_0$ is a unique dictionary, up to scaling and permutation of columns\cite{Elad_book}\todo{fungerer disse to uniqueness parameter sammen?}. Again the $\ell_0$-norm lead to an NP-hard problem an heuristic methods are need.     

\subsection{K-SVD}
The dictionary learning algorithm K-SVD provide an update rule which is applied to each column of $\textbf{A}_0 = \left[ \textbf{a}_0, \hdots , \textbf{a}_N \right] $. Updating first $\textbf{a}_i$ and then the corresponding coefficients in $\textbf{X}$ which it is multiplied with, that is the $i^{\text{th}}$ row in $\textbf{X}$ denoted by $\textbf{x}_i^T$.\\
Let $\textbf{a}_{i_{0}}$ be the column to be updated and let the remaining columns be fixed. By rewriting the objective function in \eqref{eq:SVD1} using matrix notation it is possible to isolate the contribution from $\textbf{a}_{i_{0}}$.
\begin{align}
\Vert \textbf{Y} - \textbf{AX} \Vert_{F}^{2} 
&= \left\| \textbf{Y} - \sum_{i=1}^{M} \textbf{a}_i \textbf{x}_i^{T} \right\|_{F}^{2}\nonumber\\
&= \left\| \left( \textbf{Y}- \sum_{i\neq i_{i}}^{M} \textbf{a}_i\textbf{x}_i^{T}\right) - \textbf{a}_{i_{0}}\textbf{x}_{i_{0}}^{T} \right\| _{F}^{2},\label{eq:SVD2} 
\end{align}
where $F$ is the Frobenius norm that works on matrices
\begin{align*}
\Vert \mathbf{A} \Vert_F = \sqrt{\sum_{i=1}^M \sum_{j=1}^N \vert a_{i,j} \vert^2}.
\end{align*} 
In \eqref{eq:SVD2} the term in the parenthesis makes the an error matrix $\textbf{E}_{i_0}$ without the contribution from $i_{0}$, hence minimising \eqref{eq:SVD2} with respect to $\textbf{a}_{i_{0}}$ and $\textbf{x}_{i_{0}}^{T}$ leads to the optimal contribution from $i_{0}$ (can I say it this way..?). 
\begin{align}
\min_{\textbf{a}_{i_{0}},\textbf{x}_{i_0}^{T}}\left\|\textbf{ E}_{i_{0}}-\textbf{a}_{i_{0}},\textbf{x}_{i_0}^{T} \right\|_{F}^{2}\label{eq:SVD3}
\end{align} 
The optimal solution to \eqref{eq:SVD3} is known to be the rank-1 approximation of $\textbf{E}_{i_{0}}$. This comes from the Eckart–Young–Mirsky theorem\cite{?} saying that a partial single value decomposition(SVD) makes the best low-rank approximation of a matrix such as $\textbf{E}_{i_0}$.\\
That is specifically that for $\textbf{E}_{i_0}=\textbf{U}\boldsymbol{\Sigma}\textbf{V}^T\in \mathbb{R}^{M\times N},\ M \leq N$ with 
\begin{align*}
\textbf{U}=\left[\textbf{u}_1, \hdots, \textbf{u}_M\right] \in \mathbb{R}^{M\times M}, \quad \boldsymbol{\Sigma}=\text{diag}\left[\sigma_1, \hdots , \sigma_m \right] \in \mathbb{R}^{M\times N}, \quad \textbf{V}=\left[\textbf{v}_1, \hdots, \textbf{v}_N\right] \in \mathbb{R}^{N\times N} 
\end{align*}  
where $\textbf{U}$ and $\textbf{V}$ are unitary matrices, i.e. $\textbf{U}^T\textbf{U}=\textbf{UU}^T=\textbf{I}$, and $\sigma_j$ is the non-negative singular values of $\textbf{E}_{i_0}$ such that $\sigma_1\geq \sigma_2 \geq \hdots \geq 0$. The best $k$-rank approximation to $\textbf{E}_{i_0}$, with $k< rank(\textbf{E}_{i_0})$ is then given \todo{find kilde}by\cite{Wiki..} 
\begin{align*}
\textbf{E}_{i_{0}}^{(k)}= \sum_{j=1}^{k}\sigma_j\textbf{u}_{j}\textbf{v}_{j}^T.
\end{align*} 
Since the outer product always have rank-1 letting $\textbf{a}_{i_0}=\textbf{u}_1$ and $\textbf{x}_{i_0}^T = \sigma_{i}\textbf{v}_{1}^T$ solves the optimisation problem \eqref{eq:SVD3}.
However in order to preserve the sparsity in $\textbf{X}$ while optimising, only the non-zero entries in $\textbf{x}_{i_0}^T$ are allowed to vary. For this purpose only a subset of columns in $\textbf{E}_{i_0}$ is considered, those which correspond to the non-zero entries of $\textbf{x}_{i_0}^T$. A matrix $\textbf{P}_{i_0}$ is defined such that $\textbf{x}_{i_0}^{T^{(R)}}=\textbf{x}_{i_0}^T\textbf{P}_{i_0} $ is restricted to contain only the $M_{j_0}$ non-zero entries of $\textbf{x}_{i_0}^T$. By applying SVD to the  sub-matrix $\textbf{E}_{i_0}\textbf{P}_{i_0}$ and updating $\textbf{a}_{i_0}$ and $\textbf{x}_{i_0}^{T^{(R)}}$ the rank-1 approximation is found and the original representation vector is updated as $\textbf{x}_{i_0}^{T}=\textbf{x}_{i_0}^{T^{(R)}}\textbf{P}_{i_0}^{T}$.  \\ \\
The main steps of K-SVD is described in algorithm \ref{alg:K_SVD}. 

\begin{algorithm}[H]
\caption{K-SVD}
\begin{algorithmic}[1]
			\State$k = 0$			
			\State$\text{Initialize random} \quad  \textbf{A}_{(0)}$            
			\State$\text{Initialize} \quad \textbf{X}_{(0)}=\mathbf{0}$
			\State
            \Procedure{K-SVD}{$\textbf{A}_{(0)}$}    
            \State$\text{normilize columns of} \ \textbf{A}_{(0)}$
            \While{$error \geq limit$} 
                \State $j = j+1$
                \For{$j \gets 1,2,\hdots, L$} \Comment{updating each col. in $\textbf{X}_{(k)}$}
                	\State$\hat{\textbf{x}}_{j} = \min_{\textbf{x}} \|\textbf{y}_j -\textbf{A}_{(k-1)}\textbf{x}_{j}\| \quad s.t. \quad \|\textbf{x}_{j}\| \leq k_0 $
				\EndFor
				\State$\textbf{X}_{(k)}=\{\hat{\textbf{x}}_{j}\}_{j=1}^{L}$
				\For{$i_0 \gets 1,2,\hdots, N$}
					\State$\Omega_{i_0}=\{j|1\leq j \leq L , \textbf{X}_{(k)}[i_0,j]\neq 0\}$
					\State$\text{From} \ \Omega_{i_0} \ \text{define} \ \textbf{P}_{i_0} $
					\State$\textbf{E}_{i_0} =  \textbf{Y}- \sum_{i\neq i_{0}}^{M} \textbf{a}_i\textbf{x}_i^{T}$
					\State$\textbf{E}_{i_0}^R = \textbf{E}_{i_0}\textbf{P}_{i_0}$
					\State$\textbf{E}_{i_0}^R=\textbf{U}\boldsymbol{\Sigma}\textbf{V}^T$ \Comment{perform SVD}
					\State$\textbf{a}_{i_0}\gets \textbf{u}_{1}$ \Comment{update the $i_0$ col. in $\textbf{A}_{(k)}$}
					\State$\left( \textbf{x}_{i_0}^T\right)^R \gets \sigma_{1}\textbf{v}_{1}$
					\State$\textbf{x}_{i_0}^T \gets \left( \textbf{x}_{i_0}^T\right)^R \textbf{P}_{i_0}^T $ \Comment{update the $i_0$ row in $\textbf{X}_{(k)}$}
				\EndFor
				\State$error =\| \textbf{Y}-\textbf{A}_{(k)}\textbf{X}_{(k)}\|_{F}^2 $
          		\EndWhile
            \EndProcedure
        \end{algorithmic} 
        \label{alg:K_SVD}
\end{algorithm}

The dictionary learning algorithm K-SVD is a generalisation of the well known K-means clustering also referred to as vector quantization. In K-means clustering a set of K vectors is learned referred to as mean vectors, each signal sample is then represented by its nearest mean vector. That corresponds to the case with sparsity constrict $k=1$ and the representation reduced to a binary scalar $x={1,0}$. Further instead of computing the mean of $K$ sub-sets the K-SVD algorithm computes the SVD factorisation of the K different sub-matrices that correspond to the K columns of $\textbf{A}$.\\
