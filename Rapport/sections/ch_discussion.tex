\chapter{Discussion}
The purpose of this thesis was to investigate the possibility of reproducing state of the art methods and results for recovery of source signals, from low-density scalp EEG measurements inducing an under-determined system.
The considered state of the art methods are multiple sparse Bayesian learning (M-SBL)\cite{Balkan2014} and covariance-domain dictionary learning (Cov-DL)\cite{Balkan2015}, both published by O. Balkan, et al. in the year of 2014 and 2015 respectively. 
It was found that this task was not easy. Among others did the scientific articles not provide any software to allow recreation of their results. The resulting combination of Cov-DL and M-SBL, the main algorithm, did not manage to fully solve the inverse EEG problem of recovering the mixing matrix $\mathbf{A}$ and the source matrix $\mathbf{X}$ successfully, from the EEG measurements. 
From the verification of the Cov-DL method, it was concluded that it failed to provide a sufficient estimate $\hat{\mathbf{A}}$, when applied on synthetic data. 
Due to not having a successful estimate of $\mathbf{A}$ the recovery of  $\mathbf{X}$ was compromised when using the M-SBL method.
However, when using the true $\mathbf{A}$, M-SBL provides an estimate $\hat{\mathbf{X}}$ sufficiently close to the real source matrix, in the under-determined case $M < N$. 
In the main algorithm the estimate $\hat{\mathbf{A}}$ from Cov-DL was replaced by a fixed estimate of the mixing matrix $\hat{\mathbf{A}}_{\text{fix}}$. 
The final performance of the main algorithm was found to vary significantly, when applied to synthetic data, hence a sufficient performance was not confirmed. 
As expected a similar conclusion was to be drawn from tests on real EEG measurement. 

In chapter \ref{ch:implementation} an unsuccessful implementation of the Cov-DL method concluded.
By the observed results the issue appears to be located within the definition of the optimization problem determining the columns of $\hat{\mathbf{A}}$. 
The possibility of en implementation error can not be denied as an additional thorough run-through of the implementation was not possible within the time scope of the thesis. 
However, the findings may suggest a misinterpretation of the theory and reasoning presented by the article. Lastly the possibility that the method do not manage to provide the same results when applied on different data has to be considered.       
Overall this questions the reproducibility of the scientific article \cite{Balkan2015} which proposes the method. 
The article did not provide any code or implementation specifications. 
Likewise, it was not possible to recreate or access the exact data, which was used to provide the results presented in the article. 
Thus, the intention was never to recreate the exact results from the article but rather to demonstrate the conclusions resulting from a certain success rate.  
One could argue that testing the implementation on the same data would lead to a different conclusion. 
However, this was sought approached by the stochastic simulated data, cf. section \ref{sec:dataset}, which was created with inspiration from the article.

In chapter \ref{ch:implementation} it was concluded that the M-SBL method manage to successfully estimate $\mathbf{X}$ when applied to the stochastic data sets and given the true $\mathbf{A}$. 
Though, the performance was found to decrease slightly as the number of sources increases relative to the number of sensors. 
Regarding the reproducibility of the article \cite{Balkan2014} the results indicate that the provided information about M-SBL has been sufficient.
However, this article did, as \cite{Balkan2015}, not provide any code or data disabling the possibility of recreating the exact results. In \cite{Balkan2014} tests were conducted on random simulations of $\mathbf{A}$ and $\mathbf{X}$ with various noise level added. 
Thus, due to no counter arguments, the tests of M-SBL in this thesis were conducted on the synthetic data sets already created for the tests of Cov-DL, with inspiration from \cite{Balkan2015}.

With respect to the main algorithm, uniting Cov-DL and M-SBL, an alternative to the estimate of $\mathbf{A}$ was necessary. 
Through empirical tests, which is discussed later, a fixed estimate $\hat{\mathbf{A}}_{\text{fix}}$ was chosen to replace the estimate from Cov-DL. 
With $\hat{\mathbf{A}}_{\text{fix}}$ the main algorithm manage to estimate $\mathbf{X}$ but with a significantly higher error compared to the performance conditioned on the true $\mathbf{A}$. Hence this performance is not considered to be successful.

The performance test of the main algorithm was first conducted on simulated stochastic data resembling real EEG measurements. 
Inspired by \cite{Balkan2015} the sources were simulated by independent auto-regressive processes. 
The true $\mathbf{A}$ however, was simply generated by a Gaussian distribution. 
This choice was based on a lack of information to point in different directions. 
Instead of the true $\mathbf{A}$ being chosen as a stochastic matrix a deterministic matrix could have been chosen instead. 
The choice of the stochastic true $\mathbf{A}$ for the synthetic data sets could affect the results when testing the fixed alternatives for the mixing matrix estimated by Cov-DL. 
From the test, cf. section \ref{sec:test_base}, it was found that a fixed Gaussian estimate, generated with same specifications as the true $\mathbf{A}$, did not lead to the best recovery of the $\mathbf{X}$, which went against the natural expectation -- the better estimate of $\mathbf{A}$ the better estimate of $\mathbf{X}$. 
Here a fixed estimate with Gaussian distribution of higher variance provided the best estimate of $\mathbf{X}$. 
It is here one could argue that the stochastic true $\mathbf{A}$ had an influence to the results. 
Furthermore, the choice of error measurement might also be reflected in the results. Not being sufficient towards the purpose. 

The common choice of error measurement throughout the thesis was the mean squared error (MSE). 
The MSE measures the performance of an estimate with respect to the true value, by comparing each element and summing the squared error. 
Hence, the MSE is providing a measure of how far the estimate is from the true value. 
This was considered a sufficient error measurement for evaluation of Cov-DL and M-SBL.
However, one challenge when using MSE is to set a tolerance defining when an estimate is considered successful. 
It could be argued that the chosen tolerance should vary with respect to the data of interest.
A different choice of error measurement could have been the use of correlation between the estimated values and the true values. With respect to the comparison of the main algorithm to ICA, where ICA is considered the ground truth, using the correlations as the evaluation might have overcome the scaling issue with respect to ICA.    

Consider now the performance test of the main algorithm on real EEG measurements. 
The choice of evaluating the performance by considering the ICA solution as the ground truth has been crucial. 
First of all the foundation for the evaluation was not ideal. 
It is an issue that the performance of the main algorithm on the simulated data was not as good expected, presumably due to the estimate of $\mathbf{A}$ being replaced by a fixed estimate. 
Thus, it is not reasonable to trust the results when the algorithm is applied to data for which the true results are not known at all. 
However, it can be argued that when comparing the obtained result to the best-known solution, in this case provided by ICA, a small error will indicate an acceptable performance from the main algorithm. 
The arguments accounting for the use of ICA were discussed in section \ref{seg:main_test_description}. 
However, an unreliability will be present as the ICA algorithm is limited to $M = N$. 
The true $N$ is unknown, thus ICA is never guaranteed to find all the active sources.
Furthermore, the comparison of the main algorithm to ICA was found to be compromised. 
The localization and phase of the active sources are not necessary the same for the two estimates, which distorted the MSE between the two matrices to an unknown degree.
In despite of this issue, the comparison was still performed by MSE, which suggest a potential to improve the found performance. 
Against the prior expectation, an acceptable performance was found for the case $M = N$, though for the cases of interest where $M < N$ the performance was decreased significantly.    

Due to the possible unreliability of the performance evaluation with respect to ICA, an alternative test was considered -- the alpha wave analysis. 
However, from the analysis no new conclusion was made. The expected behavior was not observed, with respect to an increased amount of alpha frequency for the test subject having closed eyes.
The behavior was more or less fluctuating over time. However, exceptions from the expected behavior were also found for the raw measurement. This indicates that different approaches, with respect to measuring the power within the alpha band, should be considered.
The advantage of this test approach, in general, is that one see past the challenge of recovering the exact source signal, but rather consider the practical usage of the source separation. For instance, when considering the usage of the source signals within a hearing aid, cf. section \ref{seg:application}, the amount of active source signals might be more significant than an exact recovery.         

The last issue addressed in this thesis was the estimation of the number of active sources $k$ relative to the maximum number of sources $N$. Through out the implementation of the main algorithm in chapter \ref{ch:implementation} it was argued that setting $k = N$, would not compromise the results. 
In fact perhaps lead to better estimates as fewer sources must be recovered and therefore reduces the amount of possible errors.
This is supported by \cite{Balkan2014} where the same assumption is used.
Likewise for M-SBL, providing $k$ would only reduce possible errors within localization of the sources.
However, by setting $k = N$ one must assume that $k$ sources are active, thus no less that $k$ sources are estimated. 
Hence, to justify this definition of $k$ one must have a qualified guess with respect to the true $k$. 
However, this is the issue addressed in the problem statement, as this is not possible in practice.    

To address this exact issue an investigation with respect to estimation of $k$ was conducted in chapter \ref{ch:estimation_k}. 
Due to interesting empirical observation, it was chosen to analyse the source signals resulting from the main algorithm when $k = N = \widetilde{M}$. 
That is estimating the maximum number of active sources under the hypothesis that the false estimates where to be identified among the true estimates. By false estimate there is referred to a non-zero estimate of a zero row. 
From visual observation of results from the simulated stochastic data sets, a potential was seen as the false estimates appear as replicas of the true estimates. 
These false estimates was identified by searching for the replicates of the true estimates.
However, this was not found successful in the desired cases where $M < k$. 
Here the false estimates manage to diverge more from the true estimates. 
Furthermore, the identification method was found to have trouble separating the true estimate from the corresponding replicates.
Hence alternative methods could have been considered with respect to estimation of $k$. 
One obvious approach is to consider the estimate of $k$ which could be provided from M-SBL, if a $k$ was not given as an input to the algorithm. 
However, the success rate of such estimation of $k$ was not provided in the article \cite{Balkan2014} of which the article was based. As the performance presented in the article was obtained by providing $k$ to the algorithm, cf. \ref{subsec:kestimate}.   

Throughout this chapter the choices which ware found essential toward the obtained conclusions have been discussed and alternative choices have been considered. 
None of these alternatives are sought investigated in the thesis but they will serve as essential aspects to be considered if further work on the main algorithm where to be conducted. 