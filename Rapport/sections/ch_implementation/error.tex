\subsection{Error Measurement}\label{sec:mse}  
To evaluate performance of the algorithms, it is evident to look at the differences between the true and estimated matrices, mixing matrix $\mathbf{A}$ and source matrix $\mathbf{X}$ -- which is possible due to the input data being simulated. 
With the model in \eqref{eq:MMV} being invariant the true difference between the true and estimated matrices can not be measured. 
However by only measuring the performance of one estimate, and not being dependent on another estimate, the invariance do not occur. 
Thus the finalized test of the main algorithm the performance would be affected and not showing the real performance of main algorithm. 
However, this can be overcome by looking the performance of the individual estimated and then with the results in mind map and empirical conclusion on the performance of the main algorithm  \todo{Jeg prøvede her at snakke om invariance problem meget kort. Tænker at de yderligere skal nævnes senere i kapitlet hvor main testes}.

For this task the mean squared error (MSE) has been chosen. 
The MSE measures the average squared difference between some estimated value and the true value. 
For $\hat{\textbf{g}}$ being the estimate of the vector $\textbf{g}$ the MSE can be written as 
\begin{align*}
\text{MSE}(\textbf{g},\hat{\textbf{g}}) = \frac{1}{T} \sum_{i=1}^T (g_i - \hat{g}_i)^2, 
\end{align*}
with $T$ being the number of elements in the vector $\textbf{g}$. 
However, for this thesis the true used of the MSE is

For this thesis the estimates form a matrix. 
Here the MSE is computed for each row, which for $\mathbf{X}$ is the estimate of one source signal. 
Then the resulting MSE is the average over all rows. 
For $\mathbf{X}, \hat{\mathbf{X}} \in \mathbb{R}^{N \times L}$ the MSE is written as 
\begin{align*}
\text{MSE}(\mathbf{X},\hat{\mathbf{X}}) = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{1}{L} \sum_{j=1}^L (\mathbf{X}_{ij} - \hat{\mathbf{X}}_{ij})^2\right).  
\end{align*}
Similarly, the MSE can be written for $\mathbf{A},\hat{\mathbf{A}} \in \mathbb{R}^{M \times N}$.  

The MSE is viewed as a measure of the quality of an estimator, in this case of how M-SBL and Cov-DL perform. 
The MSE considers both the variance among the estimated samples and the bias which is how far the average estimate value is from the true value \cite[p.305]{MSE_book}.  
Thus the larger MSE the more widely dispersed is the estimate around the true parameter.
In particular, $\hat{\mathbf{X}}_1$ is considered  a better estimated than $\hat{\mathbf{X}}_2$ if $\text{MSE}(\mathbf{X},\hat{\mathbf{X}}_1)< \text{MSE}(\mathbf{X},\hat{\mathbf{X}}_2)$. 
