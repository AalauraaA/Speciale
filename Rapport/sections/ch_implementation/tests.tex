\section{Algorithm verification}
In this section the implementation of Cov-DL and M-SBL are verified separately based on the MSE between the true and the estimated matrices.

\subsection{Test of Cov-DL}
As seen from the flow diagram figure \ref{fig:flow} Cov-DL takes a measurement matrix $\textbf{Y}$, $N$ and $k$ as input and return an estimation of the mixing matrix $\textbf{A}$. The Cov-DL algorithm is tested on the two simulations of a simple data, as specified in section \ref{subseg_simpledata}. 

For $\textbf{Y}$ specified by $N\leq \widetilde{M}$, implying Cov-DL2, the true and estimated values of $\textbf{A}$ are plotted in figure \ref{fig:cov2_simple} for visual comparison.
The resulting MSE between the true $\textbf{A}$ and the estimated $\hat{\textbf{A}}$ become 
\begin{align*}
A_{MSE} = 2.19 
\end{align*}

For $\textbf{Y}$ specified by $N > \widetilde{M}$ and $k\leq \widetilde{M}$, implying Cov-DL1, the true and estimated values of $\textbf{A}$ are plotted in figure \ref{fig:cov1_simple} for visual comparison.
The resulting MSE between the true $\textbf{A}$ and the estimated $\hat{\textbf{A}}$ become 
\begin{align*}
A_{MSE} = 1.37
\end{align*}

\begin{figure}[H]
    \begin{minipage}[t]{.45\textwidth}
    	\centering
		\includegraphics[scale=0.5]{figures/ch_6/COV2_simple.png}
		\caption{Estimated values of $\hat{\textbf{A}}$ compared to the true 					values $\textbf{A}$}
		\label{fig:cov2_simple}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/ch_6/COV1_simple.png}
		\caption{Estimated values of $\hat{\textbf{A}}$ compared to the true 				values $\textbf{A}$}
		\label{fig:cov1_simple}
    \end{minipage}
\end{figure}
It is seen that .... it is only good due to the initial $\textbf{A}$ being close to the true $\textbf{A}$... for COV-DL2 it is in fact the init A which are used, that is no optimization have been conducted, even though it writes that 25 iterations has been conducted... 

\subsection{Test of M-SBL}
From the flow diagram figure \ref{fig:flow} it seen that that the M-SBL algorithm takes $\textbf{A}$(maybe $k$) and $\textbf{Y}$ as input. The algorithm is now tested on the same two simple data sets specified by $M = 3$, $k=4$, $L=1000$ and respectively $N = 5$ and $N = 8$ as used above. 
In order to not let the performance of COV-DL affect the result of M-SBL it is the true $\textbf{A}$ which is given as input along with the corresponding $\textbf{Y}$. The estimate $\hat{\textbf{X}}$ are plotted in figure \ref{fig:M-SBL_simple1} and \ref{fig:M-SBL_simple2}. Each non zero signals are now plotted separately for simple visual comparison. Furthermore, note that each compared couple of rows have the same row index, as such the localisation of the estimated row can be evaluated.
\begin{figure}[H]
    \begin{minipage}[t]{.45\textwidth}
    	\centering
		\includegraphics[scale=0.5]{figures/ch_6/M-SBL_simple1.png}
		\caption{Estimated values of $\hat{\textbf{X}}$ compared to the true 					values $\textbf{X}$. From measurement $\textbf{Y}$ specified by $N=5$, $M = 3$, $k=4$ and $L=1000$}
		\label{fig:M-SBL_simple1}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/ch_6/M-SBL_simple2.png}
		\caption{Estimated values of $\hat{\textbf{X}}$ compared to the true 				values $\textbf{X}$. From measurement $\textbf{Y}$ specified by $N=8$, $M = 3$, $k=4$ and $L=1000$ }
		\label{fig:M-SBL_simple2}
    \end{minipage}
\end{figure}

The resulting MSE between the true $\textbf{X}$ and the estimated $\hat{\textbf{X}}$ from figure \ref{fig:M-SBL_simple1} where $N = 5$, averaged over every row, become 
\begin{align*}
X_{MSE} = 0.131 
\end{align*}
From figure \ref{fig:M-SBL_simple} it is seen that all four source signals are recovered at the right location. As suggested by the achieved MSE it the estimates are not exact, but it is clear that the estimates manage to follow the right pattern at the right location. By this plot an error margin are established(?).  

The resulting MSE between the true $\textbf{X}$ and the estimated $\hat{\textbf{X}}$ from figure \ref{fig:M-SBL_simple2} where $N = 8$ thus more sparse, become 
\begin{align*}
X_{MSE} = 0.145 
\end{align*}
From figure \ref{fig:M-SBL_simple2} it is seen that first source signal are recovered at the third entry, that is one dislocation, while the rest are recovered at the right location. This indicate that the algorithm can manage to locate the source signal however the more options the more greater chance of dislocation.     

\subsubsection*{Possiblities of $N=k$}


\begin{enumerate}
\item Describe why we want to set $N = k$, logical discussion. No results yet to back up the assuming.
	\begin{itemize}
	\item The amount of $N$ is unknown as it change for every brain
	\item Therefore, we do not know the amount of activation inside the brain, but it those there are of interest in this thesis.
	\item Due to the method for determine the mixing matrix $\textbf{A}$ the ration between $N$ and $k$ do not affect the result, hence there are no argument against letting $N=k$ which will lower the computational complexity.\todo[inline]{do the same argument regarding $N=k$ hold for the determination of $\textbf{X}$? after the awareness that we do not need to provide $k$ to M-SBL as it has been done so far. If we do not provide $k$ it will seek the sparest solution right?}    
	\end{itemize}
\end{enumerate}

\section{Test on AR data and model fitting}
The implemented algorithms are now tested on the simulated AR data which resembles the real EEG measurements. Furthermore it is sought to improve the model by fitting specific model variables to the data. In this case it is the COV-DL algorithm which can be adjusted for the overdetermined case where COV-DL2 is used.      

\input{sections/ch_implementation/model_fit.tex} 


% old results
%For the test we are looking at a system of size $M = 8$, $k = 16$ and $L = 1000$. Furthermore, for the Cov-DL the data has been divided in segments of 10 samples leading to 100 segments.
%\begin{figure}[H]
%\centering
%    \begin{minipage}[t]{.45\textwidth}
%        \centering
%		\includegraphics[scale=0.5]{figures/chapter6/Mix_Error_initial_A_m8_k16_L1000.png}
%		\subcaption{Simple Data Set - Estimated A}
%    \end{minipage} 
%    \hfill
%    \begin{minipage}[t]{.45\textwidth}
%        \centering
%		\includegraphics[scale=0.5]{figures/chapter6/AR_Error_initial_A_m8_k16_L1000.png}
%		\subcaption{Autoregressive Data Set - Estimated A}
%    \end{minipage}
%    \begin{minipage}[t]{.45\textwidth}
%        \centering
%		\includegraphics[scale=0.5]{figures/chapter6/Mix_Error_initial_A_m8_k16_L1000_RealA.png}
%		\subcaption{Simple Data Set - Real A}
%    \end{minipage} 
%    \hfill
%    \begin{minipage}[t]{.45\textwidth}
%        \centering
%		\includegraphics[scale=0.5]{figures/chapter6/AR_Error_initial_A_m8_k16_L1000_RealA.png}
%		\subcaption{Autoregressive Data Set - Real A}
%    \end{minipage}
%\caption{}
%\label{fig:initialA}
%\end{figure}
%\noindent
%The MSE values of each tests -- estimated and real mixing matrix $\mathbf{A}$ -- can be seen in table \ref{tab:iniA}. For the real mixing matrix the MSE values of source matrix $\mathbf{X}$ are identical because the real mixing matrix is not influenced by the three different initial $\mathbf{A}_{\text{ini}}$.
%\begin{table}[H]
%\begin{minipage}{.5\linewidth}
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline 
% & $\mathbf{A1}$ & $\mathbf{A2}$ & $\mathbf{A3}$ \\ 
%\hline 
%MSE of $\mathbf{A}$ & 1.16 & 1.21 & 1.17 \\ 
%\hline 
%MSE of $\mathbf{X}$ & 0.69 & 0.66 & 0.81 \\ 
%\hline 
%\end{tabular} 
%\subcaption{Simple Data Set - estimated $\mathbf{A}$}
%\end{minipage}
%\begin{minipage}{.5\linewidth}
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
% & $\mathbf{A1}$ & $\mathbf{A2}$ & $\mathbf{A3}$ \\ 
%\hline 
%MSE of $\mathbf{A}$ & 2.21 & 2.09 & 2.02 \\ 
%\hline 
%MSE of $\mathbf{X}$ & 8.43 & 9.55 & 8.93 \\ 
%\hline
%\end{tabular} 
%\subcaption{Autoregressive Data Set - estimated $\mathbf{A}$}
%\end{minipage}
%\begin{minipage}{.5\linewidth}
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline 
% & $\mathbf{A1}$ & $\mathbf{A2}$ & $\mathbf{A3}$ \\ 
%\hline 
%MSE of $\mathbf{A}$ & $\times$ & $\times$ & $\times$ \\ 
%\hline 
%MSE of $\mathbf{X}$ & 0.25 & 0.25 & 0.25 \\ 
%\hline 
%\end{tabular} 
%\subcaption{Simple Data Set - real $\mathbf{A}$}
%\end{minipage}
%\begin{minipage}{.5\linewidth}
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
% & $\mathbf{A1}$ & $\mathbf{A2}$ & $\mathbf{A3}$ \\ 
%\hline 
%MSE of $\mathbf{A}$ & $\times$ & $\times$ & $\times$ \\ 
%\hline 
%MSE of $\mathbf{X}$ & 1.84 & 1.84 & 1.84 \\ 
%\hline
%\end{tabular} 
%\subcaption{Autoregressive Data Set - real $\mathbf{A}$}
%\end{minipage}
%\caption{(a) This are the MSE values achieve from the simple data set and (b) is the MSE values achieved from the autoregressive data set.}
%\label{tab:iniA}
%\end{table}
%\noindent
%From \ref{tab:iniA} we see that the MSE values for our estimated mixing matrix $\mathbf{A}$ are overall small compared to both data sets. However, if we look at the results for the source matrix there are a big different from the simple data set and the autoregressive data set and comparing it to the results with the real mixing matrix. 
%As the autoregressive data set have a higher amplitude in the data than the simple data set, the error will become larger. 
%
%If we look at the initial $\mathbf{A}_{\text{ini}}$ drawn from the Gaussian distribution we have a small error for the mixing matrix in the autoregressive data set while the error from the simple data set also is low. The error for source matrix is the highest value for the simple data set but comes second in the autoregressive data set. As we want to take into account that the autoregressive data set resemble the EEG measurement most we conclude that the initial $\mathbf{A}_{\text{ini}}$ from the Gaussian distribution would be the best choice in Cov-DL.

\subsubsection{Segmentation in Cov-DL}
For the Cov-DL algorithm when estimating the mixing matrix $\mathbf{A}$ the measurement matrix is transformed into the covariance domain as part of the recovering process. During the transformation the measurement matrix $\mathbf{Y}$ is divided into segments consisting of $L_s$ samples each.
During this test we will be looking how many samples $L_s$ each segment can have and how this affect the performance of the algorithms.

The system of interest to be tested on is when we have $M = 8$, $k = 16$ and $L = 1000$ samples. With $k = 16$ the number of segments can not be less than the number of active sources. For this system each segment can have maximum $L_s = 62$ corresponding to 16 segments. 
For the test, six different number of samples within each segments will be tested, $L_s = \{ 10, 20, 30, 40, 50, 60 \}$.
\begin{figure}[H]
\centering
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/chapter6/Mix_Error_vary_covseg_m8_k16_L1000}
		\subcaption{Simple Data Set - Estimated A}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/chapter6/AR_Error_initial_A_m8_k16_L1000.png}
		\subcaption{Autoregressive Data Set - Estimated A}
    \end{minipage}
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/chapter6/Mix_Error_vary_covseg_m8_k16_L1000_RealA.png}
		\subcaption{Simple Data Set - Real A}
    \end{minipage} 
    \hfill
    \begin{minipage}[t]{.45\textwidth}
        \centering
		\includegraphics[scale=0.5]{figures/chapter6/AR_Error_initial_A_m8_k16_L1000_RealA.png}
		\subcaption{Autoregressive Data Set - Real A}
    \end{minipage}
\caption{}
\label{fig:seg}
\end{figure}
\noindent
The MSE values of each tests -- estimated and real mixing matrix $\mathbf{A}$ -- can be seen in table \ref{tab:seg}\todo[inline]{hvordan kan det være der kun er 3 punkter for AR dataen?\\
\textbf{X} fejlen er mærkelig høj her, er der en grund til det}.
\begin{table}[H]
\centering
\begin{minipage}{.45\textwidth}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & 10 & 20 & 30 & 40 & 50 & 60 \\ 
\hline 
MSE of $\mathbf{A}$ & 1.21 & 1.09 & 1.18 & 1.08 & 1.11 & 1.13 \\ 
\hline 
MSE of $\mathbf{X}$ & 0.69 & 0.64 & 0.68 & 0.64 & 0.65 & 0.64 \\ 
\hline 
MSE of $\mathbf{X}$ with real $\mathbf{A}$ & 0.33 & 0.33 & 0.33 & 0.33 & 0.33 & 0.33 \\ 
\hline
\end{tabular} 
\subcaption{Simple Data Set - estimated $\mathbf{A}$}
\end{minipage}
\\
\begin{minipage}{.45\textwidth}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & 10 & 20 & 30 & 40 & 50 & 60 \\ 
\hline
MSE of $\mathbf{A}$ & 1.87 & 1.90 & 1.70 & 1.92 & 1.85 & 1.88 \\
\hline 
MSE of $\mathbf{X}$ & 7.61 & 8.06 & 8.34 & 8.28 & 8.12 & 7.23 \\ 
\hline
MSE of $\mathbf{X}$ with real $\mathbf{A}$ & 2.15 & 2.15 & 2.15 & 2.15 & 2.15 & 2.15 \\  
\hline
\end{tabular} 
\subcaption{Autoregressive Data Set - estimated $\mathbf{A}$}
\end{minipage}
\caption{(a) This are the MSE values achieve from the simple data set and (b) is the MSE values achieved from the autoregressive data set.}
\label{tab:seg}
\end{table}
\noindent
Overall, in table \ref{tab:seg} there is not a big variation between the MSE values of both the simple data set and the autoregressive data set. One could argument that the number of samples within each segment does not affect the performance of the algorithms as much and therefore is more free choice. This can be seen as in the autoregressive data set the test with most and fewest samples have the smallest error.